[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Yiğit, though many people call me Yigas—a nickname blending my first and last names. I hold a degree in English Literature, but I currently work as a data scientist at DenizBank in Istanbul. I am mainly interested in causal inference, predictive modeling, explainable AI. In my free time, I dive into basketball data, exploring patterns and creating statistical models that lead to new metrics."
  },
  {
    "objectID": "about.html#the-start",
    "href": "about.html#the-start",
    "title": "About",
    "section": "The Start",
    "text": "The Start\nDuring my first year at university, I spent a semester break in Beirut, Lebanon, where I earned a human biomechanics trainer certificate. I’d been training basketball players at various levels, including those in the Turkish Basketball League (TBL). While in Beirut, I often heard about the impact of prolonged stress on human health. This led me to watch Dr. Robert Sapolsky’s TED Talk, The Biology of Our Best and Worst Selves, and I thought, “I want to understand human behavior the way he does.”"
  },
  {
    "objectID": "about.html#getting-into-psychology-lab",
    "href": "about.html#getting-into-psychology-lab",
    "title": "About",
    "section": "Getting Into Psychology Lab",
    "text": "Getting Into Psychology Lab\nI began watching Stanford’s Human Behavioral Biology courses online. While intriguing, I found the material challenging without a background in the field. To get answers, I reached out to my university’s Psychology Department, where I met Dr. Hasan Bahçekapılı and Dr. Onurcan Yılmaz. Soon, I was visiting Dr. Yılmaz’s office regularly to discuss topics related to psychology, and I grew fascinated with his research interests—especially how morality, politics, religion, and decision-making intersect. Gradually, I shifted my focus from behavioral biology to social and evolutionary psychology. When he offered me a spot in the lab (MINT Lab) he was forming, I jumped at the opportunity."
  },
  {
    "objectID": "about.html#learning-statistics",
    "href": "about.html#learning-statistics",
    "title": "About",
    "section": "Learning Statistics",
    "text": "Learning Statistics\nAhead of the lab’s start, I took an edX’s Science of Religion course to ensure I had foundational knowledge. Although enjoyable, I quickly realized, once we started reading research papers, that I needed a solid ground in statistics to evaluate them. As an English Literature major, I needed a resource that started from scratch, and OpenIntro Statistics and Learning Stats with JASP became essential guides for me. Studying statistics was unexpectedly enjoyable, and I decided to develop my skills further."
  },
  {
    "objectID": "about.html#learning-mathematics",
    "href": "about.html#learning-mathematics",
    "title": "About",
    "section": "Learning Mathematics",
    "text": "Learning Mathematics\nNot knowing mathematics bothered me, and I began to wonder how it might enhance my understanding of statistics. Fortunately, I connected with Dr. Basar Coşkunoğlu, whom I knew through playing Hearthstone. With his patient guidance, I started with basics like functions and inequalities, eventually advancing to calculus and linear algebra. We reached a point where I could continue independently, and I still study linear algebra occasionally, using my notes, Mathematics for Machine Learning, and Gilbert Strang’s works."
  },
  {
    "objectID": "about.html#python-for-data-science-ml",
    "href": "about.html#python-for-data-science-ml",
    "title": "About",
    "section": "Python for Data Science & ML",
    "text": "Python for Data Science & ML\nIn the lab and academia, statistical tools like JASP, Jamovi, and SPSS are prevalent, with some usage of R. Around this time, however, my academic interests began shifting, so I decided to learn Python. I enrolled in a Data Science & Machine Learning Bootcamp, which focused on programming and industry cases, building on my previous knowledge from Introduction to Statistical Learning."
  },
  {
    "objectID": "about.html#data-science-internship",
    "href": "about.html#data-science-internship",
    "title": "About",
    "section": "Data Science Internship",
    "text": "Data Science Internship\nWhile still an undergraduate, I received a scholarship from TUBITAK (The Scientific and Technological Research Council of Turkey) for research participation. Wanting broader experience, I sought a part-time role or long-term internship to balance with school. I started building a project portfolio and applied to various positions. Many interviews revealed that prospective employers lacked data science teams, which felt limiting for my first role. Finally, I applied for and was accepted to DenizBank’s data science internship program."
  },
  {
    "objectID": "about.html#transition-to-full-time",
    "href": "about.html#transition-to-full-time",
    "title": "About",
    "section": "Transition to Full-Time",
    "text": "Transition to Full-Time\nDuring my internship, things went well, and DenizBank invited me to join full-time after graduation. Despite six months of on-the-job coding, I needed to pass a data scientist test in SQL, Python/R, and statistics to secure the position, which I did. I now work full-time as a Data Scientist at DenizBank."
  },
  {
    "objectID": "about.html#basketball-analytics",
    "href": "about.html#basketball-analytics",
    "title": "About",
    "section": "Basketball Analytics",
    "text": "Basketball Analytics\nAfter ten years of playing and training in basketball, I always hoped to integrate it into my work. Inspired by a Formula 1 analytics account, I launched a basketball analytics account of my own. You can find links in the footer."
  },
  {
    "objectID": "posts/Bayesian Updating with Poisson and Gamma/index.html",
    "href": "posts/Bayesian Updating with Poisson and Gamma/index.html",
    "title": "Bayesian Updating: Poisson & Gamma",
    "section": "",
    "text": "The following example is taken from Allen Downey’s Think Bayes. I believe this is a great one to show how to update priors.\nDowney takes a match between France and Crotia, played back in 2018 World Cup, that France won 4-2. Then, he aims to answer two questions:\n\nHow confident we are about France being the better team?\nIn a rematch, what is the probability that France would win again?\n\nI’ll only attempt to answer the first question so that you have a reason to check Downey’s book Think Bayes.\n\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport empiricaldist as emd\nimport scipy.stats as st\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 170)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nStarting out with certain assumptions:\n\nThere is a goal scoring-rate for every team, goals per game (more specifically, per 90), which we will denote as lambda.\nA goal is equally likely during any minute of the game, regardless of strategy, tempo etc.\n(This is also an assumption under a huge framework in basketball analytics, regularized adjusted plus-minus, wanted to point out just in case you follow my basketball analytics content)\nIt isn’t possible for a team to score more than once during a single minute.\n\n\nlam = 1.4 # lambda, goal scoring rate (i.e. goals per game)\ndist = st.poisson(lam) # poisson dist. with lambda = 1.4\n\n\n# probability of scoring \"k\" goals (4 in this case)\nk = 4\ndist.pmf(k).round(3) # pmf evaluated at 4.\n\n0.039\n\n\nSo, there’s 3.9% chance to observe 4 goals, under the model above.\n\nlam = 1.4 # goal scoring rate\ngoals = np.arange(10) # possible values for goals\nprobas = st.poisson(lam).pmf(goals)\n\nprobas\n\narray([2.46596964e-01, 3.45235750e-01, 2.41665025e-01, 1.12777012e-01,\n       3.94719540e-02, 1.10521471e-02, 2.57883433e-03, 5.15766866e-04,\n       9.02592015e-05, 1.40403202e-05])\n\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.barplot(x=goals, y=probas)\n\ng.set_xlabel('Goals')\ng.set_ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\nLet’s try to move the other way around: Estimate the goal-scoring rate from given goals.\nDowney has used data from previous World Cups to estimate that the each team scores 1.4 goals per game, approximately. Hence, it is reasonable to make mean of lambda 1.4.\nThe goal scoring rate is continuous and it can’t take values below 0, hence a distribution that reflects this features would be great: Gamma distribution. Additionally, it’s easy to construct one since it only takes one parameter which is the mean which we already have value for.\n\nalpha = 1.4 # mean of the distribution\nlams = np.linspace(0, 10, 101) # possible values of lam between 0 and 10\nps = st.gamma(alpha).pdf(lams) # probability densities\n\n\nprior = emd.Pmf(ps, lams)\nprior.normalize() # Pmf offers \"normalize\" method, which divides by the total probability of the data (i.e., probability under any parameter/hypothesis)\n\n9.889360237140306\n\n\n\ndf_prior = pd.DataFrame(prior.ps, prior.qs).rename(columns={0:'probas'})\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_prior.index, y=df_prior.probas, color='orange', linestyle='--', linewidth=2)\n\ng.set_title('Prior Distribution')\ng.set_ylabel('PMF')\ng.set_xlabel('Goals')\n\nText(0.5, 0, 'Goals')\n\n\n\n\n\n\n\n\n\n\nnp.sum(df_prior.index * df_prior.probas) # mean of the distribution\n\n1.4140818156118378\n\n\n\nUpdating with new data\nSo, our initial belief for France’s goal scoring rate (in this example, for other teams as well), goals per 90 mins, was 1.4. Then we observed 4 goals from France, should we still think that France’s goal scoring rate is 1.4? If not, how much should it change?\n\nlambdas = lams # different lambdas (different goal scoring rates)\nk = 4 # observed data\nlikelihood = st.poisson(lambdas).pmf(k) # for each lambda (for each parameter), how likely we are to see 4 goals\nlikelihood[:4]\n\narray([0.00000000e+00, 3.77015591e-06, 5.45820502e-05, 2.50026149e-04])\n\n\n\ndf_prior['likelihood'] = likelihood\ndf_prior.head(4)\n\n\n\n\n\n\n\n\nprobas\nlikelihood\n\n\n\n\n0.000\n0.000\n0.000\n\n\n0.100\n0.041\n0.000\n\n\n0.200\n0.049\n0.000\n\n\n0.300\n0.052\n0.000\n\n\n\n\n\n\n\n\np_norm = emd.Pmf(df_prior['probas'] * df_prior['likelihood'], df_prior.index)\np_norm.normalize()\n\n0.05015532557804499\n\n\n\ndf_prior['posterior'] = p_norm\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_prior.index, y=df_prior.posterior, color='blue', linestyle='--')\n\ng.set_xlabel('Goal scoring rate')\ng.set_ylabel('PMF')\ng.set_title('France')\n\nText(0.5, 1.0, 'France')\n\n\n\n\n\n\n\n\n\nSame steps for Crotia:\n\ndf_crotia = df_prior[['probas']].copy()\ndf_crotia.head(3)\n\n\n\n\n\n\n\n\nprobas\n\n\n\n\n0.000\n0.000\n\n\n0.100\n0.041\n\n\n0.200\n0.049\n\n\n\n\n\n\n\n\nk = 2 # Crotia's number of goals in the match\nlikelihood_cro = st.poisson(lams).pmf(2)\nlikelihood_cro[:4]\n\narray([0.        , 0.00452419, 0.01637462, 0.03333682])\n\n\n\ndf_crotia['likelihood'] = likelihood_cro\n\n\np_norm = emd.Pmf(df_crotia['probas'] * df_crotia['likelihood'], df_crotia.index)\np_norm.normalize()\n\n0.1609321178598705\n\n\n\ndf_crotia['posterior'] = p_norm\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_crotia.index, y=df_crotia['posterior'], color='red', linestyle='--')\n\ng.set_xlabel('Goal scoring rate')\ng.set_ylabel('PMF')\ng.set_title('Crotia')\n\nText(0.5, 1.0, 'Crotia')\n\n\n\n\n\n\n\n\n\nHow confident we are that France is the better team?\n\nprint(\n    'Mean of France: ', str(np.sum(df_prior.index * df_prior.posterior).round(1)), '\\n'\n    'Mean of Crotia: ', str(np.sum(df_crotia.index * df_crotia.posterior).round(1))\n)\n\nMean of France:  2.7 \nMean of Crotia:  1.7\n\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_crotia.index, y=df_crotia['posterior'], color='red', label='Crotia')\ng = sns.lineplot(x=df_prior.index, y=df_prior['posterior'], color='blue', label='France')\n\ng.set_xlabel('Goals')\ng.set_ylabel('PMF')\ng.set_title('France vs Crotia')\n\nText(0.5, 1.0, 'France vs Crotia')\n\n\n\n\n\n\n\n\n\nProbability of superiority is a way to express an effect size. Here’s a great visualization tool that you can play with to make expression of effect sizes more intuitive: Interpreting Effect Sizes: An Interactive Visualization\n\n# Probability of superiority.\n\nfrance_proba = 0\n\nfor i in df_prior.index.tolist():\n    for j in df_crotia.index.tolist():\n        if i &gt; j:\n            france_proba += df_prior.loc[i, 'posterior'] * df_crotia.loc[j, 'posterior']\n        else:\n            continue\n\nfrance_proba.round(2)\n\n0.75\n\n\nProbability of superiority feels very intuitive: It is the probability of randomly sampled lambda for France being higher than randomly sampled lambda for Crotia. If there isn’t much overlap between two distributions, we are more confident that the one group (in this example, a team) is higher/lower than the other: Probability of superiority reflects that. You can use any tool (Python, R etc.) to simulate this, and I highly suggest it if you are not currently able to wrap your head around. Just generate different distributions and play with the idea.\nI didn’t include but there is one more reason why we used poisson and gamma distributions, it is related to something called conjugates that I first came across while I was going through Daniel Lakens’ Improving Your Statistical Inferences: Bayesian statistics. They make things computationally more feasible compared to the grid approach here.\nExamples (like this one) from many books can be found on my GitHub repo, and I highly suggest you to go through Allen Downey’s books. Have a nice weekend."
  },
  {
    "objectID": "posts/Bayes Theorem/index.html",
    "href": "posts/Bayes Theorem/index.html",
    "title": "An Intro Example to Bayes’ Theorem",
    "section": "",
    "text": "Imagine that you and I are playing a guessing game. I have two dice, 12-sided and 6-sided, and I am holding them in both of my hands. You are trying to guess which hand has the 12-sided die. At this very moment, there is no information for you. So, it’s not uncommon for a rational agent to think 50-50.\nHowever, you tell me to roll the die I hold on my left hand, and close your eyes. I inform you that I rolled above 3 (i.e., &gt;= 4). Is it still 50-50? It feels like it’s not, since rolling above 3 is more likely with the 12-sided die, right? So, how should you update your initial belief?\nI emphasized the “update” above, this is what one should think of when one encounters the word Bayesian: Updating the prior beliefs in the light of new data. Let’s go through the example.\n\n\nAt the beginning, before any information, it’s 50-50: \\(P(12\\: sided \\ LH) = 0.5\\) and \\(P(12\\: sided \\ RH) = 0.5\\) where LH and RH stands for left-hand and right-hand, respectively. Once you have the information that I rolled bigger than 3, there are two possible scenarios: Either I rolled bigger than 3 with 12-sided or with 6-sided.\n\n\\(P(&gt;3\\: |\\ 12\\ sided) = 0.75\\)\n\\(P(&gt;3\\: |\\ 6\\ sided) = 0.5\\)\n\nLet’s put those in a tree diagram.\n\n\n\nTree Diagram\n\n\n\nThe first column (left to the first vertical white bar) represents initial beliefs.\nThe second column represents the probabilities given the first column.\nThird column is the multiplication of the three, and represents P(A and B).\n\nWell, you are not interested in the whole diagram since you have observed some data: I rolled a number bigger than 3. You wonder:\n\\(P(12\\ sided\\ LH\\: | &gt;3)\\), which is \\(\\dfrac{P(12\\ sided\\ LH\\: ,\\ &gt;3)}{P(&gt;3)}\\).\nFor the numerator, you can track the first row of the tree diagram which leads to 0.375. The denominator consists of two parts, rolling above 3 under two different hypotheses: Following the path of rolling above 3 with 12-sided on the left leads to 0.375, which is the first part. In addition, tracking the other scenario of rolling above 3 with 6-sided on hand leads to 0.25. Hence: \\(\\dfrac{0.375}{0.375+0.25} = 0.6\\)\nThis represents your new belief of having the 12-sided die on my left hand. If you tell me to roll the die again and construct a new tree diagram, the first column will consist of 0.6 and 0.4 and the following columns would be adjusted accordingly.\n\n\n\nDo you have to construct tree diagram each time? After all, even for this simple question the whole process takes a bit of time. With a little bit of algebra, you don’t have to: \\(P(A\\: |\\ B) = \\dfrac{P(A,B)}{P(B)}\\). Multiplying both sides by the P(B): \\(P(A\\: |\\ B) P(B) = P(A,B)\\) and since P(A,B) = P(B,A)\n\\(P(A\\: |\\ B) P(B) = P(B,A)\\) which leads to \\(P(A\\: |\\ B) P(B) = P(B\\: |\\ A) P (A)\\)\nand voila: \\(P(A\\: |\\ B) = \\dfrac{P(B\\: |\\ A) P(A)}{P(B)}\\)\n\nThe left hand side is called posterior probability, and you may come across it in the form of P(hypothesis | data).\nThe denominator on the right is total probability of the data, sometimes referred to as marginal probability.\nP(A) is your initial belief here, the prior.\nWhile P(B | A) is called likelihood which is the probability of the data under given the hypothesis.\n\nFor our example: \\(P(12\\ sided\\: |\\ &gt; 3) = \\dfrac{P(&gt;3\\: |\\ 12\\ sided) P(12\\ sided)}{P(&gt;3)}\\)\n\n\n\nYou may see versions of this where instead of hypothesis there can be theory or parameters (and instead of data, evidence) but they all are the same initially. This type of approach has its advantages such as incorporating the prior knowledge: If I would roll the die again, you would make those calculations with new priors (learned from the first roll), making use of what you already know. It allows for priors that are subjective: Maybe I am known to favor my left hand so it is possible for you to have an initial belief that is not reflected as 50-50.\nI will talk more about this view in the future posts but if you’re interested, you can check probability and Bayesian chapter in each of the books below:\n\nLearning Statistics with R by Daniel Navarro\nPhilosophy of Quantitative Methods by Brian D. Haig\nDoing Bayesian Data Analysis by John K. Kruschke\nImproving Your Statistical Inferences by Daniel Lakens\nThink Bayes by Allen B. Downey\n\nand I believe I remember the example above from Mine Çetinkaya-Rundel."
  },
  {
    "objectID": "posts/Bayes Theorem/index.html#usual-dice-example",
    "href": "posts/Bayes Theorem/index.html#usual-dice-example",
    "title": "An Intro Example to Bayes’ Theorem",
    "section": "",
    "text": "Imagine that you and I are playing a guessing game. I have two dice, 12-sided and 6-sided, and I am holding them in both of my hands. You are trying to guess which hand has the 12-sided die. At this very moment, there is no information for you. So, it’s not uncommon for a rational agent to think 50-50.\nHowever, you tell me to roll the die I hold on my left hand, and close your eyes. I inform you that I rolled above 3 (i.e., &gt;= 4). Is it still 50-50? It feels like it’s not, since rolling above 3 is more likely with the 12-sided die, right? So, how should you update your initial belief?\nI emphasized the “update” above, this is what one should think of when one encounters the word Bayesian: Updating the prior beliefs in the light of new data. Let’s go through the example.\n\n\nAt the beginning, before any information, it’s 50-50: \\(P(12\\: sided \\ LH) = 0.5\\) and \\(P(12\\: sided \\ RH) = 0.5\\) where LH and RH stands for left-hand and right-hand, respectively. Once you have the information that I rolled bigger than 3, there are two possible scenarios: Either I rolled bigger than 3 with 12-sided or with 6-sided.\n\n\\(P(&gt;3\\: |\\ 12\\ sided) = 0.75\\)\n\\(P(&gt;3\\: |\\ 6\\ sided) = 0.5\\)\n\nLet’s put those in a tree diagram.\n\n\n\nTree Diagram\n\n\n\nThe first column (left to the first vertical white bar) represents initial beliefs.\nThe second column represents the probabilities given the first column.\nThird column is the multiplication of the three, and represents P(A and B).\n\nWell, you are not interested in the whole diagram since you have observed some data: I rolled a number bigger than 3. You wonder:\n\\(P(12\\ sided\\ LH\\: | &gt;3)\\), which is \\(\\dfrac{P(12\\ sided\\ LH\\: ,\\ &gt;3)}{P(&gt;3)}\\).\nFor the numerator, you can track the first row of the tree diagram which leads to 0.375. The denominator consists of two parts, rolling above 3 under two different hypotheses: Following the path of rolling above 3 with 12-sided on the left leads to 0.375, which is the first part. In addition, tracking the other scenario of rolling above 3 with 6-sided on hand leads to 0.25. Hence: \\(\\dfrac{0.375}{0.375+0.25} = 0.6\\)\nThis represents your new belief of having the 12-sided die on my left hand. If you tell me to roll the die again and construct a new tree diagram, the first column will consist of 0.6 and 0.4 and the following columns would be adjusted accordingly.\n\n\n\nDo you have to construct tree diagram each time? After all, even for this simple question the whole process takes a bit of time. With a little bit of algebra, you don’t have to: \\(P(A\\: |\\ B) = \\dfrac{P(A,B)}{P(B)}\\). Multiplying both sides by the P(B): \\(P(A\\: |\\ B) P(B) = P(A,B)\\) and since P(A,B) = P(B,A)\n\\(P(A\\: |\\ B) P(B) = P(B,A)\\) which leads to \\(P(A\\: |\\ B) P(B) = P(B\\: |\\ A) P (A)\\)\nand voila: \\(P(A\\: |\\ B) = \\dfrac{P(B\\: |\\ A) P(A)}{P(B)}\\)\n\nThe left hand side is called posterior probability, and you may come across it in the form of P(hypothesis | data).\nThe denominator on the right is total probability of the data, sometimes referred to as marginal probability.\nP(A) is your initial belief here, the prior.\nWhile P(B | A) is called likelihood which is the probability of the data under given the hypothesis.\n\nFor our example: \\(P(12\\ sided\\: |\\ &gt; 3) = \\dfrac{P(&gt;3\\: |\\ 12\\ sided) P(12\\ sided)}{P(&gt;3)}\\)\n\n\n\nYou may see versions of this where instead of hypothesis there can be theory or parameters (and instead of data, evidence) but they all are the same initially. This type of approach has its advantages such as incorporating the prior knowledge: If I would roll the die again, you would make those calculations with new priors (learned from the first roll), making use of what you already know. It allows for priors that are subjective: Maybe I am known to favor my left hand so it is possible for you to have an initial belief that is not reflected as 50-50.\nI will talk more about this view in the future posts but if you’re interested, you can check probability and Bayesian chapter in each of the books below:\n\nLearning Statistics with R by Daniel Navarro\nPhilosophy of Quantitative Methods by Brian D. Haig\nDoing Bayesian Data Analysis by John K. Kruschke\nImproving Your Statistical Inferences by Daniel Lakens\nThink Bayes by Allen B. Downey\n\nand I believe I remember the example above from Mine Çetinkaya-Rundel."
  },
  {
    "objectID": "posts/what happens to sample statistics/index.html",
    "href": "posts/what happens to sample statistics/index.html",
    "title": "What Happens to Sample Statistics?",
    "section": "",
    "text": "Let’s start from the basics: The idea is to gather data to make an inference about the population. We use what we know (sample data) to estimate what we don’t (population).\nSo, let’s see what happens as one collects more data.\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nfor n in range(10, 101, 10):\n    sampled = np.random.normal(loc=100, scale=15, size=n)\n    print('Sampling ' + str(n) + ' observations')\n    print('Mean: ' + str(np.mean(sampled)))\n    print('Standard Deviation ' + str(np.std(sampled)))\n    print('\\n')\n\nSampling 10 observations\nMean: 96.54930308046485\nStandard Deviation 17.69744525686926\n\n\nSampling 20 observations\nMean: 95.26529645787349\nStandard Deviation 13.557714188035797\n\n\nSampling 30 observations\nMean: 99.59631039122944\nStandard Deviation 16.67387851610786\n\n\nSampling 40 observations\nMean: 94.76579341950723\nStandard Deviation 18.06257861698366\n\n\nSampling 50 observations\nMean: 99.61537083458882\nStandard Deviation 14.897878602988285\n\n\nSampling 60 observations\nMean: 98.68203398707892\nStandard Deviation 14.644083079076118\n\n\nSampling 70 observations\nMean: 100.36682454658889\nStandard Deviation 15.427573719883997\n\n\nSampling 80 observations\nMean: 101.00471630426532\nStandard Deviation 12.678064261146886\n\n\nSampling 90 observations\nMean: 102.7631649616841\nStandard Deviation 13.03915710302493\n\n\nSampling 100 observations\nMean: 99.17823535292733\nStandard Deviation 14.951583438089918\n\n\n\n\nAs one increases the sample size taken from the population, sample statistics will approach towards the population parameter.\nDOES NOT NECESSARILY DECREASE! (do not confuse std_dev and std_err) As one can see from the example above!\nBut standard error WILL decrease as the sample size increases. It should make sense intuitively: I have more confidence in my estimates if I know more.\n\nfor n in range(10, 101, 10):\n    sampled = np.random.normal(loc=100, scale=15, size=n)\n    print('Sampling ' + str(n) + ' observations')\n    std_err = np.std(sampled) / np.sqrt(n)\n    print('Standard error approximation: ' + str(std_err))\n    print('\\n')\n\nSampling 10 observations\nStandard error approximation: 4.452823707231046\n\n\nSampling 20 observations\nStandard error approximation: 3.798915557039333\n\n\nSampling 30 observations\nStandard error approximation: 2.3932757697459746\n\n\nSampling 40 observations\nStandard error approximation: 2.624220779126406\n\n\nSampling 50 observations\nStandard error approximation: 2.2840746792281217\n\n\nSampling 60 observations\nStandard error approximation: 2.015124706208566\n\n\nSampling 70 observations\nStandard error approximation: 1.7230901229016737\n\n\nSampling 80 observations\nStandard error approximation: 1.5993946754011168\n\n\nSampling 90 observations\nStandard error approximation: 1.66349862578967\n\n\nSampling 100 observations\nStandard error approximation: 1.577820207327762\n\n\n\n\nHow close is the approximation? Let’s try it for one sample\n\nn = 51\n\npop = np.random.normal(loc=100, scale=15, size=300000) # Population with normal distribution(mean=100, sd=15)\n\nsampled = np.random.choice(pop, size=n) # randomly sampling\nestimated_mean = np.mean(sampled) # sample mean\nestimated_sd = np.std(sampled) # sample standard deviation\n\nestimated_std_err = np.std(sampled) / n**.5 # estimated standard error, expected variation for my sample statistic.\n\nprint(estimated_mean, estimated_sd, estimated_std_err)\n\n95.27311481976386 15.960503088053418 2.2349174605268747\n\n\n\n# Let's take many samples and estimate the mean\n\nmean_estimates = []\n\nfor i in range(1000): # Let's do it 1000 times, sampling 51 in each iteration.\n    sampled = np.random.choice(pop, size=n)\n    mean_estimates.append(np.mean(sampled))\n\nnp.std(mean_estimates)\n\n2.1480690130242537\n\n\nAs one can see, it’s not that far away.\n\nplt.figure(figsize=(10,6))\n\ng = sns.swarmplot(data=mean_estimates, orient=\"h\", size=6, alpha=.8, color=\"purple\", linewidth=0.5,\n                 edgecolor=\"black\")\n\n\n\n\n\n\n\n\nWhat happens when one lowers the sample size? More variation, less confidence. As the sample size increases the estimates approach towards the parameter, so with large sample size each sample ends up having similar estimates. However, that’s not the case with low sample size.\n\nn = 16\nmean_estimates = []\n\nfor i in range(1000): # Let's do it 1000 times\n    sampled = np.random.choice(pop, size=n)\n    mean_estimates.append(np.mean(sampled))\n\nnp.std(mean_estimates)\n\n3.713500386895356\n\n\n\nplt.figure(figsize=(10,6))\n\ng = sns.swarmplot(data=mean_estimates, orient=\"h\", size=6, alpha=.8, color=\"purple\", linewidth=0.5,\n                 edgecolor=\"black\")\n\n\n\n\n\n\n\n\nWatch out the x-axis, it’s much wider now.\n\nmean_estimates = {\n        16:[],\n        23:[],\n        30:[],\n        51:[],\n        84:[],\n        101:[]\n    }\n\nfor n in [16, 23, 30, 51, 84, 101]:\n    for i in range(500):\n        sampled = np.random.choice(pop, size=n)\n        mean_estimates[n].append(np.mean(sampled))\n\n\nfor key in mean_estimates.keys():\n    print('Sample size: ' + str(key))\n    print('Standard deviation (std_err) around the estimates: ' + str(np.std(mean_estimates[key])))\n    print('\\n')\n\nSample size: 16\nStandard deviation (std_err) around the estimates: 3.855337925425897\n\n\nSample size: 23\nStandard deviation (std_err) around the estimates: 3.1917223557725363\n\n\nSample size: 30\nStandard deviation (std_err) around the estimates: 2.7639204989190023\n\n\nSample size: 51\nStandard deviation (std_err) around the estimates: 2.0366022769176806\n\n\nSample size: 84\nStandard deviation (std_err) around the estimates: 1.736807903528174\n\n\nSample size: 101\nStandard deviation (std_err) around the estimates: 1.504399688755003"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nI always had the idea of blogging but a suggestion that I received recently pushed me towards taking an action. I am hoping to use this place to share the stuff that I learn, it makes me develop an intuition and I feel like I learn better.\nMain things that I think about blogging fall under statistics (both frequentist and Bayesian methods), machine learning, and psychology (less frequently).\nAim of this blog is to make this place a useful place for the mentioned topics above, while improving myself on them as well. I hope you find this place useful."
  },
  {
    "objectID": "posts/Man on a Mission - Starting 2025/index.html",
    "href": "posts/Man on a Mission - Starting 2025/index.html",
    "title": "Man on a Mission: Starting 2025",
    "section": "",
    "text": "Some time ago, I stumbled upon the website Less Certainty, More Inquiry. If you click on the link, you’ll see that Darryl Blackport shares what he listens to or reads every week of the year. I found the concept intriguing and thought it would be a great fit for me, as I’m often immersed in studying or reading something.\nThis year, I’m taking inspiration from his approach. I’ll share what I study each month, providing a brief overview of the topics I’ve explored, focusing on what I found particularly interesting or useful. To kick things off, here are my learning goals for 2025. Let’s dive in.\n\n\n\n\n\n\nFor most of my data science journey, I’ve taken a theory-first approach, focusing heavily on statistics. However, with recent advancements in tools like LLMs, I think it’s time for a shift. In 2025, I plan to delve into deep learning (a domain I’ve avoided so far) with a hands-on approach.\nHere’s my roadmap for deep learning:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n\nAdvanced Learning Algorithms\n\nDeep Learning Specialization\n\n\n\n\n\n\n\nI’ve been passionate about sports analytics for nearly a year now and have made significant strides in basketball analytics. (You can check out some of my work here.)\nThis year, I aim to expand my focus by creating videos on football (soccer) analytics, in addition to basketball, on my YouTube channel. To prepare for this, I’ve started the following specialization:\n\nSports Performance Analytics Specialization\n\nAlso, Dean Oliver’s new book, Basketball Beyond Paper, is on my reading list—it’s a must-read for anyone in this space.\n\n\n\n\n\n\nI consider statistics to be one of my stronger skills compared to other data scientists in the industry (at least in Turkey), but there’s always room for growth.\nCurrently, I’m halfway through Think Bayes, and I’m looking forward to Tom Faulkenberry’s upcoming book when it’s released.\nThere are specific topics, like meta-analysis, that I want to strengthen, but I don’t plan to tackle them systematically. Instead, I’ll explore them as curiosity strikes.\n\n\n\n\n\n\nThis one’s a “maybe.” Deep learning and LLMs are higher priorities, and the list is already packed. However, gaining production-level data science expertise could be beneficial in the long run.\nFor now, it’s something I’ll keep on the back burner, but we’ll see how the year unfolds.\n\n\n\n\n\n\n\nThis blog post was polished with help of ChatGPT! I write the whole piece and feed it to ChatGPT to improve the flow.\nAnd that’s pretty much it for my 2025 learning goals. Stay tuned for updates!"
  },
  {
    "objectID": "posts/Man on a Mission - Starting 2025/index.html#man-on-a-mission-series",
    "href": "posts/Man on a Mission - Starting 2025/index.html#man-on-a-mission-series",
    "title": "Man on a Mission: Starting 2025",
    "section": "",
    "text": "Some time ago, I stumbled upon the website Less Certainty, More Inquiry. If you click on the link, you’ll see that Darryl Blackport shares what he listens to or reads every week of the year. I found the concept intriguing and thought it would be a great fit for me, as I’m often immersed in studying or reading something.\nThis year, I’m taking inspiration from his approach. I’ll share what I study each month, providing a brief overview of the topics I’ve explored, focusing on what I found particularly interesting or useful. To kick things off, here are my learning goals for 2025. Let’s dive in.\n\n\n\n\n\n\nFor most of my data science journey, I’ve taken a theory-first approach, focusing heavily on statistics. However, with recent advancements in tools like LLMs, I think it’s time for a shift. In 2025, I plan to delve into deep learning (a domain I’ve avoided so far) with a hands-on approach.\nHere’s my roadmap for deep learning:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n\nAdvanced Learning Algorithms\n\nDeep Learning Specialization\n\n\n\n\n\n\n\nI’ve been passionate about sports analytics for nearly a year now and have made significant strides in basketball analytics. (You can check out some of my work here.)\nThis year, I aim to expand my focus by creating videos on football (soccer) analytics, in addition to basketball, on my YouTube channel. To prepare for this, I’ve started the following specialization:\n\nSports Performance Analytics Specialization\n\nAlso, Dean Oliver’s new book, Basketball Beyond Paper, is on my reading list—it’s a must-read for anyone in this space.\n\n\n\n\n\n\nI consider statistics to be one of my stronger skills compared to other data scientists in the industry (at least in Turkey), but there’s always room for growth.\nCurrently, I’m halfway through Think Bayes, and I’m looking forward to Tom Faulkenberry’s upcoming book when it’s released.\nThere are specific topics, like meta-analysis, that I want to strengthen, but I don’t plan to tackle them systematically. Instead, I’ll explore them as curiosity strikes.\n\n\n\n\n\n\nThis one’s a “maybe.” Deep learning and LLMs are higher priorities, and the list is already packed. However, gaining production-level data science expertise could be beneficial in the long run.\nFor now, it’s something I’ll keep on the back burner, but we’ll see how the year unfolds.\n\n\n\n\n\n\n\nThis blog post was polished with help of ChatGPT! I write the whole piece and feed it to ChatGPT to improve the flow.\nAnd that’s pretty much it for my 2025 learning goals. Stay tuned for updates!"
  },
  {
    "objectID": "posts/Bayesian Way - Predictions out of Posterior Distribution/index.html",
    "href": "posts/Bayesian Way - Predictions out of Posterior Distribution/index.html",
    "title": "Bayesian Way - Predictions Out of Posterior Distribution",
    "section": "",
    "text": "In the previous post, the model was fit using a grid approximation for likelihood calculation under each hypothesis (i.e., for each pair of parameters). Now, let’s take a step further and calculate posterior distribution and take the marginal distribution for each parameter (i.e, the intercept and the slope). If things seem complicated, I advice you to take a look at the previous post.\nAlthough I posted about Bayesian approach before, it might be nice to summarize it a bit:\nIn my opinion, best summation of Bayesian way of thinking is made in a single sentence by John Kruschke in his book Doing Bayesian Data Analysis:\n\"Bayesian inference is reallocation of credibility across possibilities.\"\nThe word “reallocation” implies prior allocation of credibility across possibilities. Let me exemplify the whole thing:\nI work in a hybrid office where we’re required to be present three days a week. We get to choose which days, but most of the team tends to come in on Wednesdays.\nLast Wednesday, I headed to the office assuming that my team leader, Isil, would be there as well. At first, my belief strongly leaned toward her showing up due to my prior knowledge. When I arrived on our floor five minutes early and saw her laptop, my belief was strengthened.\nHowever, as time passed and every other team member arrived—except Isil—my confidence started to fade. By 10 o’clock, I was almost certain she wasn’t coming in. Later, I found out she had to take the day off.\nThis is a nice real-life demonstration of Bayesian view: At the very start, I had a subjective degree of belief about Isil showing up. As I collected more data (e.g., spotting her laptop, others arriving etc.), I updated that belief and it gradually shifted towards the opposite direction.\nOn the example above, I had an informative prior: It wasn’t 50-50 (equally likely) in terms of her showing up or not. On the other hand, this example started with an uninformative prior where each hypothesis is equally likely.\nNow, for this one, I’ll do a very similar thing: I’ll have a prior distribution for each parameter that represents my prior belief about its possible values (i.e., “possibilities above”). Then, I’ll repeat the steps that I did in the previous post and update my belief accordingly.\n\nimport numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport statsmodels as sm\nimport empiricaldist as emd\nimport scipy.stats as st\n\nimport utils as ut\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n\npossum = pd.read_csv('possum.csv', usecols=['pop', 'tail_l'])\ndf = possum.copy()\n\ndf.head(3)\n\n\n\n\n\n\n\n\npop\ntail_l\n\n\n\n\n0\nVic\n36.0\n\n\n1\nVic\n36.5\n\n\n2\nVic\n39.0\n\n\n\n\n\n\n\nI have the same data as before where each row corresponds to features of a possum from Australia and New Guinea. Same as before, let’s attempt to predict which region a possum is from via its tail length.\n\nqs_cept = np.linspace(-1.2, 0.8, 281) # possible values for intercept (beta_0)\nqs_slope = np.linspace(-1.4, 0, 221) # possible values for slope (beta_1)\n\nI’ll create a uniform distribution for both set of possible values (i.e., sample spaces), where each value is equally likely.\n\nuniform_cept = pd.Series(data=[1/len(qs_cept) for qs in qs_cept], index=qs_cept)\nuniform_cept.head(3)\n\n-1.200000    0.003559\n-1.192857    0.003559\n-1.185714    0.003559\ndtype: float64\n\n\n\nuniform_slope = pd.Series(data=[1/len(qs_slope) for qs in qs_slope], index=qs_slope)\nuniform_slope.head(3)\n\n-1.400000    0.004525\n-1.393636    0.004525\n-1.387273    0.004525\ndtype: float64\n\n\nTaking their outer product gives us the joint distribution.\nHeads up: If your priors are normalized (i.e., sum up to 1), so does your joint as well.\n\n## Joint dist\n\nA, B = np.meshgrid(uniform_slope, uniform_cept)\n\ndf_prior = pd.DataFrame(A*B, columns=uniform_slope.index, index=uniform_cept.index)\ndf_prior.iloc[:4, :4]\n\n\n\n\n\n\n\n\n-1.400000\n-1.393636\n-1.387273\n-1.380909\n\n\n\n\n-1.200000\n0.000016\n0.000016\n0.000016\n0.000016\n\n\n-1.192857\n0.000016\n0.000016\n0.000016\n0.000016\n\n\n-1.185714\n0.000016\n0.000016\n0.000016\n0.000016\n\n\n-1.178571\n0.000016\n0.000016\n0.000016\n0.000016\n\n\n\n\n\n\n\n\ndf_prior = df_prior.stack()\ndf_prior.head(3)\n\n-1.2  -1.400000    0.000016\n      -1.393636    0.000016\n      -1.387273    0.000016\ndtype: float64\n\n\nNow the dataframe for prior distribution is ready. Each row, column pair represents possible pair of values for both parameters.\nAlthough it’s not necessary, I’m going to create a different dataframe that represents likelihoods.\n\ndf_likeli = pd.DataFrame(index=qs_cept, columns=qs_slope)\ndf_likeli.fillna(1, inplace=True)\ndf_likeli = df_likeli.stack()\n\ndf_likeli.head(3)\n\n-1.2  -1.400000    1\n      -1.393636    1\n      -1.387273    1\ndtype: int64\n\n\nNow, I ask this question: For each pair of intercept and slope, b0 and b1, how likely I am to see the observed data?\n\ndf['pop'] = df['pop'].apply(lambda x: 1 if x == 'Vic' else 0) # dummy coding\n\n\n# Centering data\n\noffset = df['tail_l'].mean().round()\ndf['x'] = df['tail_l'] - offset\ndf['y'] = df['pop']\n\n# I refer our predictor as x from now on (for convenience), y becomes our target variable which takes 1 if possum is from Victoria region and 0 otherwise.\n\n\nagg_data = df.groupby('x')['y'].agg(['sum', 'count'])\nagg_data.head(10).T\n\n\n\n\n\n\n\nx\n-5.0\n-3.5\n-3.0\n-2.5\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n0.5\n\n\n\n\nsum\n2\n1\n4\n2\n6\n6\n9\n4\n4\n1\n\n\ncount\n2\n1\n5\n2\n9\n7\n13\n12\n6\n4\n\n\n\n\n\n\n\n\nns = agg_data['count'] # represents number of observation with corresponding x values\nks = agg_data['sum'] # represents successes, which means Victoria region\n\n\nfrom scipy.special import expit # inverse of logit\n\nx_values = agg_data.index # these are centered\n\nfor cept, slope in df_likeli.index:\n    probs = expit(cept + slope * x_values) # transformation to probabilities\n    likelis = st.binom.pmf(ks, ns, probs) # likelihood of each observation\n    df_likeli[cept, slope] = likelis.prod() # likelihood of the whole data under the selected pair of parameter values\n\n\ndf_likeli.head(6)\n\n-1.2  -1.400000    4.812632e-15\n      -1.393636    5.218338e-15\n      -1.387273    5.654017e-15\n      -1.380909    6.121460e-15\n      -1.374545    6.622520e-15\n      -1.368182    7.159117e-15\ndtype: float64\n\n\n\ndf_posterior = df_prior * df_likeli\ndf_posterior.head(6) # unnormalized\n\n-1.2  -1.400000    7.749685e-20\n      -1.393636    8.402985e-20\n      -1.387273    9.104551e-20\n      -1.380909    9.857265e-20\n      -1.374545    1.066411e-19\n      -1.368182    1.152818e-19\ndtype: float64\n\n\n\ndf_posterior = df_posterior / df_posterior.sum()\ndf_posterior.head() # normalized\n\n-1.2  -1.400000    8.913907e-09\n      -1.393636    9.665351e-09\n      -1.387273    1.047231e-08\n      -1.380909    1.133810e-08\n      -1.374545    1.226616e-08\ndtype: float64\n\n\n\nut.plot_contour(df_likeli.unstack())\nut.decorate(title = 'Likelihood')\n\n\n\n\n\n\n\n\n\nut.plot_contour(df_posterior.unstack())\nut.decorate(title = 'Posterior')\n\n\n\n\n\n\n\n\nIn Bayesian framework, the posterior distribution is a negotiation between the prior distribution and the likelihood: If the prior does not have much leverage (i.e., if it’s uninformative) then likelihood gets what it wants. In this case, since the prior was uniform, the likelihood dominated. Hence, both the likelihood and the posterior distribution are exactly the same.\n\nMARGINAL DISTRIBUTIONS\nNow, I sum the values for slope to get the marginal distribution. For example, to get the probability of slope being -1.2, I need to fix the slope to -1.2 and let the intercept vary and calculate the probabilities for every pair, and sum them up to get the total probability. Well, we do have the probabilities so only thing left to do is to sum them up.\n\nmarginal_slope = df_posterior.unstack().sum(axis=0)\nmarginal_slope.plot(label='Beta_1', color='C4')\n\nprint(round(marginal_slope.idxmax(), 3))\n\nut.decorate(\n    xlabel='Possibilities',\n    ylabel='PDF',\n    title = 'Posterior Distribution for Slope'\n)\n\n-0.706\n\n\n\n\n\n\n\n\n\nThe mean of the distribution is around -0.7, which means that 1 unit increase in tail length decreases the log(odds) in favor of a possum being from Victorian, by 0.7.\n\nmarginal_cept = df_posterior.unstack().sum(axis=1)\nmarginal_cept.plot(label='Beta_0', color='C3')\n\nprint(round(marginal_cept.idxmax(), 3))\n\nut.decorate(\n    xlabel='Possibilities',\n    ylabel='PDF',\n    title = 'Posterior Distribution for the Intercept'\n)\n\n-0.321\n\n\n\n\n\n\n\n\n\nOn the previous post, I had point estimates: Single pair of values for both parameters that maximizes the likelihood of the data. Now, instead of single pair, the result is distribution of possible values for each parameter.\nOne more trick before moving on: It is possible transform distributions to show probabilities. Let’s do it for the intercept, \\(\\beta_0\\).\n\nmarginal_cept.index = np.exp(marginal_cept.index) # getting rid of the log\nmarginal_cept.index = marginal_cept.index / (marginal_cept.index + 1) # turning odds into probabilities\n\nprint(round(marginal_cept.idxmax(), 3))\n\n0.42\n\n\nSo, the mean of the distribution is around 42%, standing for probability of Victorian possum when the tail length is 37 cm.\n\n\nPREDICTIONS\nSo, how to make predictions out of this?\nFirst, let’s put the posterior distribution to a pmf object.\n\nposterior_pmf = emd.Pmf(df_posterior)\nposterior_pmf.head()\n\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n-1.2\n-1.400000\n8.913907e-09\n\n\n-1.393636\n9.665351e-09\n\n\n-1.387273\n1.047231e-08\n\n\n\n\n\n\n\nPmf object offers “choice” method that allows to sample from the distribution. I’ll sample pairs from the joint distribution.\n\nsample = posterior_pmf.choice(201)\n\n\nmin_tail_length = df['tail_l'].min() - 1\nmax_tail_length = df['tail_l'].max() + 1\n\nCreating values to make predictions for.\n\nxs = np.array([num-offset for num in np.arange(min_tail_length, max_tail_length, 0.5)])\n\n\npred = np.empty((len(sample), len(xs)))\n\nfor i, (cept, slope) in enumerate(sample):\n    odds = np.exp(cept + slope * xs)\n    pred[i] = odds / (odds + 1)\n\nOkay, ended up with a list of predictions for each x under sampled model parameters. Let’s put them into a graph to observe most likely values at different tail lengths.\n\nfor ps in pred:\n    plt.plot(xs+offset, ps, color='C4', lw=0.5, alpha=0.3)\n\nplt.scatter(df['x']+offset, df['y'], s=30)\nut.decorate()\n\n\n\n\n\n\n\n\n\nlow, median, high = np.percentile(pred, [5, 50, 95], axis=0)\n\n\nplt.fill_between(xs+offset, low, high, color='C4', alpha=0.2, label='90% Credible Interval')\nplt.plot(xs+offset, median, color='C4', label='model')\nplt.scatter(df['x']+offset, df['y'], s=30)\n\nplt.yticks([val/10 for val in np.arange(0, 11)])\n\nut.decorate()\n\n\n\n\n\n\n\n\nDon’t worry about credible intervals / highest density intervals (HDIs), I think about talking about them in a different post.\nLet’s make a prediction: For example, what’s the probability of Victorian if the tail length is 33 cm?\n\nlow = pd.Series(low, xs+offset)\nmedian = pd.Series(median, xs+offset)\nhigh = pd.Series(high, xs+offset)\n\n\ntail_l = 33\nprint(\n    round(median[tail_l], 3),\n    (round(low[tail_l], 3), round(high[tail_l], 3))\n)\n\n0.929 (0.835, 0.976)\n\n\nThe probability of Victorian given the tail length of 33 cm is at least 83.5%.\nThat’s pretty much it. I posted this blog as a follow-up to the previous one, hoping to make the relation between likelihood and posterior apparent. If you enjoyed it, you may consider subscribing to the email list.\nHave a nice weekend!"
  },
  {
    "objectID": "posts/Basketball Analytics and Daily Job/index.html",
    "href": "posts/Basketball Analytics and Daily Job/index.html",
    "title": "How Basketball Analytics May Have An Effect on Your Analytics Job",
    "section": "",
    "text": "Excerpt from the Q&A done by The F5 with Mike Beouy\n\n\n\nI don’t know if you have watched Moneyball (if you haven’t, you should), there are great monologues (mainly from Jonah Hill’s character) that point out the main purpose of sports analytics:\n\nPeople who run ball clubs, they think in terms of buying players. Your goal shouldn’t be to buy players, your goal should be to buy wins.\n\n\n… This is building in all the intelligence that we have to project players. It’s about getting things down to one number.\n\nYes, the holy grail would be a single number that summarizes a player’s contribution to the team. This problem is similar to questions that you may end up trying to find an answer for, during your day to day job as a data scientist, because at the end of the day that’s what everyone’s trying to do: Representing a customer (or an employee etc.) by a number.\n\nProblem of Dividing Credit\nIn its core, problem of assigning a number that represents a player’s contribution to the team (when he’s on the court) is about dividing credit among players: We scored 30 points with these 5 players, allowed 20. How should I distribute the net rating of 10 among these 5 players?\nProblem of dividing credit comes up when you’re a data scientist in a company where you try to figure out how much each product contributes to the revenue of a customer: You just replace the players with the products and tie it to revenue instead of points, and it’s essentially the same problem.\n\n\nFuture Projection\nNot every player is the same when they are drafted to the NBA: Some players have a very high ceiling, and it might be wise to keep them. This type of career projection is relevant in a company setting where it might be beneficial to project customers: Not every customer is the same, since some of them hold more potential in terms of revenue generation. It is useful to identify them for retention purposes and campaign targeting.\n\n\nSegmentation\nClustering players based on their effectivess and style of play serves various purposes that ranges from identifying which players are more fit to certain play types to leading the transfer negotiations (e.g., finding similar players to replace the player that left).\n\nIn summary, trying to solve sport analytics problems may aid the problem solving processes in your daily job (for example, in a banking setting), and you may end up making a difference by adapting approaches that you have learned in sports analytics to your company setting.\nJust in case you don’t want to take my word, take the words from Mike Beuoy (the mind behind the Inpredictable). If you’re interested in basketball analytics, you should definitely subscribe to the F5, where the excerpt above is taken from."
  },
  {
    "objectID": "posts/Understanding distribution of log(odds ratio) through simulation/index.html",
    "href": "posts/Understanding distribution of log(odds ratio) through simulation/index.html",
    "title": "Understanding Distribution of log(odds ratio) Through Simulation",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\nimport seaborn as sns\n\nimport scipy.stats as st\n\nI wanted to put this example out because one of my colleague asked me a question about result table of logistic regression. Back in the day while I was watching the video, Josh Starmer said distribution of log(odds ratio) is approximately normal and gave an idea of how to simulate. I didn’t end up his way of simulating, if you want that maybe you should follow the approach in the video but we actually both do the same thing.\nLet’s say we have the following information:\n\ncancer_gene_cb = pd.crosstab(['yes', 'no'], ['yes', 'no'], rownames=['mutated_gene'], colnames=['has_cancer'])\ncancer_gene_cb.loc['yes', 'yes'] = 23\ncancer_gene_cb.loc['yes', 'no'] = 117\ncancer_gene_cb.loc['no', 'no'] = 210\ncancer_gene_cb.loc['no', 'yes'] = 6\n\ncancer_gene_cb\n\n\n\n\n\n\n\nhas_cancer\nno\nyes\n\n\nmutated_gene\n\n\n\n\n\n\nno\n210\n6\n\n\nyes\n117\n23\n\n\n\n\n\n\n\nLet’s represent the information in a different way. For the ones who are not familiar, odds is ratio of something happening to something not happening. When there’s odds ratio, it actually means ratio of odds.\n\n# Given the mutated version of the gene, odds of having cancer\nmutated_gene_odds = cancer_gene_cb.loc['yes', 'yes'] / cancer_gene_cb.loc['yes', 'no'] # 23 / 117\n\n# Given the wild version of the gene, odds of having cancer\nwild_gene_odds = cancer_gene_cb.loc['no', 'yes'] / cancer_gene_cb.loc['no', 'no'] # 6 / 210\n\nodds_ratio = mutated_gene_odds / wild_gene_odds # 6.88\n\nlog_odds_ratio = round(np.log(odds_ratio), 2)\nprint(log_odds_ratio)\n\n1.93\n\n\nIt seems like people who have the mutated gene have higher odds (i.e., 6.88 times more likely) in terms of having cancer. But I wonder if this is statistically significant.\nSo, I want to do a simulation see how often would I see the log(odds ratio) of 1.93 under the assumption of no relationship between the two variables. If I don’t see it very often, it makes sense to act as if there’s a difference.\n\nLet’s go through a simulation:\n\nli = [] # creating an empty bag\nsample_size = 356 # our sample size\n\nCreating 29 “cancer” written cards and putting them in a bag, representing the number of people with cancer in our sample.\n\nfor i in range(0, 29):\n    li.append('cancer')\n\nCreating 327 “no cancer” written cards and putting them in a bag, representing the number of people without cancer in our sample.\n\nfor j in range(0, 327):\n    li.append('no cancer')\n\n\nmutated_version = []\nwild_version = []\n\nThere are 140 people with mutated variant of the gene, 216 people with wild variant. Under the assumption of no relationship between the cancer and the gene variant, I’d expect no difference between odds of cancer given the mutated gene and given the wild gene. In other words, knowing the gene variant does not provide any meaningful information.\n\nfor i in range(10000):\n    sampl_all = np.random.choice(li, size=len(li)) #  Shuffling the bag (cards)\n\n    #  Taking 140 of them (representing the number of participants with mutated gene)\n    sampl_mutated_version = sampl_all[:140].tolist()\n    # and calculating the odds in favor of cancer in the mutated gene group\n    mutated_version.append(sampl_mutated_version.count('cancer') / sampl_mutated_version.count('no cancer')) \n\n    #  Taking rest of the cards (representing the number of participants in the non-mutated gene group)\n    sampl_wild_version = sampl_all[140:].tolist() \n    #  and calculating the odds in favor of cancer in the non-mutated gene group\n    wild_version.append(sampl_wild_version.count('cancer') / sampl_wild_version.count('no cancer'))\n\n\nmutated_version_nd = np.array(mutated_version)\nwild_version_nd = np.array(wild_version)\n\n\nodds_ratio = mutated_version_nd / wild_version_nd #  Calculating odds ratio\n\n\nlog_odds_ratio = np.log(odds_ratio) #  Calculating log(odds ratio)\n\n\n#  Plotting the distribution of log(odds ratio)\n\nfig = plt.figure(figsize=(8, 6))\ng = sns.histplot(data=log_odds_ratio, bins=40, color=\"orange\", edgecolor=\"black\")\n\ng.xaxis.set_minor_locator(AutoMinorLocator())\n\ng.tick_params(which=\"both\", width=2)\ng.tick_params(which=\"major\", length=7)\ng.tick_params(which=\"minor\", length=4)\n\nplt.show(g)\n\n\n\n\n\n\n\n\n\nprint(round(np.mean(log_odds_ratio), 2))\nprint(round(np.std(log_odds_ratio), 2))\n\n-0.02\n0.42\n\n\nApparently it can be approximated by normal distribution. It is centered around zero with standard deviation of 0.42. Let’s see how often I would observe a result as or more extreme as 1.93.\n\nlen([val for val in log_odds_ratio if abs(val) &gt; 1.93]) / len(log_odds_ratio)\n\n0.0002\n\n\nSince I simulate the whole thing under the assumption of no difference between groups, selected alpha level should correspond to my long-term type I error rate. If alpha = 0.05, I should observe 5% values to fall more than 2 standard deviation (approximately). Let’s check if it’s the case.\n\nlen([val for val in log_odds_ratio if abs(val) &gt; 2 * np.std(log_odds_ratio)]) / len(log_odds_ratio)\n\n0.0479\n\n\nI really tried to simplify things here, to not make a huge mess out of this but another approach would be to fit a theoretical distribution, such as normal distribution, since that’s what the Wald Test does and then step to integrating over the area that fall larger than absolute value of 1.93 . It also requires a few extra steps, such as calculating estimated standard deviation. For the ones who would like to play with the idea, here’s the video link again.\nHave a nice weekend."
  },
  {
    "objectID": "posts/Model Fitting with Likelihood/index.html",
    "href": "posts/Model Fitting with Likelihood/index.html",
    "title": "Understanding Model Fitting with Likelihood",
    "section": "",
    "text": "In daily life, “probability” and “likelihood” are used interchangeably. But, they don’t refer to the same thing. I believe the most intuitive way to distinguish both is by thinking about the information you have while answering questions:\n\nProbability questions start with known model and unknown data. If someone asks you about the chance of seeing 6 heads in 10 coin tosses, you don’t have any observations. But, you have the model: You know the process that generates “heads” and “tails” and it’s parameter value (usually expressed as theta θ): 0.5.\nLet’s say you did 10 coin tosses and ended up with 9 heads. How likely you were to observe 9 if the coin was fair? In this situation, the data is known and the question refers to the underlying data generating process (i.e., refers to the parameter).\n\nWe can use this type of approach and let the parameter values vary and calculate the likelihood of the data under each value of the parameter(s).\n\nimport numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport statsmodels as sm\nimport empiricaldist as emd\nimport scipy.stats as st\n\nimport utils as ut\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n\npossum = pd.read_csv('possum.csv', usecols=['pop', 'tail_l'])\ndf = possum.copy()\n\ndf.head()\n\n\n\n\n\n\n\n\npop\ntail_l\n\n\n\n\n0\nVic\n36.0\n\n\n1\nVic\n36.5\n\n\n2\nVic\n39.0\n\n\n3\nVic\n38.0\n\n\n4\nVic\n36.0\n\n\n\n\n\n\n\nWe’ll use possum data, where each row corresponds to features of a possum from Australia and New Guinea. Let’s attempt to predict which region a possum is from as a function of tail length. (You can get the data from here)\nI assume basic knowledge of logistic regression, which we’ll use to model the relationship between our predictor and the binary target variable.\n\nqs_cept = np.linspace(-3, 3, 241) # possible values for intercept (beta_0)\nqs_slope = np.linspace(-1, 1, 81) # possible values for slope (beta_1)\n\n\nprint(qs_cept[:5], '\\n', qs_slope[:5])\n\n[-3.    -2.975 -2.95  -2.925 -2.9  ] \n [-1.    -0.975 -0.95  -0.925 -0.9  ]\n\n\n\ndf_likeli = pd.DataFrame(index=range(len(qs_cept)), columns=range(len(qs_slope)))\ndf_likeli.index = qs_cept\ndf_likeli.columns = qs_slope\n\n\ndf_likeli.fillna(1, inplace=True)\n\ndf_likeli.head()\n\n\n\n\n\n\n\n\n-1.000\n-0.975\n-0.950\n-0.925\n-0.900\n-0.875\n-0.850\n-0.825\n-0.800\n-0.775\n...\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\n\n\n\n\n-3.000\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n-2.975\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n-2.950\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n-2.925\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n-2.900\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n5 rows × 81 columns\n\n\n\nSo, what I have done is I created grid of equally spaced values both for intercept and the slope. I put them into a dataframe where columns represent possible values for slope and indices represent possible values for intercept.\nNow, I ask this question: For each pair of intercept and slope, b0 and b1, how likely I am to see the observed data?\n\ndf['pop'] = df['pop'].apply(lambda x: 1 if x == 'Vic' else 0)\n\n\n# Centering data\n\noffset = df['tail_l'].mean().round()\ndf['x'] = df['tail_l'] - offset\ndf['y'] = df['pop']\n\n# I refer our predictor as x from now on (for convenience), y becomes our target variable which takes 1 if possum is from Victoria region and 0 otherwise.\n\n\nagg_data = df.groupby('x')['y'].agg(['sum', 'count'])\nagg_data.head(10).T\n\n\n\n\n\n\n\nx\n-5.0\n-3.5\n-3.0\n-2.5\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n0.5\n\n\n\n\nsum\n2\n1\n4\n2\n6\n6\n9\n4\n4\n1\n\n\ncount\n2\n1\n5\n2\n9\n7\n13\n12\n6\n4\n\n\n\n\n\n\n\n\nns = agg_data['count'] # represents number of observation with corresponding x values\nks = agg_data['sum'] # represents successes, which means Victoria region\n\nPeople get confused, rightfully, when they hear logistic regression getting mentioned as a linear model since all they see is an S-shaped function in a classic graph where x-axis represents the predictor and y-axis represents the probability. But that squiggle is a result of a transformation of log odds into probabilities. You may have deduced that before if you have taken a look at the equation:\n\\(\\displaystyle log_e(\\frac{p_i}{1 - p_i}) = \\beta_0 + \\beta_1 x_{1i} + ... + \\beta_k x_{k_i}\\)\nHowever, we know that odds can be expressed as probabilities, so we’ll make that transformation.\nFor representation let’s select an intercept and a slope:\n\ncept = 0\nslope = 1\n\n\nx_values = agg_data.index\nlog_odds = cept + slope * x_values\nodds = np.exp(log_odds)\nps = odds / (odds + 1)\nps[:6] # Probabilities coming from the model\n\nIndex([0.006692850924284856, 0.029312230751356316,  0.04742587317756679,\n        0.07585818002124356,  0.11920292202211755,  0.18242552380635632],\n      dtype='float64', name='x')\n\n\nHow likely I am to observe k success in n trials with \\(p_i\\) where \\(p_i\\) comes from the model above.\n\n# How likely I am to observe k success in n trials with p_i where p_i comes from model\nlikelis = st.binom.pmf(ks, ns, ps)\nlikelis[:6]\n\narray([4.47942535e-05, 2.93122308e-02, 2.40951774e-05, 5.75446348e-03,\n       1.64675234e-04, 2.10930303e-04])\n\n\n\n# Taking the product for whole data\nlikelis.prod()\n\n6.58621661515704e-55\n\n\nLet’s take a look at the fit:\n\nplt.figure(figsize=(6,4))\n\nplt.plot(x_values+offset, ps, label='random selected model', color='C1')\nplt.scatter(df['x']+offset, df['y'], s=30, label='data')\n\nut.decorate()\n\n\n\n\n\n\n\n\nThis was for only one pair of intercept and slope, and it seems like we can do better. We calculated the likelihood for educational purposes here but in general, likelihoods by themselves does not mean much (I try to choose my words carefully here). Let’s try other possible pairs for our parameters and compare the likelihoods of each to take the one that maximizes the likelihood of the observed data.\n\nfrom scipy.special import expit\n\nlikelihood = df_likeli.copy()\n\nfor cept in likelihood.index:\n    for slope in likelihood.columns:\n        probs = expit(cept + slope * x_values) # transformation to probabilities\n        likelis = st.binom.pmf(ks, ns, probs) # likelihood of each observation\n        likelihood.loc[likelihood.index == cept, slope] = likelis.prod() # likelihood of the whole data under the selected pair of parameter values\n\n\nrow, col = likelihood.stack().idxmax()\nprint(row, col)\n\n-0.32499999999999973 -0.7\n\n\nThis is the pair that maximizes the likelihood.\n\ncept = row\nslope = col\n\nlog_odds = cept + slope * x_values\nodds = np.exp(log_odds)\nps = odds / (odds + 1)\n\n\nplt.figure(figsize=(6,4))\n\nplt.plot(x_values+offset, ps, label='model', color='C1')\nplt.scatter(df['x']+offset, df['y'], s=30, label='data')\n\nut.decorate()\n\n\n\n\n\n\n\n\nLet’s check our parameter estimates with the help of statsmodels:\n\nimport statsmodels.formula.api as smf\n\nformula = 'y ~ x'\nresults = smf.logit(formula, data=df).fit(disp=False)\nresults.params\n\nIntercept   -0.320498\nx           -0.699641\ndtype: float64\n\n\nWe didn’t get those exact values since we used grid approximation, and the grid that I created didn’t include those exact values. Let’s drop a few details here:\n\nAs the parameter space gets larger, the computational process becomes slower. So, there’s that trade-off between precision and time.\nWe only had two parameters to estimate and that’s OK for grid approach. However, once you start to get more than three, number of possible combination for parameters gets incredibly large which makes grid approach infeasible.\nAnother thing is related to the number of observation while calculating the likelihood for the whole data: When you try to take the product of the likelihoods with many observations, you may ran into an issue called underflow, where computer has trouble multiplying bunch of values around zero. Hence, log likelihood comes to rescue: Logarithms takes the values around zero away from zero, thus solving the underflow problem.\n\nI hope this example made likelihoods more intuitive. There’s more to know about them: Are they probability densities? Are they probability masses? Or are they both? That’s for a whole different post, things can get messy in a second. However, it’s important to understand likelihoods in the context above, since they can be used to update our beliefs about parameters which is what we do with Bayesian methods.\nAs usual, have a nice weekend :)"
  },
  {
    "objectID": "posts/Man on a Mission - January 2025/index.html",
    "href": "posts/Man on a Mission - January 2025/index.html",
    "title": "Man on a Mission: January 2025",
    "section": "",
    "text": "As I promised at the start of the year, here’s what I have studied this month.\n\n\nStatistics\nThis month I finished Think Bayes way before than I anticipated, and played with PyMC a bit. I really enjoyed the whole book but wanted to dive a bit deep so moved on to John Kruschke’s Doing Bayesian Data Analysis and I’m glad I did it. The book starts with main ideas of Bayesian view, “Bayesian inference is reallocation of credibility across possibilities” where credibility stands for probability and possibilities stand for possible values that our parameters can take when we attempt to model. After this introduction, Kruschke talks about what probabilities are and provides intuitive description for the distinction between probability mass and probability density. If you have trouble understanding probability distributions, you may want to take a look at it.\nBayesian stuff aside, I really enjoyed the interpretation of mean as minimized variance alongside how we end up with median and mode when we change the distance measure (e.g., l2 norm to l1 etc.). Also, I coded Metropolis Algorithm (1-D) by using the example in the MCMC chapter. You can find it on my GitHub under learn-stats-with-sims.\nI’m currently at chapter 9, going through hierarchical models. We’ll see if the book keeps me engaged throughout the upcoming chapters as well.\n\n\n\nDeep Learning (DL)\nWell, I haven’t done much for the DL. Throughout the year, amount of time I spent on statistics will gradually decline to open up space for DL. I’ve just started to Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, skimming through Scikit-Learn part to start with Keras and Tensorflow while not missing out on anything (despite using sklearn pretty often, I don’t want to miss a useful thing that may come in handy in the future).\n\nIn terms of sports analytics or production level data science, I haven’t done more than reading a few blog posts. Data science stuff aside, I have improved a lot on the website, added new functionalities such as widget to subscribe to the email list which allows you to get notified when I write a new post. It took quite some time to handle the front-end, so I count that as a win as well.\nSee you next month!"
  },
  {
    "objectID": "posts/p-value distribution/index.html",
    "href": "posts/p-value distribution/index.html",
    "title": "Which p-values to expect under different realities?",
    "section": "",
    "text": "Statistics classes in social sciences, during an undergraduate degree, revolve around p-values mainly. Despite that, I have never seen the mention of p-value distributions. So, here we go:\n\nimport numpy as np\nimport scipy.stats as st\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nHow does the p-value distribution look like when there is no effect?\n\np_val_list = [] # a bag to hold p-values\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51) # sampling from a normal distribution with mean 100, std 15\n    trt = np.random.normal(loc=100, scale=15, size=51) # sampling from a normal distribution with mean 100, std 15\n    p_val = st.ttest_ind(trt, ctrl)[1]  # doing a t-test and grabbing the p-value\n\n    p_val_list.append(p_val) # storing the p-value in the bag\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('When the true effect size = 0')\n\nText(0.5, 1.0, 'When the true effect size = 0')\n\n\n\n\n\n\n\n\n\nEvery p-value is equally likely. That’s why chosen alpha level corresponds to how often you will fool yourself in the long run, when there is no effect.\nIf the chosen alpha level is .10, then under the null hypothesis 10 percent of the p-values fall below 0.10.\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.05\n\n\nThis is our long-term type I error rate, since theoretically the power is undefined in this case (i.e., the null is the truth). We don’t expect to fool ourselves more than 5% in the long run, when there is no effect.\nLet’s see what happens when there is an effect:\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=104.5, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.3')\n\nText(0.5, 1.0, \"For Cohen's d of 0.3\")\n\n\n\n\n\n\n\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.32\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.3')\n\nText(0.5, 1.0, \"For Cohen's d of 0.3\")\n\n\n\n\n\n\n\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=107.5, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--', alpha=.5)\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.71\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=109.5, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--', alpha=.5)\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.7')\n\nText(0.5, 1.0, \"For Cohen's d of 0.7\")\n\n\n\n\n\n\n\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.89\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.7')\n\nText(0.5, 1.0, \"For Cohen's d of 0.7\")\n\n\n\n\n\n\n\n\n\n\nprint(len([val for val in p_val_list if val &gt;= .00 and val &lt;= .01]) / len(p_val_list))\n\n0.715716\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=112, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xticks(np.arange(0, 1, 0.05))\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.8')\n\nText(0.5, 1.0, \"For Cohen's d of 0.8\")\n\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.8')\n\nText(0.5, 1.0, \"For Cohen's d of 0.8\")\n\n\n\n\n\n\n\n\n\n\nprint(len([val for val in p_val_list if val &gt;= .04 and val &lt;= .05]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .03 and val &lt;= .04]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .02 and val &lt;= .03]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .01 and val &lt;= .02]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .00 and val &lt;= .01]) / len(p_val_list))\n\n0.005389 \n 0.008187 \n 0.014401 \n 0.032377 \n 0.91928\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.98\n\n\nAs the statistical power increases, distribution of p-values pile up at the very left: Some p-values below 0.05 become more likely (ones more close to 0.00). And when you have very high power, certain p-values below 0.05 (relatively high ones) become more likely under the null:\nHence, wouldn’t be wise to reject the null despite p-value &lt; .05\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=65) # different sample size\n    trt = np.random.normal(loc=107.5, scale=15, size=65) # different sample size\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.81\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xticks(np.arange(0, 1, 0.05))\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=100) # different sample size\n    trt = np.random.normal(loc=107.5, scale=15, size=100) # different sample size\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.94\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xticks(np.arange(0, 1, 0.05))\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\nAs one can see, statistical power also depends on the sample size and it should make sense: p-value is a function of test statistic (t, in this case) and following from there, function of sample size. If this doesn’t feel comfortable to you, I suggest you to take a look at this post, and check what happens to standard error as the sample size increases.\nIn the examples below, we always knew the underlying distributions of the populations we sample from. In reality, we usually end up with a result and we wonder the true effect: What is the chance that there is an effect, given that I observed one. This conditional probability is called positive predictive value, something that I heard from a political scientist during pre-COVID days, and if you’re interested in these stuff I suggest you to take a look at it. You can start from Ioannidis’ paper or Lakens’ book for more structured follow."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "YIGIT ASIK, Data Scientist",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow Basketball Analytics May Have An Effect on Your Analytics Job\n\n\n\n\n\n\nSports Analytics\n\n\n2-cents\n\n\n\n\n\n\n\n\n\nFeb 5, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Way - Predictions Out of Posterior Distribution\n\n\n\n\n\n\nStats\n\n\nBayesian\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nMan on a Mission: January 2025\n\n\n\n\n\n\nupdate\n\n\n\n\n\n\n\n\n\nJan 31, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Model Fitting with Likelihood\n\n\n\n\n\n\nStats\n\n\nBayesian\n\n\n\n\n\n\n\n\n\nJan 25, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Distribution of log(odds ratio) Through Simulation\n\n\n\n\n\n\nStats\n\n\nInference\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWhich p-values to expect under different realities?\n\n\n\n\n\n\nStats\n\n\nInference\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Updating: Poisson & Gamma\n\n\n\n\n\n\nStats\n\n\nBayesian\n\n\nSports Analytics\n\n\n\n\n\n\n\n\n\nJan 10, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nAn Intro Example to Bayes’ Theorem\n\n\n\n\n\n\nStats\n\n\nprobability\n\n\nBayesian\n\n\n\n\n\n\n\n\n\nJan 5, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nMan on a Mission: Starting 2025\n\n\n\n\n\n\nupdate\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Happens to Sample Statistics?\n\n\n\n\n\n\nStats\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 28, 2024\n\n\nYiğit Aşık\n\n\n\n\n\n\nNo matching items"
  }
]
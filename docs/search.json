[
  {
    "objectID": "posts/what happens to sample statistics/index.html",
    "href": "posts/what happens to sample statistics/index.html",
    "title": "What Happens to Sample Statistics?",
    "section": "",
    "text": "Let’s start from the basics: The idea is to gather data to make an inference about the population. We use what we know (sample data) to estimate what we don’t (population).\nSo, let’s see what happens as one collects more data.\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nfor n in range(10, 101, 10):\n    sampled = np.random.normal(loc=100, scale=15, size=n)\n    print('Sampling ' + str(n) + ' observations')\n    print('Mean: ' + str(np.mean(sampled)))\n    print('Standard Deviation ' + str(np.std(sampled)))\n    print('\\n')\n\nSampling 10 observations\nMean: 96.54930308046485\nStandard Deviation 17.69744525686926\n\n\nSampling 20 observations\nMean: 95.26529645787349\nStandard Deviation 13.557714188035797\n\n\nSampling 30 observations\nMean: 99.59631039122944\nStandard Deviation 16.67387851610786\n\n\nSampling 40 observations\nMean: 94.76579341950723\nStandard Deviation 18.06257861698366\n\n\nSampling 50 observations\nMean: 99.61537083458882\nStandard Deviation 14.897878602988285\n\n\nSampling 60 observations\nMean: 98.68203398707892\nStandard Deviation 14.644083079076118\n\n\nSampling 70 observations\nMean: 100.36682454658889\nStandard Deviation 15.427573719883997\n\n\nSampling 80 observations\nMean: 101.00471630426532\nStandard Deviation 12.678064261146886\n\n\nSampling 90 observations\nMean: 102.7631649616841\nStandard Deviation 13.03915710302493\n\n\nSampling 100 observations\nMean: 99.17823535292733\nStandard Deviation 14.951583438089918\n\n\n\n\nAs one increases the sample size taken from the population, sample statistics will approach towards the population parameter.\nDOES NOT NECESSARILY DECREASE! (do not confuse std_dev and std_err) As one can see from the example above!\nBut standard error WILL decrease as the sample size increases. It should make sense intuitively: I have more confidence in my estimates if I know more.\n\nfor n in range(10, 101, 10):\n    sampled = np.random.normal(loc=100, scale=15, size=n)\n    print('Sampling ' + str(n) + ' observations')\n    std_err = np.std(sampled) / np.sqrt(n)\n    print('Standard error approximation: ' + str(std_err))\n    print('\\n')\n\nSampling 10 observations\nStandard error approximation: 4.452823707231046\n\n\nSampling 20 observations\nStandard error approximation: 3.798915557039333\n\n\nSampling 30 observations\nStandard error approximation: 2.3932757697459746\n\n\nSampling 40 observations\nStandard error approximation: 2.624220779126406\n\n\nSampling 50 observations\nStandard error approximation: 2.2840746792281217\n\n\nSampling 60 observations\nStandard error approximation: 2.015124706208566\n\n\nSampling 70 observations\nStandard error approximation: 1.7230901229016737\n\n\nSampling 80 observations\nStandard error approximation: 1.5993946754011168\n\n\nSampling 90 observations\nStandard error approximation: 1.66349862578967\n\n\nSampling 100 observations\nStandard error approximation: 1.577820207327762\n\n\n\n\nHow close is the approximation? Let’s try it for one sample\n\nn = 51\n\npop = np.random.normal(loc=100, scale=15, size=300000) # Population with normal distribution(mean=100, sd=15)\n\nsampled = np.random.choice(pop, size=n) # randomly sampling\nestimated_mean = np.mean(sampled) # sample mean\nestimated_sd = np.std(sampled) # sample standard deviation\n\nestimated_std_err = np.std(sampled) / n**.5 # estimated standard error, expected variation for my sample statistic.\n\nprint(estimated_mean, estimated_sd, estimated_std_err)\n\n95.27311481976386 15.960503088053418 2.2349174605268747\n\n\n\n# Let's take many samples and estimate the mean\n\nmean_estimates = []\n\nfor i in range(1000): # Let's do it 1000 times, sampling 51 in each iteration.\n    sampled = np.random.choice(pop, size=n)\n    mean_estimates.append(np.mean(sampled))\n\nnp.std(mean_estimates)\n\n2.1480690130242537\n\n\nAs one can see, it’s not that far away.\n\nplt.figure(figsize=(10,6))\n\ng = sns.swarmplot(data=mean_estimates, orient=\"h\", size=6, alpha=.8, color=\"purple\", linewidth=0.5,\n                 edgecolor=\"black\")\n\n\n\n\n\n\n\n\nWhat happens when one lowers the sample size? More variation, less confidence. As the sample size increases the estimates approach towards the parameter, so with large sample size each sample ends up having similar estimates. However, that’s not the case with low sample size.\n\nn = 16\nmean_estimates = []\n\nfor i in range(1000): # Let's do it 1000 times\n    sampled = np.random.choice(pop, size=n)\n    mean_estimates.append(np.mean(sampled))\n\nnp.std(mean_estimates)\n\n3.713500386895356\n\n\n\nplt.figure(figsize=(10,6))\n\ng = sns.swarmplot(data=mean_estimates, orient=\"h\", size=6, alpha=.8, color=\"purple\", linewidth=0.5,\n                 edgecolor=\"black\")\n\n\n\n\n\n\n\n\nWatch out the x-axis, it’s much wider now.\n\nmean_estimates = {\n        16:[],\n        23:[],\n        30:[],\n        51:[],\n        84:[],\n        101:[]\n    }\n\nfor n in [16, 23, 30, 51, 84, 101]:\n    for i in range(500):\n        sampled = np.random.choice(pop, size=n)\n        mean_estimates[n].append(np.mean(sampled))\n\n\nfor key in mean_estimates.keys():\n    print('Sample size: ' + str(key))\n    print('Standard deviation (std_err) around the estimates: ' + str(np.std(mean_estimates[key])))\n    print('\\n')\n\nSample size: 16\nStandard deviation (std_err) around the estimates: 3.855337925425897\n\n\nSample size: 23\nStandard deviation (std_err) around the estimates: 3.1917223557725363\n\n\nSample size: 30\nStandard deviation (std_err) around the estimates: 2.7639204989190023\n\n\nSample size: 51\nStandard deviation (std_err) around the estimates: 2.0366022769176806\n\n\nSample size: 84\nStandard deviation (std_err) around the estimates: 1.736807903528174\n\n\nSample size: 101\nStandard deviation (std_err) around the estimates: 1.504399688755003"
  },
  {
    "objectID": "posts/p-value distribution/index.html",
    "href": "posts/p-value distribution/index.html",
    "title": "Which p-values to expect under different realities?",
    "section": "",
    "text": "Statistics classes in social sciences, during an undergraduate degree, revolve around p-values mainly. Despite that, I have never seen the mention of p-value distributions. So, here we go:\n\nimport numpy as np\nimport scipy.stats as st\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nHow does the p-value distribution look like when there is no effect?\n\np_val_list = [] # a bag to hold p-values\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51) # sampling from a normal distribution with mean 100, std 15\n    trt = np.random.normal(loc=100, scale=15, size=51) # sampling from a normal distribution with mean 100, std 15\n    p_val = st.ttest_ind(trt, ctrl)[1]  # doing a t-test and grabbing the p-value\n\n    p_val_list.append(p_val) # storing the p-value in the bag\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('When the true effect size = 0')\n\nText(0.5, 1.0, 'When the true effect size = 0')\n\n\n\n\n\n\n\n\n\nEvery p-value is equally likely. That’s why chosen alpha level corresponds to how often you will fool yourself in the long run, when there is no effect.\nIf the chosen alpha level is .10, then under the null hypothesis 10 percent of the p-values fall below 0.10.\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.05\n\n\nThis is our long-term type I error rate, since theoretically the power is undefined in this case (i.e., the null is the truth). We don’t expect to fool ourselves more than 5% in the long run, when there is no effect.\nLet’s see what happens when there is an effect:\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=104.5, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.3')\n\nText(0.5, 1.0, \"For Cohen's d of 0.3\")\n\n\n\n\n\n\n\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.32\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.3')\n\nText(0.5, 1.0, \"For Cohen's d of 0.3\")\n\n\n\n\n\n\n\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=107.5, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--', alpha=.5)\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.71\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=109.5, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--', alpha=.5)\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.7')\n\nText(0.5, 1.0, \"For Cohen's d of 0.7\")\n\n\n\n\n\n\n\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.89\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.7')\n\nText(0.5, 1.0, \"For Cohen's d of 0.7\")\n\n\n\n\n\n\n\n\n\n\nprint(len([val for val in p_val_list if val &gt;= .00 and val &lt;= .01]) / len(p_val_list))\n\n0.715716\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=112, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xticks(np.arange(0, 1, 0.05))\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.8')\n\nText(0.5, 1.0, \"For Cohen's d of 0.8\")\n\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.8')\n\nText(0.5, 1.0, \"For Cohen's d of 0.8\")\n\n\n\n\n\n\n\n\n\n\nprint(len([val for val in p_val_list if val &gt;= .04 and val &lt;= .05]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .03 and val &lt;= .04]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .02 and val &lt;= .03]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .01 and val &lt;= .02]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .00 and val &lt;= .01]) / len(p_val_list))\n\n0.005389 \n 0.008187 \n 0.014401 \n 0.032377 \n 0.91928\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.98\n\n\nAs the statistical power increases, distribution of p-values pile up at the very left: Some p-values below 0.05 become more likely (ones more close to 0.00). And when you have very high power, certain p-values below 0.05 (relatively high ones) become more likely under the null:\nHence, wouldn’t be wise to reject the null despite p-value &lt; .05\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=65) # different sample size\n    trt = np.random.normal(loc=107.5, scale=15, size=65) # different sample size\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.81\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xticks(np.arange(0, 1, 0.05))\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=100) # different sample size\n    trt = np.random.normal(loc=107.5, scale=15, size=100) # different sample size\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.94\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xticks(np.arange(0, 1, 0.05))\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\nAs one can see, statistical power also depends on the sample size and it should make sense: p-value is a function of test statistic (t, in this case) and following from there, function of sample size. If this doesn’t feel comfortable to you, I suggest you to take a look at this post, and check what happens to standard error as the sample size increases.\nIn the examples below, we always knew the underlying distributions of the populations we sample from. In reality, we usually end up with a result and we wonder the true effect: What is the chance that there is an effect, given that I observed one. This conditional probability is called positive predictive value, something that I heard from a political scientist during pre-COVID days, and if you’re interested in these stuff I suggest you to take a look at it. You can start from Ioannidis’ paper or Lakens’ book for more structured follow."
  },
  {
    "objectID": "posts/Bayesian Updating with Poisson & Gamma/index.html",
    "href": "posts/Bayesian Updating with Poisson & Gamma/index.html",
    "title": "Bayesian Updating: Poisson & Gamma",
    "section": "",
    "text": "The following example is taken from Allen Downey’s Think Bayes. I believe this is a great one to show how to update priors.\nDowney takes a match between France and Crotia, played back in 2018 World Cup, that France won 4-2. Then, he aims to answer two questions:\n\nHow confident we are about France being the better team?\nIn a rematch, what is the probability that France would win again?\n\nI’ll only attempt to answer the first question so that you have a reason to check Downey’s book Think Bayes.\n\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport empiricaldist as emd\nimport scipy.stats as st\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 170)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nStarting out with certain assumptions:\n\nThere is a goal scoring-rate for every team, goals per game (more specifically, per 90), which we will denote as lambda.\nA goal is equally likely during any minute of the game, regardless of strategy, tempo etc.\n(This is also an assumption under a huge framework in basketball analytics, regularized adjusted plus-minus, wanted to point out just in case you follow my basketball analytics content)\nIt isn’t possible for a team to score more than once during a single minute.\n\n\nlam = 1.4 # lambda, goal scoring rate (i.e. goals per game)\ndist = st.poisson(lam) # poisson dist. with lambda = 1.4\n\n\n# probability of scoring \"k\" goals (4 in this case)\nk = 4\ndist.pmf(k).round(3) # pmf evaluated at 4.\n\n0.039\n\n\nSo, there’s 3.9% chance to observe 4 goals, under the model above.\n\nlam = 1.4 # goal scoring rate\ngoals = np.arange(10) # possible values for goals\nprobas = st.poisson(lam).pmf(goals)\n\nprobas\n\narray([2.46596964e-01, 3.45235750e-01, 2.41665025e-01, 1.12777012e-01,\n       3.94719540e-02, 1.10521471e-02, 2.57883433e-03, 5.15766866e-04,\n       9.02592015e-05, 1.40403202e-05])\n\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.barplot(x=goals, y=probas)\n\ng.set_xlabel('Goals')\ng.set_ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\nLet’s try to move the other way around: Estimate the goal-scoring rate from given goals.\nDowney has used data from previous World Cups to estimate that the each team scores 1.4 goals per game, approximately. Hence, it is reasonable to make mean of lambda 1.4.\nThe goal scoring rate is continuous and it can’t take values below 0, hence a distribution that reflects this features would be great: Gamma distribution. Additionally, it’s easy to construct one since it only takes one parameter which is the mean which we already have value for.\n\nalpha = 1.4 # mean of the distribution\nlams = np.linspace(0, 10, 101) # possible values of lam between 0 and 10\nps = st.gamma(alpha).pdf(lams) # probability densities\n\n\nprior = emd.Pmf(ps, lams)\nprior.normalize() # Pmf offers \"normalize\" method, which divides by the total probability of the data (i.e., probability under any parameter/hypothesis)\n\n9.889360237140306\n\n\n\ndf_prior = pd.DataFrame(prior.ps, prior.qs).rename(columns={0:'probas'})\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_prior.index, y=df_prior.probas, color='orange', linestyle='--', linewidth=2)\n\ng.set_title('Prior Distribution')\ng.set_ylabel('PMF')\ng.set_xlabel('Goals')\n\nText(0.5, 0, 'Goals')\n\n\n\n\n\n\n\n\n\n\nnp.sum(df_prior.index * df_prior.probas) # mean of the distribution\n\n1.4140818156118378\n\n\n\nUpdating with new data\nSo, our initial belief for France’s goal scoring rate (in this example, for other teams as well), goals per 90 mins, was 1.4. Then we observed 4 goals from France, should we still think that France’s goal scoring rate is 1.4? If not, how much should it change?\n\nlambdas = lams # different lambdas (different goal scoring rates)\nk = 4 # observed data\nlikelihood = st.poisson(lambdas).pmf(k) # for each lambda (for each parameter), how likely we are to see 4 goals\nlikelihood[:4]\n\narray([0.00000000e+00, 3.77015591e-06, 5.45820502e-05, 2.50026149e-04])\n\n\n\ndf_prior['likelihood'] = likelihood\ndf_prior.head(4)\n\n\n\n\n\n\n\n\nprobas\nlikelihood\n\n\n\n\n0.000\n0.000\n0.000\n\n\n0.100\n0.041\n0.000\n\n\n0.200\n0.049\n0.000\n\n\n0.300\n0.052\n0.000\n\n\n\n\n\n\n\n\np_norm = emd.Pmf(df_prior['probas'] * df_prior['likelihood'], df_prior.index)\np_norm.normalize()\n\n0.05015532557804499\n\n\n\ndf_prior['posterior'] = p_norm\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_prior.index, y=df_prior.posterior, color='blue', linestyle='--')\n\ng.set_xlabel('Goal scoring rate')\ng.set_ylabel('PMF')\ng.set_title('France')\n\nText(0.5, 1.0, 'France')\n\n\n\n\n\n\n\n\n\nSame steps for Crotia:\n\ndf_crotia = df_prior[['probas']].copy()\ndf_crotia.head(3)\n\n\n\n\n\n\n\n\nprobas\n\n\n\n\n0.000\n0.000\n\n\n0.100\n0.041\n\n\n0.200\n0.049\n\n\n\n\n\n\n\n\nk = 2 # Crotia's number of goals in the match\nlikelihood_cro = st.poisson(lams).pmf(2)\nlikelihood_cro[:4]\n\narray([0.        , 0.00452419, 0.01637462, 0.03333682])\n\n\n\ndf_crotia['likelihood'] = likelihood_cro\n\n\np_norm = emd.Pmf(df_crotia['probas'] * df_crotia['likelihood'], df_crotia.index)\np_norm.normalize()\n\n0.1609321178598705\n\n\n\ndf_crotia['posterior'] = p_norm\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_crotia.index, y=df_crotia['posterior'], color='red', linestyle='--')\n\ng.set_xlabel('Goal scoring rate')\ng.set_ylabel('PMF')\ng.set_title('Crotia')\n\nText(0.5, 1.0, 'Crotia')\n\n\n\n\n\n\n\n\n\nHow confident we are that France is the better team?\n\nprint(\n    'Mean of France: ', str(np.sum(df_prior.index * df_prior.posterior).round(1)), '\\n'\n    'Mean of Crotia: ', str(np.sum(df_crotia.index * df_crotia.posterior).round(1))\n)\n\nMean of France:  2.7 \nMean of Crotia:  1.7\n\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_crotia.index, y=df_crotia['posterior'], color='red', label='Crotia')\ng = sns.lineplot(x=df_prior.index, y=df_prior['posterior'], color='blue', label='France')\n\ng.set_xlabel('Goals')\ng.set_ylabel('PMF')\ng.set_title('France vs Crotia')\n\nText(0.5, 1.0, 'France vs Crotia')\n\n\n\n\n\n\n\n\n\nProbability of superiority is a way to express an effect size. Here’s a great visualization tool that you can play with to make expression of effect sizes more intuitive: Interpreting Effect Sizes: An Interactive Visualization\n\n# Probability of superiority.\n\nfrance_proba = 0\n\nfor i in df_prior.index.tolist():\n    for j in df_crotia.index.tolist():\n        if i &gt; j:\n            france_proba += df_prior.loc[i, 'posterior'] * df_crotia.loc[j, 'posterior']\n        else:\n            continue\n\nfrance_proba.round(2)\n\n0.75\n\n\nProbability of superiority feels very intuitive: It is the probability of randomly sampled lambda for France being higher than randomly sampled lambda for Crotia. If there isn’t much overlap between two distributions, we are more confident that the one group (in this example, a team) is higher/lower than the other: Probability of superiority reflects that. You can use any tool (Python, R etc.) to simulate this, and I highly suggest it if you are not currently able to wrap your head around. Just generate different distributions and play with the idea.\nI didn’t include but there is one more reason why we used poisson and gamma distributions, it is related to something called conjugates that I first came across while I was going through Daniel Lakens’ Improving Your Statistical Inferences: Bayesian statistics. They make things computationally more feasible compared to the grid approach here.\nExamples (like this one) from many books can be found on my GitHub repo, and I highly suggest you to go through Allen Downey’s books. Have a nice weekend."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "YIGIT ASIK, Data Scientist",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWhich p-values to expect under different realities?\n\n\n\n\n\n\nStats\n\n\nInference\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Updating: Poisson & Gamma\n\n\n\n\n\n\nStats\n\n\nBayesian\n\n\nSports Analytics\n\n\n\n\n\n\n\n\n\nJan 10, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nAn Intro Example to Bayes’ Theorem\n\n\n\n\n\n\nStats\n\n\nprobability\n\n\nBayesian\n\n\n\n\n\n\n\n\n\nJan 5, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nMan on a Mission: Starting 2025\n\n\n\n\n\n\nupdate\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Happens to Sample Statistics?\n\n\n\n\n\n\nStats\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 28, 2024\n\n\nYiğit Aşık\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Yiğit, though many people call me Yigas—a nickname blending my first and last names. I hold a degree in English Literature, but I currently work as a data scientist at DenizBank in Istanbul. I am mainly interested in causal inference, predictive modeling, explainable AI. In my free time, I dive into basketball data, exploring patterns and creating statistical models that lead to new metrics."
  },
  {
    "objectID": "about.html#the-start",
    "href": "about.html#the-start",
    "title": "About",
    "section": "The Start",
    "text": "The Start\nDuring my first year at university, I spent a semester break in Beirut, Lebanon, where I earned a human biomechanics trainer certificate. I’d been training basketball players at various levels, including those in the Turkish Basketball League (TBL). While in Beirut, I often heard about the impact of prolonged stress on human health. This led me to watch Dr. Robert Sapolsky’s TED Talk, The Biology of Our Best and Worst Selves, and I thought, “I want to understand human behavior the way he does.”"
  },
  {
    "objectID": "about.html#getting-into-psychology-lab",
    "href": "about.html#getting-into-psychology-lab",
    "title": "About",
    "section": "Getting Into Psychology Lab",
    "text": "Getting Into Psychology Lab\nI began watching Stanford’s Human Behavioral Biology courses online. While intriguing, I found the material challenging without a background in the field. To get answers, I reached out to my university’s Psychology Department, where I met Dr. Hasan Bahçekapılı and Dr. Onurcan Yılmaz. Soon, I was visiting Dr. Yılmaz’s office regularly to discuss topics related to psychology, and I grew fascinated with his research interests—especially how morality, politics, religion, and decision-making intersect. Gradually, I shifted my focus from behavioral biology to social and evolutionary psychology. When he offered me a spot in the lab (MINT Lab) he was forming, I jumped at the opportunity."
  },
  {
    "objectID": "about.html#learning-statistics",
    "href": "about.html#learning-statistics",
    "title": "About",
    "section": "Learning Statistics",
    "text": "Learning Statistics\nAhead of the lab’s start, I took an edX’s Science of Religion course to ensure I had foundational knowledge. Although enjoyable, I quickly realized, once we started reading research papers, that I needed a solid ground in statistics to evaluate them. As an English Literature major, I needed a resource that started from scratch, and OpenIntro Statistics and Learning Stats with JASP became essential guides for me. Studying statistics was unexpectedly enjoyable, and I decided to develop my skills further."
  },
  {
    "objectID": "about.html#learning-mathematics",
    "href": "about.html#learning-mathematics",
    "title": "About",
    "section": "Learning Mathematics",
    "text": "Learning Mathematics\nNot knowing mathematics bothered me, and I began to wonder how it might enhance my understanding of statistics. Fortunately, I connected with Dr. Basar Coşkunoğlu, whom I knew through playing Hearthstone. With his patient guidance, I started with basics like functions and inequalities, eventually advancing to calculus and linear algebra. We reached a point where I could continue independently, and I still study linear algebra occasionally, using my notes, Mathematics for Machine Learning, and Gilbert Strang’s works."
  },
  {
    "objectID": "about.html#python-for-data-science-ml",
    "href": "about.html#python-for-data-science-ml",
    "title": "About",
    "section": "Python for Data Science & ML",
    "text": "Python for Data Science & ML\nIn the lab and academia, statistical tools like JASP, Jamovi, and SPSS are prevalent, with some usage of R. Around this time, however, my academic interests began shifting, so I decided to learn Python. I enrolled in a Data Science & Machine Learning Bootcamp, which focused on programming and industry cases, building on my previous knowledge from Introduction to Statistical Learning."
  },
  {
    "objectID": "about.html#data-science-internship",
    "href": "about.html#data-science-internship",
    "title": "About",
    "section": "Data Science Internship",
    "text": "Data Science Internship\nWhile still an undergraduate, I received a scholarship from TUBITAK (The Scientific and Technological Research Council of Turkey) for research participation. Wanting broader experience, I sought a part-time role or long-term internship to balance with school. I started building a project portfolio and applied to various positions. Many interviews revealed that prospective employers lacked data science teams, which felt limiting for my first role. Finally, I applied for and was accepted to DenizBank’s data science internship program."
  },
  {
    "objectID": "about.html#transition-to-full-time",
    "href": "about.html#transition-to-full-time",
    "title": "About",
    "section": "Transition to Full-Time",
    "text": "Transition to Full-Time\nDuring my internship, things went well, and DenizBank invited me to join full-time after graduation. Despite six months of on-the-job coding, I needed to pass a data scientist test in SQL, Python/R, and statistics to secure the position, which I did. I now work full-time as a Data Scientist at DenizBank."
  },
  {
    "objectID": "about.html#basketball-analytics",
    "href": "about.html#basketball-analytics",
    "title": "About",
    "section": "Basketball Analytics",
    "text": "Basketball Analytics\nAfter ten years of playing and training in basketball, I always hoped to integrate it into my work. Inspired by a Formula 1 analytics account, I launched a basketball analytics account of my own. You can find links in the footer."
  },
  {
    "objectID": "posts/Bayes Theorem/index.html",
    "href": "posts/Bayes Theorem/index.html",
    "title": "An Intro Example to Bayes’ Theorem",
    "section": "",
    "text": "Imagine that you and I are playing a guessing game. I have two dice, 12-sided and 6-sided, and I am holding them in both of my hands. You are trying to guess which hand has the 12-sided die. At this very moment, there is no information for you. So, it’s not uncommon for a rational agent to think 50-50.\nHowever, you tell me to roll the die I hold on my left hand, and close your eyes. I inform you that I rolled above 3 (i.e., &gt;= 4). Is it still 50-50? It feels like it’s not, since rolling above 3 is more likely with the 12-sided die, right? So, how should you update your initial belief?\nI emphasized the “update” above, this is what one should think of when one encounters the word Bayesian: Updating the prior beliefs in the light of new data. Let’s go through the example.\n\n\nAt the beginning, before any information, it’s 50-50: \\(P(12\\: sided \\ LH) = 0.5\\) and \\(P(12\\: sided \\ RH) = 0.5\\) where LH and RH stands for left-hand and right-hand, respectively. Once you have the information that I rolled bigger than 3, there are two possible scenarios: Either I rolled bigger than 3 with 12-sided or with 6-sided.\n\n\\(P(&gt;3\\: |\\ 12\\ sided) = 0.75\\)\n\\(P(&gt;3\\: |\\ 6\\ sided) = 0.5\\)\n\nLet’s put those in a tree diagram.\n\n\n\nTree Diagram\n\n\n\nThe first column (left to the first vertical white bar) represents initial beliefs.\nThe second column represents the probabilities given the first column.\nThird column is the multiplication of the three, and represents P(A and B).\n\nWell, you are not interested in the whole diagram since you have observed some data: I rolled a number bigger than 3. You wonder:\n\\(P(12\\ sided\\ LH\\: | &gt;3)\\), which is \\(\\dfrac{P(12\\ sided\\ LH\\: ,\\ &gt;3)}{P(&gt;3)}\\).\nFor the numerator, you can track the first row of the tree diagram which leads to 0.375. The denominator consists of two parts, rolling above 3 under two different hypotheses: Following the path of rolling above 3 with 12-sided on the left leads to 0.375, which is the first part. In addition, tracking the other scenario of rolling above 3 with 6-sided on hand leads to 0.25. Hence: \\(\\dfrac{0.375}{0.375+0.25} = 0.6\\)\nThis represents your new belief of having the 12-sided die on my left hand. If you tell me to roll the die again and construct a new tree diagram, the first column will consist of 0.6 and 0.4 and the following columns would be adjusted accordingly.\n\n\n\nDo you have to construct tree diagram each time? After all, even for this simple question the whole process takes a bit of time. With a little bit of algebra, you don’t have to: \\(P(A\\: |\\ B) = \\dfrac{P(A,B)}{P(B)}\\). Multiplying both sides by the P(B): \\(P(A\\: |\\ B) P(B) = P(A,B)\\) and since P(A,B) = P(B,A)\n\\(P(A\\: |\\ B) P(B) = P(B,A)\\) which leads to \\(P(A\\: |\\ B) P(B) = P(B\\: |\\ A) P (A)\\)\nand voila: \\(P(A\\: |\\ B) = \\dfrac{P(B\\: |\\ A) P(A)}{P(B)}\\)\n\nThe left hand side is called posterior probability, and you may come across it in the form of P(hypothesis | data).\nThe denominator on the right is total probability of the data, sometimes referred to as marginal probability.\nP(A) is your initial belief here, the prior.\nWhile P(B | A) is called likelihood which is the probability of the data under given the hypothesis.\n\nFor our example: \\(P(12\\ sided\\: |\\ &gt; 3) = \\dfrac{P(&gt;3\\: |\\ 12\\ sided) P(12\\ sided)}{P(&gt;3)}\\)\n\n\n\nYou may see versions of this where instead of hypothesis there can be theory or parameters (and instead of data, evidence) but they all are the same initially. This type of approach has its advantages such as incorporating the prior knowledge: If I would roll the die again, you would make those calculations with new priors (learned from the first roll), making use of what you already know. It allows for priors that are subjective: Maybe I am known to favor my left hand so it is possible for you to have an initial belief that is not reflected as 50-50.\nI will talk more about this view in the future posts but if you’re interested, you can check probability and Bayesian chapter in each of the books below:\n\nLearning Statistics with R by Daniel Navarro\nPhilosophy of Quantitative Methods by Brian D. Haig\nDoing Bayesian Data Analysis by John K. Kruschke\nImproving Your Statistical Inferences by Daniel Lakens\nThink Bayes by Allen B. Downey\n\nand I believe I remember the example above from Mine Çetinkaya-Rundel."
  },
  {
    "objectID": "posts/Bayes Theorem/index.html#usual-dice-example",
    "href": "posts/Bayes Theorem/index.html#usual-dice-example",
    "title": "An Intro Example to Bayes’ Theorem",
    "section": "",
    "text": "Imagine that you and I are playing a guessing game. I have two dice, 12-sided and 6-sided, and I am holding them in both of my hands. You are trying to guess which hand has the 12-sided die. At this very moment, there is no information for you. So, it’s not uncommon for a rational agent to think 50-50.\nHowever, you tell me to roll the die I hold on my left hand, and close your eyes. I inform you that I rolled above 3 (i.e., &gt;= 4). Is it still 50-50? It feels like it’s not, since rolling above 3 is more likely with the 12-sided die, right? So, how should you update your initial belief?\nI emphasized the “update” above, this is what one should think of when one encounters the word Bayesian: Updating the prior beliefs in the light of new data. Let’s go through the example.\n\n\nAt the beginning, before any information, it’s 50-50: \\(P(12\\: sided \\ LH) = 0.5\\) and \\(P(12\\: sided \\ RH) = 0.5\\) where LH and RH stands for left-hand and right-hand, respectively. Once you have the information that I rolled bigger than 3, there are two possible scenarios: Either I rolled bigger than 3 with 12-sided or with 6-sided.\n\n\\(P(&gt;3\\: |\\ 12\\ sided) = 0.75\\)\n\\(P(&gt;3\\: |\\ 6\\ sided) = 0.5\\)\n\nLet’s put those in a tree diagram.\n\n\n\nTree Diagram\n\n\n\nThe first column (left to the first vertical white bar) represents initial beliefs.\nThe second column represents the probabilities given the first column.\nThird column is the multiplication of the three, and represents P(A and B).\n\nWell, you are not interested in the whole diagram since you have observed some data: I rolled a number bigger than 3. You wonder:\n\\(P(12\\ sided\\ LH\\: | &gt;3)\\), which is \\(\\dfrac{P(12\\ sided\\ LH\\: ,\\ &gt;3)}{P(&gt;3)}\\).\nFor the numerator, you can track the first row of the tree diagram which leads to 0.375. The denominator consists of two parts, rolling above 3 under two different hypotheses: Following the path of rolling above 3 with 12-sided on the left leads to 0.375, which is the first part. In addition, tracking the other scenario of rolling above 3 with 6-sided on hand leads to 0.25. Hence: \\(\\dfrac{0.375}{0.375+0.25} = 0.6\\)\nThis represents your new belief of having the 12-sided die on my left hand. If you tell me to roll the die again and construct a new tree diagram, the first column will consist of 0.6 and 0.4 and the following columns would be adjusted accordingly.\n\n\n\nDo you have to construct tree diagram each time? After all, even for this simple question the whole process takes a bit of time. With a little bit of algebra, you don’t have to: \\(P(A\\: |\\ B) = \\dfrac{P(A,B)}{P(B)}\\). Multiplying both sides by the P(B): \\(P(A\\: |\\ B) P(B) = P(A,B)\\) and since P(A,B) = P(B,A)\n\\(P(A\\: |\\ B) P(B) = P(B,A)\\) which leads to \\(P(A\\: |\\ B) P(B) = P(B\\: |\\ A) P (A)\\)\nand voila: \\(P(A\\: |\\ B) = \\dfrac{P(B\\: |\\ A) P(A)}{P(B)}\\)\n\nThe left hand side is called posterior probability, and you may come across it in the form of P(hypothesis | data).\nThe denominator on the right is total probability of the data, sometimes referred to as marginal probability.\nP(A) is your initial belief here, the prior.\nWhile P(B | A) is called likelihood which is the probability of the data under given the hypothesis.\n\nFor our example: \\(P(12\\ sided\\: |\\ &gt; 3) = \\dfrac{P(&gt;3\\: |\\ 12\\ sided) P(12\\ sided)}{P(&gt;3)}\\)\n\n\n\nYou may see versions of this where instead of hypothesis there can be theory or parameters (and instead of data, evidence) but they all are the same initially. This type of approach has its advantages such as incorporating the prior knowledge: If I would roll the die again, you would make those calculations with new priors (learned from the first roll), making use of what you already know. It allows for priors that are subjective: Maybe I am known to favor my left hand so it is possible for you to have an initial belief that is not reflected as 50-50.\nI will talk more about this view in the future posts but if you’re interested, you can check probability and Bayesian chapter in each of the books below:\n\nLearning Statistics with R by Daniel Navarro\nPhilosophy of Quantitative Methods by Brian D. Haig\nDoing Bayesian Data Analysis by John K. Kruschke\nImproving Your Statistical Inferences by Daniel Lakens\nThink Bayes by Allen B. Downey\n\nand I believe I remember the example above from Mine Çetinkaya-Rundel."
  },
  {
    "objectID": "posts/Man on a Mission - Starting 2025/index.html",
    "href": "posts/Man on a Mission - Starting 2025/index.html",
    "title": "Man on a Mission: Starting 2025",
    "section": "",
    "text": "Some time ago, I stumbled upon the website Less Certainty, More Inquiry. If you click on the link, you’ll see that Darryl Blackport shares what he listens to or reads every week of the year. I found the concept intriguing and thought it would be a great fit for me, as I’m often immersed in studying or reading something.\nThis year, I’m taking inspiration from his approach. I’ll share what I study each month, providing a brief overview of the topics I’ve explored, focusing on what I found particularly interesting or useful. To kick things off, here are my learning goals for 2025. Let’s dive in.\n\n\n\n\n\n\nFor most of my data science journey, I’ve taken a theory-first approach, focusing heavily on statistics. However, with recent advancements in tools like LLMs, I think it’s time for a shift. In 2025, I plan to delve into deep learning (a domain I’ve avoided so far) with a hands-on approach.\nHere’s my roadmap for deep learning:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n\nAdvanced Learning Algorithms\n\nDeep Learning Specialization\n\n\n\n\n\n\n\nI’ve been passionate about sports analytics for nearly a year now and have made significant strides in basketball analytics. (You can check out some of my work here.)\nThis year, I aim to expand my focus by creating videos on football (soccer) analytics, in addition to basketball, on my YouTube channel. To prepare for this, I’ve started the following specialization:\n\nSports Performance Analytics Specialization\n\nAlso, Dean Oliver’s new book, Basketball Beyond Paper, is on my reading list—it’s a must-read for anyone in this space.\n\n\n\n\n\n\nI consider statistics to be one of my stronger skills compared to other data scientists in the industry (at least in Turkey), but there’s always room for growth.\nCurrently, I’m halfway through Think Bayes, and I’m looking forward to Tom Faulkenberry’s upcoming book when it’s released.\nThere are specific topics, like meta-analysis, that I want to strengthen, but I don’t plan to tackle them systematically. Instead, I’ll explore them as curiosity strikes.\n\n\n\n\n\n\nThis one’s a “maybe.” Deep learning and LLMs are higher priorities, and the list is already packed. However, gaining production-level data science expertise could be beneficial in the long run.\nFor now, it’s something I’ll keep on the back burner, but we’ll see how the year unfolds.\n\n\n\n\n\n\n\nThis blog post was polished with help of ChatGPT! I write the whole piece and feed it to ChatGPT to improve the flow.\nAnd that’s pretty much it for my 2025 learning goals. Stay tuned for updates!"
  },
  {
    "objectID": "posts/Man on a Mission - Starting 2025/index.html#man-on-a-mission-series",
    "href": "posts/Man on a Mission - Starting 2025/index.html#man-on-a-mission-series",
    "title": "Man on a Mission: Starting 2025",
    "section": "",
    "text": "Some time ago, I stumbled upon the website Less Certainty, More Inquiry. If you click on the link, you’ll see that Darryl Blackport shares what he listens to or reads every week of the year. I found the concept intriguing and thought it would be a great fit for me, as I’m often immersed in studying or reading something.\nThis year, I’m taking inspiration from his approach. I’ll share what I study each month, providing a brief overview of the topics I’ve explored, focusing on what I found particularly interesting or useful. To kick things off, here are my learning goals for 2025. Let’s dive in.\n\n\n\n\n\n\nFor most of my data science journey, I’ve taken a theory-first approach, focusing heavily on statistics. However, with recent advancements in tools like LLMs, I think it’s time for a shift. In 2025, I plan to delve into deep learning (a domain I’ve avoided so far) with a hands-on approach.\nHere’s my roadmap for deep learning:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n\nAdvanced Learning Algorithms\n\nDeep Learning Specialization\n\n\n\n\n\n\n\nI’ve been passionate about sports analytics for nearly a year now and have made significant strides in basketball analytics. (You can check out some of my work here.)\nThis year, I aim to expand my focus by creating videos on football (soccer) analytics, in addition to basketball, on my YouTube channel. To prepare for this, I’ve started the following specialization:\n\nSports Performance Analytics Specialization\n\nAlso, Dean Oliver’s new book, Basketball Beyond Paper, is on my reading list—it’s a must-read for anyone in this space.\n\n\n\n\n\n\nI consider statistics to be one of my stronger skills compared to other data scientists in the industry (at least in Turkey), but there’s always room for growth.\nCurrently, I’m halfway through Think Bayes, and I’m looking forward to Tom Faulkenberry’s upcoming book when it’s released.\nThere are specific topics, like meta-analysis, that I want to strengthen, but I don’t plan to tackle them systematically. Instead, I’ll explore them as curiosity strikes.\n\n\n\n\n\n\nThis one’s a “maybe.” Deep learning and LLMs are higher priorities, and the list is already packed. However, gaining production-level data science expertise could be beneficial in the long run.\nFor now, it’s something I’ll keep on the back burner, but we’ll see how the year unfolds.\n\n\n\n\n\n\n\nThis blog post was polished with help of ChatGPT! I write the whole piece and feed it to ChatGPT to improve the flow.\nAnd that’s pretty much it for my 2025 learning goals. Stay tuned for updates!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nI always had the idea of blogging but a suggestion that I received recently pushed me towards taking an action. I am hoping to use this place to share the stuff that I learn, it makes me develop an intuition and I feel like I learn better.\nMain things that I think about blogging fall under statistics (both frequentist and Bayesian methods), machine learning, and psychology (less frequently).\nAim of this blog is to make this place a useful place for the mentioned topics above, while improving myself on them as well. I hope you find this place useful."
  }
]
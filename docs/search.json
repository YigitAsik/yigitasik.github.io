[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Yiğit, though many people call me Yigas—a nickname blending my first and last names. I hold a degree in English Literature, but I currently work as a data scientist at DenizBank in Istanbul. I am mainly interested in causal inference, predictive modeling, explainable AI. In my free time, I dive into basketball data, exploring patterns and creating statistical models that lead to new metrics."
  },
  {
    "objectID": "about.html#the-start",
    "href": "about.html#the-start",
    "title": "About",
    "section": "The Start",
    "text": "The Start\nDuring my first year at university, I spent a semester break in Beirut, Lebanon, where I earned a human biomechanics trainer certificate. I’d been training basketball players at various levels, including those in the Turkish Basketball League (TBL). While in Beirut, I often heard about the impact of prolonged stress on human health. This led me to watch Dr. Robert Sapolsky’s TED Talk, The Biology of Our Best and Worst Selves, and I thought, “I want to understand human behavior the way he does.”"
  },
  {
    "objectID": "about.html#getting-into-psychology-lab",
    "href": "about.html#getting-into-psychology-lab",
    "title": "About",
    "section": "Getting Into Psychology Lab",
    "text": "Getting Into Psychology Lab\nI began watching Stanford’s Human Behavioral Biology courses online. While intriguing, I found the material challenging without a background in the field. To get answers, I reached out to my university’s Psychology Department, where I met Dr. Hasan Bahçekapılı and Dr. Onurcan Yılmaz. Soon, I was visiting Dr. Yılmaz’s office regularly to discuss topics related to psychology, and I grew fascinated with his research interests—especially how morality, politics, religion, and decision-making intersect. Gradually, I shifted my focus from behavioral biology to social and evolutionary psychology. When he offered me a spot in the lab (MINT Lab) he was forming, I jumped at the opportunity."
  },
  {
    "objectID": "about.html#learning-statistics",
    "href": "about.html#learning-statistics",
    "title": "About",
    "section": "Learning Statistics",
    "text": "Learning Statistics\nAhead of the lab’s start, I took an edX’s Science of Religion course to ensure I had foundational knowledge. Although enjoyable, I quickly realized, once we started reading research papers, that I needed a solid ground in statistics to evaluate them. As an English Literature major, I needed a resource that started from scratch, and OpenIntro Statistics and Learning Stats with JASP became essential guides for me. Studying statistics was unexpectedly enjoyable, and I decided to develop my skills further."
  },
  {
    "objectID": "about.html#learning-mathematics",
    "href": "about.html#learning-mathematics",
    "title": "About",
    "section": "Learning Mathematics",
    "text": "Learning Mathematics\nNot knowing mathematics bothered me, and I began to wonder how it might enhance my understanding of statistics. Fortunately, I connected with Dr. Basar Coşkunoğlu, whom I knew through playing Hearthstone. With his patient guidance, I started with basics like functions and inequalities, eventually advancing to calculus and linear algebra. We reached a point where I could continue independently, and I still study linear algebra occasionally, using my notes, Mathematics for Machine Learning, and Gilbert Strang’s works."
  },
  {
    "objectID": "about.html#python-for-data-science-ml",
    "href": "about.html#python-for-data-science-ml",
    "title": "About",
    "section": "Python for Data Science & ML",
    "text": "Python for Data Science & ML\nIn the lab and academia, statistical tools like JASP, Jamovi, and SPSS are prevalent, with some usage of R. Around this time, however, my academic interests began shifting, so I decided to learn Python. I enrolled in a Data Science & Machine Learning Bootcamp, which focused on programming and industry cases, building on my previous knowledge from Introduction to Statistical Learning."
  },
  {
    "objectID": "about.html#data-science-internship",
    "href": "about.html#data-science-internship",
    "title": "About",
    "section": "Data Science Internship",
    "text": "Data Science Internship\nWhile still an undergraduate, I received a scholarship from TUBITAK (The Scientific and Technological Research Council of Turkey) for research participation. Wanting broader experience, I sought a part-time role or long-term internship to balance with school. I started building a project portfolio and applied to various positions. Many interviews revealed that prospective employers lacked data science teams, which felt limiting for my first role. Finally, I applied for and was accepted to DenizBank’s data science internship program."
  },
  {
    "objectID": "about.html#transition-to-full-time",
    "href": "about.html#transition-to-full-time",
    "title": "About",
    "section": "Transition to Full-Time",
    "text": "Transition to Full-Time\nDuring my internship, things went well, and DenizBank invited me to join full-time after graduation. Despite six months of on-the-job coding, I needed to pass a data scientist test in SQL, Python/R, and statistics to secure the position, which I did. I now work full-time as a Data Scientist at DenizBank."
  },
  {
    "objectID": "about.html#basketball-analytics",
    "href": "about.html#basketball-analytics",
    "title": "About",
    "section": "Basketball Analytics",
    "text": "Basketball Analytics\nAfter ten years of playing and training in basketball, I always hoped to integrate it into my work. Inspired by a Formula 1 analytics account, I launched a basketball analytics account of my own. You can find links in the footer."
  },
  {
    "objectID": "posts/Bayesian Updating with Poisson and Gamma/index.html",
    "href": "posts/Bayesian Updating with Poisson and Gamma/index.html",
    "title": "Bayesian Updating: Poisson & Gamma",
    "section": "",
    "text": "The following example is taken from Allen Downey’s Think Bayes. I believe this is a great one to show how to update priors.\nDowney takes a match between France and Crotia, played back in 2018 World Cup, that France won 4-2. Then, he aims to answer two questions:\n\nHow confident we are about France being the better team?\nIn a rematch, what is the probability that France would win again?\n\nI’ll only attempt to answer the first question so that you have a reason to check Downey’s book Think Bayes.\n\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport empiricaldist as emd\nimport scipy.stats as st\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 170)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nStarting out with certain assumptions:\n\nThere is a goal scoring-rate for every team, goals per game (more specifically, per 90), which we will denote as lambda.\nA goal is equally likely during any minute of the game, regardless of strategy, tempo etc.\n(This is also an assumption under a huge framework in basketball analytics, regularized adjusted plus-minus, wanted to point out just in case you follow my basketball analytics content)\nIt isn’t possible for a team to score more than once during a single minute.\n\n\nlam = 1.4 # lambda, goal scoring rate (i.e. goals per game)\ndist = st.poisson(lam) # poisson dist. with lambda = 1.4\n\n\n# probability of scoring \"k\" goals (4 in this case)\nk = 4\ndist.pmf(k).round(3) # pmf evaluated at 4.\n\n0.039\n\n\nSo, there’s 3.9% chance to observe 4 goals, under the model above.\n\nlam = 1.4 # goal scoring rate\ngoals = np.arange(10) # possible values for goals\nprobas = st.poisson(lam).pmf(goals)\n\nprobas\n\narray([2.46596964e-01, 3.45235750e-01, 2.41665025e-01, 1.12777012e-01,\n       3.94719540e-02, 1.10521471e-02, 2.57883433e-03, 5.15766866e-04,\n       9.02592015e-05, 1.40403202e-05])\n\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.barplot(x=goals, y=probas)\n\ng.set_xlabel('Goals')\ng.set_ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\nLet’s try to move the other way around: Estimate the goal-scoring rate from given goals.\nDowney has used data from previous World Cups to estimate that the each team scores 1.4 goals per game, approximately. Hence, it is reasonable to make mean of lambda 1.4.\nThe goal scoring rate is continuous and it can’t take values below 0, hence a distribution that reflects this features would be great: Gamma distribution. Additionally, it’s easy to construct one since it only takes one parameter which is the mean which we already have value for.\n\nalpha = 1.4 # mean of the distribution\nlams = np.linspace(0, 10, 101) # possible values of lam between 0 and 10\nps = st.gamma(alpha).pdf(lams) # probability densities\n\n\nprior = emd.Pmf(ps, lams)\nprior.normalize() # Pmf offers \"normalize\" method, which divides by the total probability of the data (i.e., probability under any parameter/hypothesis)\n\n9.889360237140306\n\n\n\ndf_prior = pd.DataFrame(prior.ps, prior.qs).rename(columns={0:'probas'})\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_prior.index, y=df_prior.probas, color='orange', linestyle='--', linewidth=2)\n\ng.set_title('Prior Distribution')\ng.set_ylabel('PMF')\ng.set_xlabel('Goals')\n\nText(0.5, 0, 'Goals')\n\n\n\n\n\n\n\n\n\n\nnp.sum(df_prior.index * df_prior.probas) # mean of the distribution\n\n1.4140818156118378\n\n\n\nUpdating with new data\nSo, our initial belief for France’s goal scoring rate (in this example, for other teams as well), goals per 90 mins, was 1.4. Then we observed 4 goals from France, should we still think that France’s goal scoring rate is 1.4? If not, how much should it change?\n\nlambdas = lams # different lambdas (different goal scoring rates)\nk = 4 # observed data\nlikelihood = st.poisson(lambdas).pmf(k) # for each lambda (for each parameter), how likely we are to see 4 goals\nlikelihood[:4]\n\narray([0.00000000e+00, 3.77015591e-06, 5.45820502e-05, 2.50026149e-04])\n\n\n\ndf_prior['likelihood'] = likelihood\ndf_prior.head(4)\n\n\n\n\n\n\n\n\nprobas\nlikelihood\n\n\n\n\n0.000\n0.000\n0.000\n\n\n0.100\n0.041\n0.000\n\n\n0.200\n0.049\n0.000\n\n\n0.300\n0.052\n0.000\n\n\n\n\n\n\n\n\np_norm = emd.Pmf(df_prior['probas'] * df_prior['likelihood'], df_prior.index)\np_norm.normalize()\n\n0.05015532557804499\n\n\n\ndf_prior['posterior'] = p_norm\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_prior.index, y=df_prior.posterior, color='blue', linestyle='--')\n\ng.set_xlabel('Goal scoring rate')\ng.set_ylabel('PMF')\ng.set_title('France')\n\nText(0.5, 1.0, 'France')\n\n\n\n\n\n\n\n\n\nSame steps for Crotia:\n\ndf_crotia = df_prior[['probas']].copy()\ndf_crotia.head(3)\n\n\n\n\n\n\n\n\nprobas\n\n\n\n\n0.000\n0.000\n\n\n0.100\n0.041\n\n\n0.200\n0.049\n\n\n\n\n\n\n\n\nk = 2 # Crotia's number of goals in the match\nlikelihood_cro = st.poisson(lams).pmf(2)\nlikelihood_cro[:4]\n\narray([0.        , 0.00452419, 0.01637462, 0.03333682])\n\n\n\ndf_crotia['likelihood'] = likelihood_cro\n\n\np_norm = emd.Pmf(df_crotia['probas'] * df_crotia['likelihood'], df_crotia.index)\np_norm.normalize()\n\n0.1609321178598705\n\n\n\ndf_crotia['posterior'] = p_norm\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_crotia.index, y=df_crotia['posterior'], color='red', linestyle='--')\n\ng.set_xlabel('Goal scoring rate')\ng.set_ylabel('PMF')\ng.set_title('Crotia')\n\nText(0.5, 1.0, 'Crotia')\n\n\n\n\n\n\n\n\n\nHow confident we are that France is the better team?\n\nprint(\n    'Mean of France: ', str(np.sum(df_prior.index * df_prior.posterior).round(1)), '\\n'\n    'Mean of Crotia: ', str(np.sum(df_crotia.index * df_crotia.posterior).round(1))\n)\n\nMean of France:  2.7 \nMean of Crotia:  1.7\n\n\n\nfig = plt.figure(figsize=(8,6))\ng = sns.lineplot(x=df_crotia.index, y=df_crotia['posterior'], color='red', label='Crotia')\ng = sns.lineplot(x=df_prior.index, y=df_prior['posterior'], color='blue', label='France')\n\ng.set_xlabel('Goals')\ng.set_ylabel('PMF')\ng.set_title('France vs Crotia')\n\nText(0.5, 1.0, 'France vs Crotia')\n\n\n\n\n\n\n\n\n\nProbability of superiority is a way to express an effect size. Here’s a great visualization tool that you can play with to make expression of effect sizes more intuitive: Interpreting Effect Sizes: An Interactive Visualization\n\n# Probability of superiority.\n\nfrance_proba = 0\n\nfor i in df_prior.index.tolist():\n    for j in df_crotia.index.tolist():\n        if i &gt; j:\n            france_proba += df_prior.loc[i, 'posterior'] * df_crotia.loc[j, 'posterior']\n        else:\n            continue\n\nfrance_proba.round(2)\n\n0.75\n\n\nProbability of superiority feels very intuitive: It is the probability of randomly sampled lambda for France being higher than randomly sampled lambda for Crotia. If there isn’t much overlap between two distributions, we are more confident that the one group (in this example, a team) is higher/lower than the other: Probability of superiority reflects that. You can use any tool (Python, R etc.) to simulate this, and I highly suggest it if you are not currently able to wrap your head around. Just generate different distributions and play with the idea.\nI didn’t include but there is one more reason why we used poisson and gamma distributions, it is related to something called conjugates that I first came across while I was going through Daniel Lakens’ Improving Your Statistical Inferences: Bayesian statistics. They make things computationally more feasible compared to the grid approach here.\nExamples (like this one) from many books can be found on my GitHub repo, and I highly suggest you to go through Allen Downey’s books. Have a nice weekend."
  },
  {
    "objectID": "posts/Fixed Effects/index.html",
    "href": "posts/Fixed Effects/index.html",
    "title": "Fixed Effects",
    "section": "",
    "text": "I mentioned fixed effects on difference in differences post but I wanted to elaborate a bit further on the topic and show where it’s useful. I’m diving right into an example and explain along the way.\n\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom linearmodels.panel import PanelOLS\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\ndf = pd.read_csv('Grunfeld.csv', index_col=0)\ndf.head()\n\n\n\n\n\n\n\n\ninvest\nvalue\ncapital\nfirm\nyear\n\n\n\n\n1\n317.600\n3078.500\n2.800\nGeneral Motors\n1935\n\n\n2\n391.800\n4661.700\n52.600\nGeneral Motors\n1936\n\n\n3\n410.600\n5387.100\n156.900\nGeneral Motors\n1937\n\n\n4\n257.700\n2792.200\n209.200\nGeneral Motors\n1938\n\n\n5\n330.800\n4313.200\n203.400\nGeneral Motors\n1939\n\n\n\n\n\n\n\nI have data from 11 firms: Their capital, market value, investment for each year between 1935 to 1954. This is a panel data, since I have multiple observations for each firm, on different time periods.\nLet’s say that I am interested in the relationship between market value and investment. For simplicity, if we had data on a single year we could estimate the following for each firm i:\n\\(\\displaystyle invest_i = \\beta_0 + \\beta_1 value_i + \\beta_2 capital_i + \\epsilon_i\\)\nHowever, there are things that we miss with this approach:\n\nThere might be firm-level variables that we would like to have in the model. These are assumed to be constant for a firm.\n\nThe idea is pretty neat actually. Think of having two years of data. Let’s say 1935 and 1936:\n\\(\\displaystyle invest_{i \\, 1936} = \\beta_0 + \\beta_{1}value_{i \\, 1936} + \\beta_{2}capital_{i \\, 1936} + \\beta_{3}\\alpha_i + \\epsilon_{i \\, 1936}\\)\n\\(\\displaystyle invest_{i \\, 1935} = \\beta_0 + \\beta_{1}value_{i \\, 1935} + \\beta_{2}capital_{i \\, 1935} + \\beta_{3}\\alpha_i + \\epsilon_{i \\, 1935}\\)\nNow, if you take the difference what happens is those \\(\\beta_{3}\\alpha_i\\) terms get cancelled. What you’re left with is:\n\\(\\displaystyle invest_{i \\, 1936} - invest_{i \\, 1935} = \\beta_{1}(value_{i\\,1936} - value_{i\\,1935}) + \\beta_{2}(capital_{i\\,1936} - capital_{i\\,1935}) + (\\epsilon_{i \\, 1936} - \\epsilon_{i \\, 1935})\\)\nI believe this is a very intuitive example. Accounting for unobserved firm-level characteristics is just adding firm as dummy in the regression!\n\nThe other thing that I haven’t mentioed above is the effects that are constant within a time period but may differ between years. These are shared between firms. Think of things like inflation, market trends etc.\n\nWell, I’ve got the idea. Let’s add that as a dummy as well?\n\nlm = smf.ols(\n    'invest ~ value + capital + C(firm) + C(year)',\n    data=df\n)\nres = lm.fit()\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ninvest\nR-squared:\n0.953\n\n\nModel:\nOLS\nAdj. R-squared:\n0.945\n\n\nMethod:\nLeast Squares\nF-statistic:\n122.1\n\n\nDate:\nSun, 12 Oct 2025\nProb (F-statistic):\n5.20e-108\n\n\nTime:\n01:14:10\nLog-Likelihood:\n-1153.0\n\n\nNo. Observations:\n220\nAIC:\n2370.\n\n\nDf Residuals:\n188\nBIC:\n2479.\n\n\nDf Model:\n31\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n18.0876\n18.656\n0.970\n0.334\n-18.715\n54.890\n\n\nC(firm)[T.Atlantic Refining]\n-112.5008\n17.752\n-6.337\n0.000\n-147.520\n-77.482\n\n\nC(firm)[T.Chrysler]\n-13.5993\n17.540\n-0.775\n0.439\n-48.199\n21.001\n\n\nC(firm)[T.Diamond Match]\n16.4928\n15.692\n1.051\n0.295\n-14.462\n47.448\n\n\nC(firm)[T.General Electric]\n-241.0850\n28.000\n-8.610\n0.000\n-296.319\n-185.851\n\n\nC(firm)[T.General Motors]\n-101.7696\n55.177\n-1.844\n0.067\n-210.615\n7.075\n\n\nC(firm)[T.Goodyear]\n-77.9628\n16.435\n-4.744\n0.000\n-110.383\n-45.543\n\n\nC(firm)[T.IBM]\n-6.4573\n16.271\n-0.397\n0.692\n-38.554\n25.640\n\n\nC(firm)[T.US Steel]\n100.5492\n28.438\n3.536\n0.001\n44.450\n156.648\n\n\nC(firm)[T.Union Oil]\n-56.7936\n16.403\n-3.462\n0.001\n-89.151\n-24.436\n\n\nC(firm)[T.Westinghouse]\n-41.7165\n17.483\n-2.386\n0.018\n-76.204\n-7.229\n\n\nC(year)[T.1936]\n-16.9592\n21.518\n-0.788\n0.432\n-59.407\n25.488\n\n\nC(year)[T.1937]\n-36.3756\n22.364\n-1.627\n0.106\n-80.492\n7.741\n\n\nC(year)[T.1938]\n-35.6237\n21.162\n-1.683\n0.094\n-77.370\n6.122\n\n\nC(year)[T.1939]\n-63.0994\n21.505\n-2.934\n0.004\n-105.522\n-20.677\n\n\nC(year)[T.1940]\n-39.8248\n21.626\n-1.842\n0.067\n-82.486\n2.836\n\n\nC(year)[T.1941]\n-16.4878\n21.529\n-0.766\n0.445\n-58.957\n25.982\n\n\nC(year)[T.1942]\n-17.9993\n21.275\n-0.846\n0.399\n-59.967\n23.968\n\n\nC(year)[T.1943]\n-37.7724\n21.415\n-1.764\n0.079\n-80.016\n4.471\n\n\nC(year)[T.1944]\n-38.3201\n21.459\n-1.786\n0.076\n-80.652\n4.012\n\n\nC(year)[T.1945]\n-49.5395\n21.687\n-2.284\n0.023\n-92.322\n-6.757\n\n\nC(year)[T.1946]\n-27.7544\n21.866\n-1.269\n0.206\n-70.888\n15.379\n\n\nC(year)[T.1947]\n-34.8775\n21.589\n-1.616\n0.108\n-77.464\n7.709\n\n\nC(year)[T.1948]\n-38.3307\n21.734\n-1.764\n0.079\n-81.204\n4.542\n\n\nC(year)[T.1949]\n-65.2008\n21.901\n-2.977\n0.003\n-108.404\n-21.998\n\n\nC(year)[T.1950]\n-67.3877\n22.028\n-3.059\n0.003\n-110.841\n-23.935\n\n\nC(year)[T.1951]\n-54.8346\n22.437\n-2.444\n0.015\n-99.095\n-10.574\n\n\nC(year)[T.1952]\n-56.4890\n22.819\n-2.475\n0.014\n-101.504\n-11.474\n\n\nC(year)[T.1953]\n-58.5126\n23.819\n-2.457\n0.015\n-105.500\n-11.525\n\n\nC(year)[T.1954]\n-81.7939\n24.204\n-3.379\n0.001\n-129.540\n-34.047\n\n\nvalue\n0.1167\n0.013\n9.022\n0.000\n0.091\n0.142\n\n\ncapital\n0.3514\n0.021\n16.696\n0.000\n0.310\n0.393\n\n\n\n\n\n\n\n\nOmnibus:\n32.466\nDurbin-Watson:\n0.988\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n180.276\n\n\nSkew:\n0.311\nProb(JB):\n7.14e-40\n\n\nKurtosis:\n7.391\nCond. No.\n3.92e+04\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.92e+04. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nYou can fit the same with PanelOLS, like below, and get a cleaner table.\n\nfe_model = PanelOLS.from_formula('invest ~ value + capital + EntityEffects + TimeEffects', data=df.set_index(['firm', 'year']))\nfe_res = fe_model.fit()\n\nfe_res.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ninvest\nR-squared:\n0.7253\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.7637\n\n\nNo. Observations:\n220\nR-squared (Within):\n0.7566\n\n\nDate:\nSun, Oct 12 2025\nR-squared (Overall):\n0.7625\n\n\nTime:\n01:14:15\nLog-likelihood\n-1153.0\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n248.15\n\n\nEntities:\n11\nP-value\n0.0000\n\n\nAvg Obs:\n20.000\nDistribution:\nF(2,188)\n\n\nMin Obs:\n20.000\n\n\n\n\nMax Obs:\n20.000\nF-statistic (robust):\n248.15\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n20\nDistribution:\nF(2,188)\n\n\nAvg Obs:\n11.000\n\n\n\n\nMin Obs:\n11.000\n\n\n\n\nMax Obs:\n11.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nvalue\n0.1167\n0.0129\n9.0219\n0.0000\n0.0912\n0.1422\n\n\ncapital\n0.3514\n0.0210\n16.696\n0.0000\n0.3099\n0.3930\n\n\n\nF-test for Poolability: 18.476P-value: 0.0000Distribution: F(29,188)Included effects: Entity, Time\n\n\nOne more thing though, check covariance type on both tables (nonrobust, unadjusted). It means errors are assumed to be independent which might be violated here. Think about it, observations are grouped in the sense that they belong to same firm. So, they share some unobserved component. Hence, errors might be correlated within each firm (across year).\nFor the same reason, errors might be correlated within each year (e.g., firms are subject to same inflation).\nSo, we should allow residuals to be correlated within groups.\nIt’s possible to use clustered covariance type with statsmodels but it doesn’t allow it to be 2 dimensional. In other words, you either cluster by entity dimension (e.g., firm) or time dimension (e.g., year). PanelOLS, on the other hand, allows for two-way clustering.\n\nfe_model = PanelOLS.from_formula('invest ~ value + capital + EntityEffects + TimeEffects', data=df.set_index(['firm', 'year']))\nfe_res = fe_model.fit(cov_type='clustered', cluster_entity=True, cluster_time=True)\n\nfe_res.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ninvest\nR-squared:\n0.7253\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.7637\n\n\nNo. Observations:\n220\nR-squared (Within):\n0.7566\n\n\nDate:\nSun, Oct 12 2025\nR-squared (Overall):\n0.7625\n\n\nTime:\n01:15:17\nLog-likelihood\n-1153.0\n\n\nCov. Estimator:\nClustered\n\n\n\n\n\n\nF-statistic:\n248.15\n\n\nEntities:\n11\nP-value\n0.0000\n\n\nAvg Obs:\n20.000\nDistribution:\nF(2,188)\n\n\nMin Obs:\n20.000\n\n\n\n\nMax Obs:\n20.000\nF-statistic (robust):\n84.060\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n20\nDistribution:\nF(2,188)\n\n\nAvg Obs:\n11.000\n\n\n\n\nMin Obs:\n11.000\n\n\n\n\nMax Obs:\n11.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nvalue\n0.1167\n0.0117\n10.015\n0.0000\n0.0937\n0.1397\n\n\ncapital\n0.3514\n0.0447\n7.8622\n0.0000\n0.2633\n0.4396\n\n\n\nF-test for Poolability: 18.476P-value: 0.0000Distribution: F(29,188)Included effects: Entity, Time\n\n\nI feel like this one is a very intuitive example but for more, you can check this."
  },
  {
    "objectID": "posts/Fixed Effects/index.html#entity-time-fixed-effects",
    "href": "posts/Fixed Effects/index.html#entity-time-fixed-effects",
    "title": "Fixed Effects",
    "section": "",
    "text": "I mentioned fixed effects on difference in differences post but I wanted to elaborate a bit further on the topic and show where it’s useful. I’m diving right into an example and explain along the way.\n\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom linearmodels.panel import PanelOLS\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\ndf = pd.read_csv('Grunfeld.csv', index_col=0)\ndf.head()\n\n\n\n\n\n\n\n\ninvest\nvalue\ncapital\nfirm\nyear\n\n\n\n\n1\n317.600\n3078.500\n2.800\nGeneral Motors\n1935\n\n\n2\n391.800\n4661.700\n52.600\nGeneral Motors\n1936\n\n\n3\n410.600\n5387.100\n156.900\nGeneral Motors\n1937\n\n\n4\n257.700\n2792.200\n209.200\nGeneral Motors\n1938\n\n\n5\n330.800\n4313.200\n203.400\nGeneral Motors\n1939\n\n\n\n\n\n\n\nI have data from 11 firms: Their capital, market value, investment for each year between 1935 to 1954. This is a panel data, since I have multiple observations for each firm, on different time periods.\nLet’s say that I am interested in the relationship between market value and investment. For simplicity, if we had data on a single year we could estimate the following for each firm i:\n\\(\\displaystyle invest_i = \\beta_0 + \\beta_1 value_i + \\beta_2 capital_i + \\epsilon_i\\)\nHowever, there are things that we miss with this approach:\n\nThere might be firm-level variables that we would like to have in the model. These are assumed to be constant for a firm.\n\nThe idea is pretty neat actually. Think of having two years of data. Let’s say 1935 and 1936:\n\\(\\displaystyle invest_{i \\, 1936} = \\beta_0 + \\beta_{1}value_{i \\, 1936} + \\beta_{2}capital_{i \\, 1936} + \\beta_{3}\\alpha_i + \\epsilon_{i \\, 1936}\\)\n\\(\\displaystyle invest_{i \\, 1935} = \\beta_0 + \\beta_{1}value_{i \\, 1935} + \\beta_{2}capital_{i \\, 1935} + \\beta_{3}\\alpha_i + \\epsilon_{i \\, 1935}\\)\nNow, if you take the difference what happens is those \\(\\beta_{3}\\alpha_i\\) terms get cancelled. What you’re left with is:\n\\(\\displaystyle invest_{i \\, 1936} - invest_{i \\, 1935} = \\beta_{1}(value_{i\\,1936} - value_{i\\,1935}) + \\beta_{2}(capital_{i\\,1936} - capital_{i\\,1935}) + (\\epsilon_{i \\, 1936} - \\epsilon_{i \\, 1935})\\)\nI believe this is a very intuitive example. Accounting for unobserved firm-level characteristics is just adding firm as dummy in the regression!\n\nThe other thing that I haven’t mentioed above is the effects that are constant within a time period but may differ between years. These are shared between firms. Think of things like inflation, market trends etc.\n\nWell, I’ve got the idea. Let’s add that as a dummy as well?\n\nlm = smf.ols(\n    'invest ~ value + capital + C(firm) + C(year)',\n    data=df\n)\nres = lm.fit()\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ninvest\nR-squared:\n0.953\n\n\nModel:\nOLS\nAdj. R-squared:\n0.945\n\n\nMethod:\nLeast Squares\nF-statistic:\n122.1\n\n\nDate:\nSun, 12 Oct 2025\nProb (F-statistic):\n5.20e-108\n\n\nTime:\n01:14:10\nLog-Likelihood:\n-1153.0\n\n\nNo. Observations:\n220\nAIC:\n2370.\n\n\nDf Residuals:\n188\nBIC:\n2479.\n\n\nDf Model:\n31\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n18.0876\n18.656\n0.970\n0.334\n-18.715\n54.890\n\n\nC(firm)[T.Atlantic Refining]\n-112.5008\n17.752\n-6.337\n0.000\n-147.520\n-77.482\n\n\nC(firm)[T.Chrysler]\n-13.5993\n17.540\n-0.775\n0.439\n-48.199\n21.001\n\n\nC(firm)[T.Diamond Match]\n16.4928\n15.692\n1.051\n0.295\n-14.462\n47.448\n\n\nC(firm)[T.General Electric]\n-241.0850\n28.000\n-8.610\n0.000\n-296.319\n-185.851\n\n\nC(firm)[T.General Motors]\n-101.7696\n55.177\n-1.844\n0.067\n-210.615\n7.075\n\n\nC(firm)[T.Goodyear]\n-77.9628\n16.435\n-4.744\n0.000\n-110.383\n-45.543\n\n\nC(firm)[T.IBM]\n-6.4573\n16.271\n-0.397\n0.692\n-38.554\n25.640\n\n\nC(firm)[T.US Steel]\n100.5492\n28.438\n3.536\n0.001\n44.450\n156.648\n\n\nC(firm)[T.Union Oil]\n-56.7936\n16.403\n-3.462\n0.001\n-89.151\n-24.436\n\n\nC(firm)[T.Westinghouse]\n-41.7165\n17.483\n-2.386\n0.018\n-76.204\n-7.229\n\n\nC(year)[T.1936]\n-16.9592\n21.518\n-0.788\n0.432\n-59.407\n25.488\n\n\nC(year)[T.1937]\n-36.3756\n22.364\n-1.627\n0.106\n-80.492\n7.741\n\n\nC(year)[T.1938]\n-35.6237\n21.162\n-1.683\n0.094\n-77.370\n6.122\n\n\nC(year)[T.1939]\n-63.0994\n21.505\n-2.934\n0.004\n-105.522\n-20.677\n\n\nC(year)[T.1940]\n-39.8248\n21.626\n-1.842\n0.067\n-82.486\n2.836\n\n\nC(year)[T.1941]\n-16.4878\n21.529\n-0.766\n0.445\n-58.957\n25.982\n\n\nC(year)[T.1942]\n-17.9993\n21.275\n-0.846\n0.399\n-59.967\n23.968\n\n\nC(year)[T.1943]\n-37.7724\n21.415\n-1.764\n0.079\n-80.016\n4.471\n\n\nC(year)[T.1944]\n-38.3201\n21.459\n-1.786\n0.076\n-80.652\n4.012\n\n\nC(year)[T.1945]\n-49.5395\n21.687\n-2.284\n0.023\n-92.322\n-6.757\n\n\nC(year)[T.1946]\n-27.7544\n21.866\n-1.269\n0.206\n-70.888\n15.379\n\n\nC(year)[T.1947]\n-34.8775\n21.589\n-1.616\n0.108\n-77.464\n7.709\n\n\nC(year)[T.1948]\n-38.3307\n21.734\n-1.764\n0.079\n-81.204\n4.542\n\n\nC(year)[T.1949]\n-65.2008\n21.901\n-2.977\n0.003\n-108.404\n-21.998\n\n\nC(year)[T.1950]\n-67.3877\n22.028\n-3.059\n0.003\n-110.841\n-23.935\n\n\nC(year)[T.1951]\n-54.8346\n22.437\n-2.444\n0.015\n-99.095\n-10.574\n\n\nC(year)[T.1952]\n-56.4890\n22.819\n-2.475\n0.014\n-101.504\n-11.474\n\n\nC(year)[T.1953]\n-58.5126\n23.819\n-2.457\n0.015\n-105.500\n-11.525\n\n\nC(year)[T.1954]\n-81.7939\n24.204\n-3.379\n0.001\n-129.540\n-34.047\n\n\nvalue\n0.1167\n0.013\n9.022\n0.000\n0.091\n0.142\n\n\ncapital\n0.3514\n0.021\n16.696\n0.000\n0.310\n0.393\n\n\n\n\n\n\n\n\nOmnibus:\n32.466\nDurbin-Watson:\n0.988\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n180.276\n\n\nSkew:\n0.311\nProb(JB):\n7.14e-40\n\n\nKurtosis:\n7.391\nCond. No.\n3.92e+04\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.92e+04. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nYou can fit the same with PanelOLS, like below, and get a cleaner table.\n\nfe_model = PanelOLS.from_formula('invest ~ value + capital + EntityEffects + TimeEffects', data=df.set_index(['firm', 'year']))\nfe_res = fe_model.fit()\n\nfe_res.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ninvest\nR-squared:\n0.7253\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.7637\n\n\nNo. Observations:\n220\nR-squared (Within):\n0.7566\n\n\nDate:\nSun, Oct 12 2025\nR-squared (Overall):\n0.7625\n\n\nTime:\n01:14:15\nLog-likelihood\n-1153.0\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n248.15\n\n\nEntities:\n11\nP-value\n0.0000\n\n\nAvg Obs:\n20.000\nDistribution:\nF(2,188)\n\n\nMin Obs:\n20.000\n\n\n\n\nMax Obs:\n20.000\nF-statistic (robust):\n248.15\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n20\nDistribution:\nF(2,188)\n\n\nAvg Obs:\n11.000\n\n\n\n\nMin Obs:\n11.000\n\n\n\n\nMax Obs:\n11.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nvalue\n0.1167\n0.0129\n9.0219\n0.0000\n0.0912\n0.1422\n\n\ncapital\n0.3514\n0.0210\n16.696\n0.0000\n0.3099\n0.3930\n\n\n\nF-test for Poolability: 18.476P-value: 0.0000Distribution: F(29,188)Included effects: Entity, Time\n\n\nOne more thing though, check covariance type on both tables (nonrobust, unadjusted). It means errors are assumed to be independent which might be violated here. Think about it, observations are grouped in the sense that they belong to same firm. So, they share some unobserved component. Hence, errors might be correlated within each firm (across year).\nFor the same reason, errors might be correlated within each year (e.g., firms are subject to same inflation).\nSo, we should allow residuals to be correlated within groups.\nIt’s possible to use clustered covariance type with statsmodels but it doesn’t allow it to be 2 dimensional. In other words, you either cluster by entity dimension (e.g., firm) or time dimension (e.g., year). PanelOLS, on the other hand, allows for two-way clustering.\n\nfe_model = PanelOLS.from_formula('invest ~ value + capital + EntityEffects + TimeEffects', data=df.set_index(['firm', 'year']))\nfe_res = fe_model.fit(cov_type='clustered', cluster_entity=True, cluster_time=True)\n\nfe_res.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ninvest\nR-squared:\n0.7253\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.7637\n\n\nNo. Observations:\n220\nR-squared (Within):\n0.7566\n\n\nDate:\nSun, Oct 12 2025\nR-squared (Overall):\n0.7625\n\n\nTime:\n01:15:17\nLog-likelihood\n-1153.0\n\n\nCov. Estimator:\nClustered\n\n\n\n\n\n\nF-statistic:\n248.15\n\n\nEntities:\n11\nP-value\n0.0000\n\n\nAvg Obs:\n20.000\nDistribution:\nF(2,188)\n\n\nMin Obs:\n20.000\n\n\n\n\nMax Obs:\n20.000\nF-statistic (robust):\n84.060\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n20\nDistribution:\nF(2,188)\n\n\nAvg Obs:\n11.000\n\n\n\n\nMin Obs:\n11.000\n\n\n\n\nMax Obs:\n11.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nvalue\n0.1167\n0.0117\n10.015\n0.0000\n0.0937\n0.1397\n\n\ncapital\n0.3514\n0.0447\n7.8622\n0.0000\n0.2633\n0.4396\n\n\n\nF-test for Poolability: 18.476P-value: 0.0000Distribution: F(29,188)Included effects: Entity, Time\n\n\nI feel like this one is a very intuitive example but for more, you can check this."
  },
  {
    "objectID": "posts/How to Structure A Shiny App/index.html",
    "href": "posts/How to Structure A Shiny App/index.html",
    "title": "How To Structure A Shiny App: Brief Overview",
    "section": "",
    "text": "I’ve been working with Shiny Apps for a while, primarily using them to develop basketball analytics tools (example here). Recently, I started building a new version (live app) to improve structural issues I encountered.\nThis seemed like the perfect time to write a brief guide on structuring a Shiny app."
  },
  {
    "objectID": "posts/How to Structure A Shiny App/index.html#organizing-a-shiny-app",
    "href": "posts/How to Structure A Shiny App/index.html#organizing-a-shiny-app",
    "title": "How To Structure A Shiny App: Brief Overview",
    "section": "Organizing a Shiny App",
    "text": "Organizing a Shiny App\n\n\n\nShiny App Structure Example\n\n\nI structure my Shiny app using three key R files:\n- Library.R – Loads dependencies\n- Global.R – Contains utility functions\n- app.R – Runs the application\nIn app.R, I first load dependencies (source('Library.R')), followed by app-specific functions (source('Global.R')).\nThe UI definition comes next. Instead of keeping all UI code in a single file, I store different panels inside a dedicated UI/ folder. This makes it easier to manage each component separately.\nAt the bottom of app.R, before calling shinyApp, I include the server logic, which is housed in the Server/ folder."
  },
  {
    "objectID": "posts/How to Structure A Shiny App/index.html#the-server-structure",
    "href": "posts/How to Structure A Shiny App/index.html#the-server-structure",
    "title": "How To Structure A Shiny App: Brief Overview",
    "section": "The Server Structure",
    "text": "The Server Structure\nThe Server/ folder contains two separate files:\n- reactiveData.R – Handles data loading and preprocessing\n- data.R – Manages rendering and user interactions\n\nreactiveData.R\nThis file is responsible for loading and preprocessing data, including:\n- Converting column data types\n- Sorting data frames for better readability\n- Preparing datasets for interactive features\n\n\ndata.R\nThis file renders outputs and handles user interactions.\n\nRendering Tables\nFor dataframes, I use datatable with filtering enabled, allowing users to focus on specific sections of the data.\n\n\nHandling User Inputs\nWhen user interaction is required, such as submitting a request, I provide an action button (e.g., “Submit”) and monitor it using observeEvent.\nThe workflow:\n1. The user clicks the button\n2. The app captures the input\n3. Necessary calculations are performed\n4. The output is dynamically rendered\nThis logic can become complex depending on:\n- The type of input (numeric vs. categorical)\n- The complexity of calculations\n- The number of panels and inputs\nFor a real-world example, check out the data.R file from my old Euroleague App, which contained different plotting options and expected different inputs.\n\nI’m hoping to finish both of my apps this month. If you’re interested, keep an eye on my LinkedIn. Have a nice weekend!"
  },
  {
    "objectID": "posts/Man on a Mission - January 2025/index.html",
    "href": "posts/Man on a Mission - January 2025/index.html",
    "title": "Man on a Mission: January 2025",
    "section": "",
    "text": "As I promised at the start of the year, here’s what I have studied this month.\n\n\nStatistics\nThis month I finished Think Bayes way before than I anticipated, and played with PyMC a bit. I really enjoyed the whole book but wanted to dive a bit deep so moved on to John Kruschke’s Doing Bayesian Data Analysis and I’m glad I did it. The book starts with main ideas of Bayesian view, “Bayesian inference is reallocation of credibility across possibilities” where credibility stands for probability and possibilities stand for possible values that our parameters can take when we attempt to model. After this introduction, Kruschke talks about what probabilities are and provides intuitive description for the distinction between probability mass and probability density. If you have trouble understanding probability distributions, you may want to take a look at it.\nBayesian stuff aside, I really enjoyed the interpretation of mean as minimized variance alongside how we end up with median and mode when we change the distance measure (e.g., l2 norm to l1 etc.). Also, I coded Metropolis Algorithm (1-D) by using the example in the MCMC chapter. You can find it on my GitHub under learn-stats-with-sims.\nI’m currently at chapter 9, going through hierarchical models. We’ll see if the book keeps me engaged throughout the upcoming chapters as well.\n\n\n\nDeep Learning (DL)\nWell, I haven’t done much for the DL. Throughout the year, amount of time I spent on statistics will gradually decline to open up space for DL. I’ve just started to Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, skimming through Scikit-Learn part to start with Keras and Tensorflow while not missing out on anything (despite using sklearn pretty often, I don’t want to miss a useful thing that may come in handy in the future).\n\nIn terms of sports analytics or production level data science, I haven’t done more than reading a few blog posts. Data science stuff aside, I have improved a lot on the website, added new functionalities such as widget to subscribe to the email list which allows you to get notified when I write a new post. It took quite some time to handle the front-end, so I count that as a win as well.\nSee you next month!"
  },
  {
    "objectID": "posts/Man on a Mission - February 2025/index.html",
    "href": "posts/Man on a Mission - February 2025/index.html",
    "title": "Man on a Mission: February 2025",
    "section": "",
    "text": "Another month has passed. In these monthly posts, I won’t dive deep into every little bit of stuff that I studied during the month (that would be impossible), what I’ll try to convey the taste to increase your appetite.\n\n\nExplainable AI\nThe main focus of the month was on Molnar’sInterpretable ML:\nI feel like many people miss out on explainable AI, maybe not interested in it as much as making predictions. Before my 9-6, I was more into causal inference and maybe that’s why I get drawn to explaining so-called black box models (since explanations are causal to the model). Even if you’re not into it, it can help you debug the model and some methods (such as SHAP) can be used for feature selection.\nThe book starts with generalized linear models (GLMs), including the good old classic linear regression. These are highly interpretable models by design, along with decision trees. This section ends with RuleFit, where you learn interactions through decision tree(s) and use those with a sparse linear model. I didn’t like this approach very much since rules may not be independent (e.g., \\(x_1 &gt; 20\\), \\(x_1 &gt; 30\\)).\nThe next section covers the model-agnostic methods, methods that are applicable regardless of the model. These mainly consist of permutation importance, partial dependence plots, accumulated local effects, SHAP etc. Some of these methods provide explanations on a local scope (you can use those methods to explain individual predictions) while others (global scope methods) give an expected value.\nThen there are example based explanations, which I like since they are relatively easy to grasp: What would have happen to the prediction if I changed the value of a feature? These explanations are contrastive, making it easier to grasp for anyone.\nThe book itself is great, gives you the libraries that you can use to implement the methods.\n\n\nStatistics\nI didn’t make a lot of progress here, other than going over some of the stuff that I’m already familiar with. I focused on explainable AI since I have a problem at work that may require using it. But, I listened to Learning Bayesian Statistics podcasts a lot. This brings me to the another point: I read a lot of blog posts this month, related to statistics. I wish I noted them somewhere. From now on, I will note what I listen and read as well and include them in my upcoming monthly blog posts!\n\nLastly, I started to become active on my YouTube channel (hoping to keep it up, it takes a lot of time). I’m thinking about doing a “study with me”-like a thing so if you’re interested, don’t forget to subscribe!\nThat’s pretty much it for today. See you next week!"
  },
  {
    "objectID": "posts/Nicer Contour Plots/index.html",
    "href": "posts/Nicer Contour Plots/index.html",
    "title": "Nicer Contour Plots",
    "section": "",
    "text": "I’m going to keep it short this week, been studying a lot lately (you may see more deep learning and explainable AI posts from me in the future). Anyway, let’s dive:\nI realized contour plots in my previous post (here) look extremely bad: - There are no labels annotating the contour lines - No colorbar - No fill between contour lines\nSo, let’s try to make them better. Before we move on, 2 things: 1. If you are not familiar with what contour plots are, Grant Sanderson (3blue1brown) has a great video on Khan Academy that explains what they are: You can watch it here. It allows you to plot 2-D functions (\\(f(x, y)\\)) 2. I’m going to skip to the plotting, not showing processes up to that point. If you want to follow along, you can find the previous steps here.\n\ndata = df_likeli.unstack()\ndata.iloc[:5, :5].head()\n\n\n\n\n\n\n\n\n-1.400000\n-1.393636\n-1.387273\n-1.380909\n-1.374545\n\n\n\n\n-1.200000\n4.812632e-15\n5.218338e-15\n5.654017e-15\n6.121460e-15\n6.622520e-15\n\n\n-1.192857\n5.117237e-15\n5.550729e-15\n6.016454e-15\n6.516356e-15\n7.052453e-15\n\n\n-1.185714\n5.437699e-15\n5.900567e-15\n6.398073e-15\n6.932326e-15\n7.505517e-15\n\n\n-1.178571\n5.774592e-15\n6.268495e-15\n6.799591e-15\n7.370167e-15\n7.982601e-15\n\n\n-1.171429\n6.128495e-15\n6.655158e-15\n7.221728e-15\n7.830684e-15\n8.484602e-15\n\n\n\n\n\n\n\nThis was the joint likelihood from the post that I have mentioned above, and here’s the ugly contour plot:\n\nut.plot_contour(df_likeli.unstack())\nut.decorate(title = 'Likelihood')\n\n\n\n\n\n\n\n\nLet’s start by being more explicit. I’ll use matplotlib’s contour for plotting. I provide variables (in this case, column values and indices were corresponding to our model’s parameter spaces), data itself in 2D form, number of lines, and a color map.\n\nplt.contour(data.columns, data.index, data, 10, cmap='viridis')\n\n\n\n\n\n\n\n\nThis looks very similar to our ugly version, lots of information get lost in this one. Let’s play with it, starting by using contourf instead of contour which fills the spaces in between.\n\nplt.contourf(data.columns, data.index, data, 6)\n\n\n\n\n\n\n\n\nNow, this was a step forward. However, we have no idea what those colors correspond to.\n\nplt.contourf(data.columns, data.index, data, 6)\nplt.colorbar()\n\n\n\n\n\n\n\n\nThis looks better but is there more room to improve? I don’t like this discreteness between contours, want more of a smooth transition. imshow allows for such transition, but you need to provide extent since the function does not take x and y as inputs. Also, you need to specify the origin because the default origin is top left.\n\nplt.imshow(data, extent=[data.columns.min(), data.columns.max(), data.index.min(), data.index.max()], origin='lower',\ncmap='viridis')\nplt.colorbar()\n\n\n\n\n\n\n\n\nThis definitely looks better, but one last touch: Adding contour lines with labels (clabel).\n\ncontours = plt.contour(data.columns, data.index, data, 6, colors='black')\nplt.clabel(contours, inline=True, fontsize=8)\n\nplt.imshow(data, extent=[data.columns.min(), data.columns.max(), data.index.min(), data.index.max()], origin='lower',\ncmap='viridis', alpha=1)\n\nplt.colorbar()\n\n\n\n\n\n\n\n\nThat’s a quick guide to make your contour plots better. Hope you enjoyed it. If you did and want more, you may want to take a look at Python Data Science Handbook by Jake VanderPlas. I learned some tricks from it.\nHave a nice weekend!"
  },
  {
    "objectID": "posts/How to Automatize Your Notebooks/index.html",
    "href": "posts/How to Automatize Your Notebooks/index.html",
    "title": "How To Automatize Your Tasks",
    "section": "",
    "text": "Last Sunday, I spent my whole day automatizing my sports analytics codes to run weekly. During the season, I need to update the data regularly which was holding me back from focusing on the things that I want the most: modeling and analyzing. I ended up not doing anything at all and stepped away from basketball analytics. However, I can finally get back to it since the data gets pulled while I’m sleeping. Here’s how I did it.\n\n\nConverting Notebooks to Scripts\nI usually run my codes in python notebooks, cell by cell structure makes it easy to check plots and follow my thoughts when I take a break and come back. If you’re using notebooks as well, to automize you need to turn them into “.py” files. Luckily, with nbconvert, you don’t have to copy-paste all the stuff. With the following command, your notebooks become scripts:\njupyter nbconvert --to script notebook_name.ipynb\nwhere “notebook_name” is the placeholder for the notebook. If you want to specify the output path along with changing its name:\njupyter nbconvert --to script notebook_name.ipynb --output /path/to/dir/script_name.py\n\n\nCreating a Batch File\nNext step is to create a batch file that you will schedule later with the help of Windows’ Task Scheduler. All you need to do is to open up a text editor and add the following commands:\n@echo off\n\n\"C:\\Path\\To\\Python\\python.exe\" \"C:\\Path\\To\\script_name.py\"\nThis just runs a single file, but if you’re interested in running more than one sequentially, all you have to do is to add a second line to the code above. Also, if you do so, I suggest you to add pauses in between like the following for the test run:\n@echo off\n\n\"C:\\Path\\To\\Python\\python.exe\" \"C:\\Path\\To\\script_name.py\"\npause\n\n\"C:\\Path\\To\\Python\\python.exe\" \"C:\\Path\\To\\script_name_2.py\"\npause\nTo test, you just need to run the .bat file that you save with the text editor. Pauses keeps the bash-cmd open anytime it finishes running a script or if it encounters an error. It expects you to press a button to move onto the next file. If it runs smoothly (i.e., if there is no errors), delete the pauses and save the .bat again.\nAdditionally, you can give path to your Python interpreter if you’re using any environment. I do so, since my codes depend on certain packages.\n\n\nScheduling with Task Scheduler\nOn Windows, search for “Task Scheduler” under Start and open it. From the right panel that is titled as “Actions”, click “Create Basic Task”. Name it appropriately and add a description if you need to. Once you click “Next”, you’ll see trigger options. I’m running my scripts weekly, but there are bunch of different options. Select the one that you want and click next again. For the action you want the task to perform, select “Start a Program” and locate your batch file. Click next and lastly, check the summary. Don’t worry, you can change the triggers later if you change your mind, since they appear under Task Scheduler.\n\nThat’s pretty much it. I hope you make good use of Task Scheduler."
  },
  {
    "objectID": "posts/Model Fitting with Likelihood/index.html",
    "href": "posts/Model Fitting with Likelihood/index.html",
    "title": "Understanding Model Fitting with Likelihood",
    "section": "",
    "text": "In daily life, “probability” and “likelihood” are used interchangeably. But, they don’t refer to the same thing. I believe the most intuitive way to distinguish both is by thinking about the information you have while answering questions:\n\nProbability questions start with known model and unknown data. If someone asks you about the chance of seeing 6 heads in 10 coin tosses, you don’t have any observations. But, you have the model: You know the process that generates “heads” and “tails” and it’s parameter value (usually expressed as theta θ): 0.5.\nLet’s say you did 10 coin tosses and ended up with 9 heads. How likely you were to observe 9 if the coin was fair? In this situation, the data is known and the question refers to the underlying data generating process (i.e., refers to the parameter).\n\nWe can use this type of approach and let the parameter values vary and calculate the likelihood of the data under each value of the parameter(s).\n\nimport numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport statsmodels as sm\nimport empiricaldist as emd\nimport scipy.stats as st\n\nimport utils as ut\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n\npossum = pd.read_csv('possum.csv', usecols=['pop', 'tail_l'])\ndf = possum.copy()\n\ndf.head()\n\n\n\n\n\n\n\n\npop\ntail_l\n\n\n\n\n0\nVic\n36.0\n\n\n1\nVic\n36.5\n\n\n2\nVic\n39.0\n\n\n3\nVic\n38.0\n\n\n4\nVic\n36.0\n\n\n\n\n\n\n\nWe’ll use possum data, where each row corresponds to features of a possum from Australia and New Guinea. Let’s attempt to predict which region a possum is from as a function of tail length. (You can get the data from here)\nI assume basic knowledge of logistic regression, which we’ll use to model the relationship between our predictor and the binary target variable.\n\nqs_cept = np.linspace(-3, 3, 241) # possible values for intercept (beta_0)\nqs_slope = np.linspace(-1, 1, 81) # possible values for slope (beta_1)\n\n\nprint(qs_cept[:5], '\\n', qs_slope[:5])\n\n[-3.    -2.975 -2.95  -2.925 -2.9  ] \n [-1.    -0.975 -0.95  -0.925 -0.9  ]\n\n\n\ndf_likeli = pd.DataFrame(index=range(len(qs_cept)), columns=range(len(qs_slope)))\ndf_likeli.index = qs_cept\ndf_likeli.columns = qs_slope\n\n\ndf_likeli.fillna(1, inplace=True)\n\ndf_likeli.head()\n\n\n\n\n\n\n\n\n-1.000\n-0.975\n-0.950\n-0.925\n-0.900\n-0.875\n-0.850\n-0.825\n-0.800\n-0.775\n...\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\n\n\n\n\n-3.000\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n-2.975\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n-2.950\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n-2.925\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n-2.900\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n5 rows × 81 columns\n\n\n\nSo, what I have done is I created grid of equally spaced values both for intercept and the slope. I put them into a dataframe where columns represent possible values for slope and indices represent possible values for intercept.\nNow, I ask this question: For each pair of intercept and slope, b0 and b1, how likely I am to see the observed data?\n\ndf['pop'] = df['pop'].apply(lambda x: 1 if x == 'Vic' else 0)\n\n\n# Centering data\n\noffset = df['tail_l'].mean().round()\ndf['x'] = df['tail_l'] - offset\ndf['y'] = df['pop']\n\n# I refer our predictor as x from now on (for convenience), y becomes our target variable which takes 1 if possum is from Victoria region and 0 otherwise.\n\n\nagg_data = df.groupby('x')['y'].agg(['sum', 'count'])\nagg_data.head(10).T\n\n\n\n\n\n\n\nx\n-5.0\n-3.5\n-3.0\n-2.5\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n0.5\n\n\n\n\nsum\n2\n1\n4\n2\n6\n6\n9\n4\n4\n1\n\n\ncount\n2\n1\n5\n2\n9\n7\n13\n12\n6\n4\n\n\n\n\n\n\n\n\nns = agg_data['count'] # represents number of observation with corresponding x values\nks = agg_data['sum'] # represents successes, which means Victoria region\n\nPeople get confused, rightfully, when they hear logistic regression getting mentioned as a linear model since all they see is an S-shaped function in a classic graph where x-axis represents the predictor and y-axis represents the probability. But that squiggle is a result of a transformation of log odds into probabilities. You may have deduced that before if you have taken a look at the equation:\n\\(\\displaystyle log_e(\\frac{p_i}{1 - p_i}) = \\beta_0 + \\beta_1 x_{1i} + ... + \\beta_k x_{k_i}\\)\nHowever, we know that odds can be expressed as probabilities, so we’ll make that transformation.\nFor representation let’s select an intercept and a slope:\n\ncept = 0\nslope = 1\n\n\nx_values = agg_data.index\nlog_odds = cept + slope * x_values\nodds = np.exp(log_odds)\nps = odds / (odds + 1)\nps[:6] # Probabilities coming from the model\n\nIndex([0.006692850924284856, 0.029312230751356316,  0.04742587317756679,\n        0.07585818002124356,  0.11920292202211755,  0.18242552380635632],\n      dtype='float64', name='x')\n\n\nHow likely I am to observe k success in n trials with \\(p_i\\) where \\(p_i\\) comes from the model above.\n\n# How likely I am to observe k success in n trials with p_i where p_i comes from model\nlikelis = st.binom.pmf(ks, ns, ps)\nlikelis[:6]\n\narray([4.47942535e-05, 2.93122308e-02, 2.40951774e-05, 5.75446348e-03,\n       1.64675234e-04, 2.10930303e-04])\n\n\n\n# Taking the product for whole data\nlikelis.prod()\n\n6.58621661515704e-55\n\n\nLet’s take a look at the fit:\n\nplt.figure(figsize=(6,4))\n\nplt.plot(x_values+offset, ps, label='random selected model', color='C1')\nplt.scatter(df['x']+offset, df['y'], s=30, label='data')\n\nut.decorate()\n\n\n\n\n\n\n\n\nThis was for only one pair of intercept and slope, and it seems like we can do better. We calculated the likelihood for educational purposes here but in general, likelihoods by themselves does not mean much (I try to choose my words carefully here). Let’s try other possible pairs for our parameters and compare the likelihoods of each to take the one that maximizes the likelihood of the observed data.\n\nfrom scipy.special import expit\n\nlikelihood = df_likeli.copy()\n\nfor cept in likelihood.index:\n    for slope in likelihood.columns:\n        probs = expit(cept + slope * x_values) # transformation to probabilities\n        likelis = st.binom.pmf(ks, ns, probs) # likelihood of each observation\n        likelihood.loc[likelihood.index == cept, slope] = likelis.prod() # likelihood of the whole data under the selected pair of parameter values\n\n\nrow, col = likelihood.stack().idxmax()\nprint(row, col)\n\n-0.32499999999999973 -0.7\n\n\nThis is the pair that maximizes the likelihood.\n\ncept = row\nslope = col\n\nlog_odds = cept + slope * x_values\nodds = np.exp(log_odds)\nps = odds / (odds + 1)\n\n\nplt.figure(figsize=(6,4))\n\nplt.plot(x_values+offset, ps, label='model', color='C1')\nplt.scatter(df['x']+offset, df['y'], s=30, label='data')\n\nut.decorate()\n\n\n\n\n\n\n\n\nLet’s check our parameter estimates with the help of statsmodels:\n\nimport statsmodels.formula.api as smf\n\nformula = 'y ~ x'\nresults = smf.logit(formula, data=df).fit(disp=False)\nresults.params\n\nIntercept   -0.320498\nx           -0.699641\ndtype: float64\n\n\nWe didn’t get those exact values since we used grid approximation, and the grid that I created didn’t include those exact values. Let’s drop a few details here:\n\nAs the parameter space gets larger, the computational process becomes slower. So, there’s that trade-off between precision and time.\nWe only had two parameters to estimate and that’s OK for grid approach. However, once you start to get more than three, number of possible combination for parameters gets incredibly large which makes grid approach infeasible.\nAnother thing is related to the number of observation while calculating the likelihood for the whole data: When you try to take the product of the likelihoods with many observations, you may ran into an issue called underflow, where computer has trouble multiplying bunch of values around zero. Hence, log likelihood comes to rescue: Logarithms takes the values around zero away from zero, thus solving the underflow problem.\n\nI hope this example made likelihoods more intuitive. There’s more to know about them: Are they probability densities? Are they probability masses? Or are they both? That’s for a whole different post, things can get messy in a second. However, it’s important to understand likelihoods in the context above, since they can be used to update our beliefs about parameters which is what we do with Bayesian methods.\nAs usual, have a nice weekend :)"
  },
  {
    "objectID": "posts/Interpretablog - ALEs/index.html",
    "href": "posts/Interpretablog - ALEs/index.html",
    "title": "Interpretablog - Handling the Enemy with ALEs",
    "section": "",
    "text": "I mentioned partial dependence plots (PDPs) before and how things get odd when the predictors are correlated. While varying one variable, we might be creating data points that are very unlikely or even impossible (i.e., we don’t take joint distribution into account).\nFirst solution that comes to mind is to average over conditional distribution. However, then the effect is the combination of variables. What we want is the isolated effect.\nThere comes the accumulated local effects (ALEs): Taking the difference between predictions in a small window, hence canceling the other variables.\nLet’s exemplify the whole thing to make it more concrete.\n\ndf = df[['head_l', 'total_l', 'tail_l']]\ndf.head()\n\n\n\n\n\n\n\n\nhead_l\ntotal_l\ntail_l\n\n\n\n\n0\n94.1\n89.0\n36.0\n\n\n1\n92.5\n91.5\n36.5\n\n\n2\n94.0\n95.5\n39.0\n\n\n3\n93.2\n92.0\n38.0\n\n\n4\n91.5\n85.5\n36.0\n\n\n\n\n\n\n\n\nplt.figure(figsize=(7, 5))\n\nplt.scatter(df['head_l'], df['total_l'], alpha=.67, s=15)\n\nplt.xticks(np.arange(80, 105, 3))\n\nplt.xlabel('Head length')\nplt.ylabel('Total length')\n\nText(0, 0.5, 'Total length')\n\n\n\n\n\n\n\n\n\nWithin the whole range, the correlation is very obvious. However, if we zoom into any small window…\n\nplt.figure(figsize=(7, 5))\n\nplt.scatter(df['head_l'], df['total_l'], alpha=.67, s=15)\nplt.plot(np.repeat(95, 30), np.arange(70, 100, 1), ls='--', color='black', alpha=.67)\nplt.plot(np.repeat(96, 30), np.arange(70, 100, 1), ls='--', color='black', alpha=.67)\n\nplt.xticks(np.arange(80, 105, 3))\n\nplt.xlabel('head_l')\nplt.ylabel('total_l')\n\nText(0, 0.5, 'total_l')\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(7, 5))\n\nplt.scatter(df['head_l'], df['total_l'], alpha=.67, s=15)\n\nplt.xlim(95, 96)\n\n\n\n\n\n\n\n\nNow, not so much. That’s what we are going to take advantage of.\n\nX = df[['head_l', 'total_l']]\ny = df['tail_l']\n\n\nale = ali.ALE(rf.predict, feature_names=X.columns, target_names=['tail_l'])\nexp = ale.explain(X.to_numpy())\n\n\nali.plot_ale(\n    exp, \n    features=[0,1], \n    fig_kw={'figwidth':10, 'figheight': 4}\n)\n\narray([[&lt;Axes: xlabel='head_l', ylabel='ALE'&gt;,\n        &lt;Axes: xlabel='total_l', ylabel='ALE'&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nWhat those plots tell us?\nWell, it seems like total_l has larger effect compared to head_l, as the total_l increase so does the height prediction of the model.\nThe plots are centered, which changes the interpretation – makes it relative to the average.\nFor example, total_l of 75 decreases the predicted tail_l by 3 relative to the average prediction.\nI hope you enjoyed this one. If you did, don’t forget to follow me on LinkedIn.\nUntil next time, take care!"
  },
  {
    "objectID": "posts/Understanding distribution of log(odds ratio) through simulation/index.html",
    "href": "posts/Understanding distribution of log(odds ratio) through simulation/index.html",
    "title": "Understanding Distribution of log(odds ratio) Through Simulation",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\nimport seaborn as sns\n\nimport scipy.stats as st\n\nI wanted to put this example out because one of my colleague asked me a question about result table of logistic regression. Back in the day while I was watching the video, Josh Starmer said distribution of log(odds ratio) is approximately normal and gave an idea of how to simulate. I didn’t end up his way of simulating, if you want that maybe you should follow the approach in the video but we actually both do the same thing.\nLet’s say we have the following information:\n\ncancer_gene_cb = pd.crosstab(['yes', 'no'], ['yes', 'no'], rownames=['mutated_gene'], colnames=['has_cancer'])\ncancer_gene_cb.loc['yes', 'yes'] = 23\ncancer_gene_cb.loc['yes', 'no'] = 117\ncancer_gene_cb.loc['no', 'no'] = 210\ncancer_gene_cb.loc['no', 'yes'] = 6\n\ncancer_gene_cb\n\n\n\n\n\n\n\nhas_cancer\nno\nyes\n\n\nmutated_gene\n\n\n\n\n\n\nno\n210\n6\n\n\nyes\n117\n23\n\n\n\n\n\n\n\nLet’s represent the information in a different way. For the ones who are not familiar, odds is ratio of something happening to something not happening. When there’s odds ratio, it actually means ratio of odds.\n\n# Given the mutated version of the gene, odds of having cancer\nmutated_gene_odds = cancer_gene_cb.loc['yes', 'yes'] / cancer_gene_cb.loc['yes', 'no'] # 23 / 117\n\n# Given the wild version of the gene, odds of having cancer\nwild_gene_odds = cancer_gene_cb.loc['no', 'yes'] / cancer_gene_cb.loc['no', 'no'] # 6 / 210\n\nodds_ratio = mutated_gene_odds / wild_gene_odds # 6.88\n\nlog_odds_ratio = round(np.log(odds_ratio), 2)\nprint(log_odds_ratio)\n\n1.93\n\n\nIt seems like people who have the mutated gene have higher odds (i.e., 6.88 times more likely) in terms of having cancer. But I wonder if this is statistically significant.\nSo, I want to do a simulation see how often would I see the log(odds ratio) of 1.93 under the assumption of no relationship between the two variables. If I don’t see it very often, it makes sense to act as if there’s a difference.\n\nLet’s go through a simulation:\n\nli = [] # creating an empty bag\nsample_size = 356 # our sample size\n\nCreating 29 “cancer” written cards and putting them in a bag, representing the number of people with cancer in our sample.\n\nfor i in range(0, 29):\n    li.append('cancer')\n\nCreating 327 “no cancer” written cards and putting them in a bag, representing the number of people without cancer in our sample.\n\nfor j in range(0, 327):\n    li.append('no cancer')\n\n\nmutated_version = []\nwild_version = []\n\nThere are 140 people with mutated variant of the gene, 216 people with wild variant. Under the assumption of no relationship between the cancer and the gene variant, I’d expect no difference between odds of cancer given the mutated gene and given the wild gene. In other words, knowing the gene variant does not provide any meaningful information.\n\nfor i in range(10000):\n    sampl_all = np.random.choice(li, size=len(li)) #  Shuffling the bag (cards)\n\n    #  Taking 140 of them (representing the number of participants with mutated gene)\n    sampl_mutated_version = sampl_all[:140].tolist()\n    # and calculating the odds in favor of cancer in the mutated gene group\n    mutated_version.append(sampl_mutated_version.count('cancer') / sampl_mutated_version.count('no cancer')) \n\n    #  Taking rest of the cards (representing the number of participants in the non-mutated gene group)\n    sampl_wild_version = sampl_all[140:].tolist() \n    #  and calculating the odds in favor of cancer in the non-mutated gene group\n    wild_version.append(sampl_wild_version.count('cancer') / sampl_wild_version.count('no cancer'))\n\n\nmutated_version_nd = np.array(mutated_version)\nwild_version_nd = np.array(wild_version)\n\n\nodds_ratio = mutated_version_nd / wild_version_nd #  Calculating odds ratio\n\n\nlog_odds_ratio = np.log(odds_ratio) #  Calculating log(odds ratio)\n\n\n#  Plotting the distribution of log(odds ratio)\n\nfig = plt.figure(figsize=(8, 6))\ng = sns.histplot(data=log_odds_ratio, bins=40, color=\"orange\", edgecolor=\"black\")\n\ng.xaxis.set_minor_locator(AutoMinorLocator())\n\ng.tick_params(which=\"both\", width=2)\ng.tick_params(which=\"major\", length=7)\ng.tick_params(which=\"minor\", length=4)\n\nplt.show(g)\n\n\n\n\n\n\n\n\n\nprint(round(np.mean(log_odds_ratio), 2))\nprint(round(np.std(log_odds_ratio), 2))\n\n-0.02\n0.42\n\n\nApparently it can be approximated by normal distribution. It is centered around zero with standard deviation of 0.42. Let’s see how often I would observe a result as or more extreme as 1.93.\n\nlen([val for val in log_odds_ratio if abs(val) &gt; 1.93]) / len(log_odds_ratio)\n\n0.0002\n\n\nSince I simulate the whole thing under the assumption of no difference between groups, selected alpha level should correspond to my long-term type I error rate. If alpha = 0.05, I should observe 5% values to fall more than 2 standard deviation (approximately). Let’s check if it’s the case.\n\nlen([val for val in log_odds_ratio if abs(val) &gt; 2 * np.std(log_odds_ratio)]) / len(log_odds_ratio)\n\n0.0479\n\n\nI really tried to simplify things here, to not make a huge mess out of this but another approach would be to fit a theoretical distribution, such as normal distribution, since that’s what the Wald Test does and then step to integrating over the area that fall larger than absolute value of 1.93 . It also requires a few extra steps, such as calculating estimated standard deviation. For the ones who would like to play with the idea, here’s the video link again.\nHave a nice weekend."
  },
  {
    "objectID": "posts/Regression for Causal Inference/index.html",
    "href": "posts/Regression for Causal Inference/index.html",
    "title": "Residuals, Multicollinearity, and the Problem with Treatment-Only Predictors",
    "section": "",
    "text": "I came across this post on LinkedIn yesterday and I wanted to elaborate bit more on why you should not include the ones that predict only treatment, for the purpose of making it more intuitive. It’s hidden in regression…\nIn many textbooks, the way to estimate linear regression parameters (without calculus/iterative solutions) is only given in a bivariate setting. When moved to “multivariate”, they let the programming do the job. And I get it, it makes sense, since this is what we’re doing in real life when we’re at the job. However, a simple expression can make things way smoother:\n\\(\\displaystyle y_i = \\beta_0 + \\beta_1{X_1} + \\beta_2{X_2} + ... + \\beta_p{X_p}\\)\nWe can get any beta by:\n\\(\\displaystyle \\frac{\\text{Cov}(Y_i, r_p)}{\\text{Var}(r_p)}\\) where \\(r_p\\) is residuals from a regression: \\(X_p\\) ~ \\(X_{-p}\\)\nIn plain English, \\(r_p\\) is the residuals from a regression of \\(X_p\\) onto other predictors that are not \\(X_p\\). That’s why the interpretation in regression is as value of variable \\(X_p\\), after accounting for all the other predictors. Since \\(r_p\\) is the portion of the \\(X_p\\) that is purged from other variables (i.e., part of \\(X_p\\) that cannot be explained by other predictors).\nNow, let’s think about this throughly (and about how beatiful this is) and I actually mentioned this on the last interpretablog post (actually, stuff above can be understood from that post as well): If there’s high multicollinearity, there wouldn’t be much variance left since most of the variable would get explained by others. Well, if that variable is the treatment, that would cause issue right? Since there isn’t much variance left in the treatment.\nLet’s put it this way: Imagine you want to estimate the effect of a campaign. If every customer has taken the campaign, hence no variance, you’d have hard time estimating its effect, right? Ofc., this is not specific to the “treatment”. You may have actually came across this before, if you have ever looked at a weight plot. When there isn’t much variance in a variable, the weight plot would show its “lines” wider — reflecting its large standard error, in line with how difficult it is to estimate its coefficient. If you haven’t seen one before, here’s one below that I borrowed from Molnar’s Interpretable ML.\n\n\n\nWell, about “including the good predictors of the outcome”: Even if they are not confounders, you should include good predictors of the outcome to reduce its variance. This is not a must but it should help specifically in situations where most of the variance can be explained by other factors that are not the treatment. Hence, you’re trying to understand how much of the leftover variability does your treatment explains, after accounting for the other factors that have BIG say in the outcome.\nI usually think of “gene” related examples: Like estimating the effect of diet on adolescent height. I suspect gene to explain big portion of height, so controlling for gene would help. I am not sure if this is proper in scientific sense, but you got the idea.\nI hope you enjoyed reading this. I believe there are a lot of issues in internalizing the regression so I’ll follow this one up with unidentifiability issue, related to including variables.\nSee you next time."
  },
  {
    "objectID": "posts/Man on a Mission - Starting 2025/index.html",
    "href": "posts/Man on a Mission - Starting 2025/index.html",
    "title": "Man on a Mission: Starting 2025",
    "section": "",
    "text": "Some time ago, I stumbled upon the website Less Certainty, More Inquiry. If you click on the link, you’ll see that Darryl Blackport shares what he listens to or reads every week of the year. I found the concept intriguing and thought it would be a great fit for me, as I’m often immersed in studying or reading something.\nThis year, I’m taking inspiration from his approach. I’ll share what I study each month, providing a brief overview of the topics I’ve explored, focusing on what I found particularly interesting or useful. To kick things off, here are my learning goals for 2025. Let’s dive in.\n\n\n\n\n\n\nFor most of my data science journey, I’ve taken a theory-first approach, focusing heavily on statistics. However, with recent advancements in tools like LLMs, I think it’s time for a shift. In 2025, I plan to delve into deep learning (a domain I’ve avoided so far) with a hands-on approach.\nHere’s my roadmap for deep learning:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n\nAdvanced Learning Algorithms\n\nDeep Learning Specialization\n\n\n\n\n\n\n\nI’ve been passionate about sports analytics for nearly a year now and have made significant strides in basketball analytics. (You can check out some of my work here.)\nThis year, I aim to expand my focus by creating videos on football (soccer) analytics, in addition to basketball, on my YouTube channel. To prepare for this, I’ve started the following specialization:\n\nSports Performance Analytics Specialization\n\nAlso, Dean Oliver’s new book, Basketball Beyond Paper, is on my reading list—it’s a must-read for anyone in this space.\n\n\n\n\n\n\nI consider statistics to be one of my stronger skills compared to other data scientists in the industry (at least in Turkey), but there’s always room for growth.\nCurrently, I’m halfway through Think Bayes, and I’m looking forward to Tom Faulkenberry’s upcoming book when it’s released.\nThere are specific topics, like meta-analysis, that I want to strengthen, but I don’t plan to tackle them systematically. Instead, I’ll explore them as curiosity strikes.\n\n\n\n\n\n\nThis one’s a “maybe.” Deep learning and LLMs are higher priorities, and the list is already packed. However, gaining production-level data science expertise could be beneficial in the long run.\nFor now, it’s something I’ll keep on the back burner, but we’ll see how the year unfolds.\n\n\n\n\n\n\n\nThis blog post was polished with help of ChatGPT! I write the whole piece and feed it to ChatGPT to improve the flow.\nAnd that’s pretty much it for my 2025 learning goals. Stay tuned for updates!"
  },
  {
    "objectID": "posts/Man on a Mission - Starting 2025/index.html#man-on-a-mission-series",
    "href": "posts/Man on a Mission - Starting 2025/index.html#man-on-a-mission-series",
    "title": "Man on a Mission: Starting 2025",
    "section": "",
    "text": "Some time ago, I stumbled upon the website Less Certainty, More Inquiry. If you click on the link, you’ll see that Darryl Blackport shares what he listens to or reads every week of the year. I found the concept intriguing and thought it would be a great fit for me, as I’m often immersed in studying or reading something.\nThis year, I’m taking inspiration from his approach. I’ll share what I study each month, providing a brief overview of the topics I’ve explored, focusing on what I found particularly interesting or useful. To kick things off, here are my learning goals for 2025. Let’s dive in.\n\n\n\n\n\n\nFor most of my data science journey, I’ve taken a theory-first approach, focusing heavily on statistics. However, with recent advancements in tools like LLMs, I think it’s time for a shift. In 2025, I plan to delve into deep learning (a domain I’ve avoided so far) with a hands-on approach.\nHere’s my roadmap for deep learning:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n\nAdvanced Learning Algorithms\n\nDeep Learning Specialization\n\n\n\n\n\n\n\nI’ve been passionate about sports analytics for nearly a year now and have made significant strides in basketball analytics. (You can check out some of my work here.)\nThis year, I aim to expand my focus by creating videos on football (soccer) analytics, in addition to basketball, on my YouTube channel. To prepare for this, I’ve started the following specialization:\n\nSports Performance Analytics Specialization\n\nAlso, Dean Oliver’s new book, Basketball Beyond Paper, is on my reading list—it’s a must-read for anyone in this space.\n\n\n\n\n\n\nI consider statistics to be one of my stronger skills compared to other data scientists in the industry (at least in Turkey), but there’s always room for growth.\nCurrently, I’m halfway through Think Bayes, and I’m looking forward to Tom Faulkenberry’s upcoming book when it’s released.\nThere are specific topics, like meta-analysis, that I want to strengthen, but I don’t plan to tackle them systematically. Instead, I’ll explore them as curiosity strikes.\n\n\n\n\n\n\nThis one’s a “maybe.” Deep learning and LLMs are higher priorities, and the list is already packed. However, gaining production-level data science expertise could be beneficial in the long run.\nFor now, it’s something I’ll keep on the back burner, but we’ll see how the year unfolds.\n\n\n\n\n\n\n\nThis blog post was polished with help of ChatGPT! I write the whole piece and feed it to ChatGPT to improve the flow.\nAnd that’s pretty much it for my 2025 learning goals. Stay tuned for updates!"
  },
  {
    "objectID": "posts/Intuitive Approach to Mean and Variance/index.html",
    "href": "posts/Intuitive Approach to Mean and Variance/index.html",
    "title": "Intuitive Conceptual Framing For Summary Statistics",
    "section": "",
    "text": "In statistics and data science, there is a pattern of trying to boil down things into a single number. Mean and variance fit well to that pattern. Let’s take a look at them, but with not-so-common conceptual framing that I learned from Kruschke’s Doing Bayesian Data Analysis.\n\nCenter and Variability: Basics\nIdea with the mean is to represent the center of a probability distribution (series of values and their corresponding probabilities). It’s defined as: \\(\\displaystyle E[x] = \\sum{p(x)x}\\). Don’t get confused by \\(E[x]\\), which is called expected value (i.e., mean). Well, this equation is only applicable when the values (xs) are discrete, hence \\(p(x)\\) corresponds to probability mass. When xs are continuous, you have the following:\n\\(\\displaystyle E[x] = \\int p(x)x \\, dx\\) where \\(p(x)\\) corresponds to probability densities.\nOn the other hand, variability of a distribution is also something that we are interested in. There comes the variance, telling us (roughly) how much a typical value stands away from the distribution’s mean. I said roughly, because what I have said is actually the deviance. The variance is the mean squared deviance: \\(\\displaystyle \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x - E[x])^2\\),\nand for continuous values of x: \\(\\displaystyle \\sigma^2 = \\int p(x) (x - E[x])^2 \\, dx\\)\n\n\nConceptual Framing: From variance to mean\nEvery book in statistics (at least the ones that I know of) starts with mean and gets to the variance. This conceptual framing is reversed, in the sense that it makes one end with the center of a distribution.\n\n\n\nStacked dot plot of interest rates of some previous data, with the mean as a red triangle\n\n\nThink of a distribution: If you thought of a discrete one, let’s represent it in your head with a stacked dot plot. If it’s continuous, then think of a density plot. In either way, we would like to represent center of the distribution with a value that is around the most stacked (for the former) or most dense values (for the latter). So, we are looking for a value M that minimizes the expected distance between it and rest of the values of the distribution, in the long run (as a rule of thumb, always think in long runs when it’s a frequentist method). Most common way of defining the distance is squared difference which gets larger if two quantity is far away from each other: \\((x - M)^2\\). So, we need a value M that minimizes the following: \\(\\displaystyle \\int p(x)(x-M)^2 \\, dx\\)\nGuess which value minimizes the equation above? You probably guessed it right, since it’s also available from the preceding section: \\(E[x]\\). Here’s where things get very interesting for me. If you decide to go with \\(|x - M|\\) as the measure for distance, then it is the median that minimizes the expected distance in the long run. If you plug zeros for exact matches and ones for mismatches as a distance, you get the mode.\n\nThe beauty in mathematics or statistics is connection, at least to me. Even while writing this, I like how distances invoke vector norms, and minimization brings up calculus etc. This example connects mean, median, mode in my head, hence making it more intuitive. The book itself is great btw, if you’re interested in Bayesian methods I highly suggest it. Until next time, take care!"
  },
  {
    "objectID": "posts/Basketball Analytics and Daily Job/index.html",
    "href": "posts/Basketball Analytics and Daily Job/index.html",
    "title": "How Basketball Analytics May Have An Effect on Your Analytics Job",
    "section": "",
    "text": "Excerpt from the Q&A done by The F5 with Mike Beouy\n\n\n\nI don’t know if you have watched Moneyball (if you haven’t, you should), there are great monologues (mainly from Jonah Hill’s character) that point out the main purpose of sports analytics:\n\nPeople who run ball clubs, they think in terms of buying players. Your goal shouldn’t be to buy players, your goal should be to buy wins.\n\n\n… This is building in all the intelligence that we have to project players. It’s about getting things down to one number.\n\nYes, the holy grail would be a single number that summarizes a player’s contribution to the team. This problem is similar to questions that you may end up trying to find an answer for, during your day to day job as a data scientist, because at the end of the day that’s what everyone’s trying to do: Representing a customer (or an employee etc.) by a number.\n\nProblem of Dividing Credit\nIn its core, problem of assigning a number that represents a player’s contribution to the team (when he’s on the court) is about dividing credit among players: We scored 30 points with these 5 players, allowed 20. How should I distribute the net rating of 10 among these 5 players?\nProblem of dividing credit comes up when you’re a data scientist in a company where you try to figure out how much each product contributes to the revenue of a customer: You just replace the players with the products and tie it to revenue instead of points, and it’s essentially the same problem.\n\n\nFuture Projection\nNot every player is the same when they are drafted to the NBA: Some players have a very high ceiling, and it might be wise to keep them. This type of career projection is relevant in a company setting where it might be beneficial to project customers: Not every customer is the same, since some of them hold more potential in terms of revenue generation. It is useful to identify them for retention purposes and campaign targeting.\n\n\nSegmentation\nClustering players based on their effectivess and style of play serves various purposes that ranges from identifying which players are more fit to certain play types to leading the transfer negotiations (e.g., finding similar players to replace the player that left).\n\nIn summary, trying to solve sport analytics problems may aid the problem solving processes in your daily job (for example, in a banking setting), and you may end up making a difference by adapting approaches that you have learned in sports analytics to your company setting.\nJust in case you don’t want to take my word, take the words from Mike Beuoy (the mind behind the Inpredictable). If you’re interested in basketball analytics, you should definitely subscribe to the F5, where the excerpt above is taken from."
  },
  {
    "objectID": "posts/what is an interaction effect/index.html",
    "href": "posts/what is an interaction effect/index.html",
    "title": "What’s an Interaction Effect? An example",
    "section": "",
    "text": "Some time ago, I had the chance to get my hands on some NBA data from 2013–14 and 2014–15 season. Back in the day SportsVu was the provider for the NBA data (it changed in 2017) and variables such as number of dribbles, touch time, closest defender, closest defender distance were available (these are not publicly accessible anymore). I wanted to leverage that data and attempt to create a new metric that captures players’ ability to shoot the ball better: Idea was to penalize misses that were likely to go in and reward makes that are less likely to go in while weighing 3 pointers as usual. I actually created something along these lines and called it shot proficiency which also takes shot volume into account. I may post about it later.\nI decided to go with two separate models (we are looking at the one made for 2 pointers). I had an idea of how it would look like in certain metrics since I have taken a look at the literature before and was satisfied with the results. I wanted to know if what the model thinks is in line with my intuition of what makes shots more likely to go in so I started with SHAP summary plot.\n\n\n\nSHAP Summary Plot\n\n\nIf you haven’t seen this plot before, basically it tells how each variables’ values (indicated by the color, check the colorbar) effect the outcome (corresponding horizontal axis) for each observation (observations correspond to data points). For example, looking at the shot distance, the model lowered the prediction as the shot distance gets larger. Overall, it seems intuitive: Shots that are closer to the basket have higher prediction, as the defender distance decreases so does the prediction for that shot to go in etc. However, there are wide range of predictions for similar values of shot distance (specifically for shots that are close to the basket) and closest defender distance. In other words, the effect is not the same across the data. When an effect of a value on a prediction is not constant, next step is to investigate interactions: Does the effect get moderated by other variables? In other words, does the effect of one variable depends on the value of another variable?\nSHAP offers methods to visualize both the main and interaction effects, an example can be found here. For this example, I had domain knowledge (I played organizational basketball for 10+ years) so I progressed with dependence plot.\n\n\n\nPlot to observe interaction between height difference and closest defender distance\n\n\nHorizontal axis of the dependence plot represents the real values of diff_in_height while vertical axis corresponds to how it effects the prediction. So, without the color it gives you the main effect of the variable which is pretty intuitive: Higher height differences in favor of the defender lowers the probability of shots to go in.\nHowever, there is something counterintuitive: Towards the very right side, you can see the predictions being lower for the shots when the defender is far away (and higher when the defender is close). That’s pretty counter-intuitive, I wouldn’t want a defender bothering me while shooting despite having height advantage. At this moment, domain knowledge kicks in.\n\n\nI suspected that to be moderated by a third variable, shot distance. Difference in height creates more advantage when the shooter is close to the basket and less so as one gets closer to the 3-pt line. This means the effect of height difference increases as the shot distance decreases.\nAnother thing: There are a lot of shots where the difference in height is in favor of the defender (so, looking at the other end of the plot) and the closest defender distance is high. This might be due to defenders swapping who they defend after a play called pick & roll. I do not have play-by-play data that labels those so we are not going to be able to check them directly but indirectly: Switch on a pick & roll usually results in quick ball handler against a slow big guy relatively away from the basket, which forces big guy to take back a step or two to be able to stand in front of the ball handler. This gap creates relatively comfortable space for the ball handler to shoot, hence I suspect those shots to come from distance.\n\n\n\n\nInvestigating many interactions via scatter plot with color and size parameters\n\n\nThe plot aligns with both hypotheses:\n\nThe model predicts a higher shot probability when height difference favors the shooter and the closest defender is nearby — these are shots close to the basket, confirming the first assumption.\nFor shots where the defender is far away despite having a height advantage, the model predicts lower probabilities. While I can’t directly verify if these shots follow a pick & roll, the assumption led to the hypothesis that these shots come from longer distances — which is confirmed in the plot.\n\n\nI like explainable AI methods to understand how the model makes predictions along with some validation: Does the model act in a way that I expect it to? Interactions are huge part of this process, many effects in the real world are not uniform across groups, so it’s something I am always interested in. If you’re like me, a good starting point might be this chapter from Lakens’ book.\nAnyway, I hope you enjoyed the read. Have a nice Sunday!"
  },
  {
    "objectID": "posts/Bayesian Way - Predictions out of Posterior Distribution/index.html",
    "href": "posts/Bayesian Way - Predictions out of Posterior Distribution/index.html",
    "title": "Bayesian Way - Predictions Out of Posterior Distribution",
    "section": "",
    "text": "In the previous post, the model was fit using a grid approximation for likelihood calculation under each hypothesis (i.e., for each pair of parameters). Now, let’s take a step further and calculate posterior distribution and take the marginal distribution for each parameter (i.e, the intercept and the slope). If things seem complicated, I advice you to take a look at the previous post.\nAlthough I posted about Bayesian approach before, it might be nice to summarize it a bit:\nIn my opinion, best summation of Bayesian way of thinking is made in a single sentence by John Kruschke in his book Doing Bayesian Data Analysis:\n\"Bayesian inference is reallocation of credibility across possibilities.\"\nThe word “reallocation” implies prior allocation of credibility across possibilities. Let me exemplify the whole thing:\nI work in a hybrid office where we’re required to be present three days a week. We get to choose which days, but most of the team tends to come in on Wednesdays.\nLast Wednesday, I headed to the office assuming that my team leader, Isil, would be there as well. At first, my belief strongly leaned toward her showing up due to my prior knowledge. When I arrived on our floor five minutes early and saw her laptop, my belief was strengthened.\nHowever, as time passed and every other team member arrived—except Isil—my confidence started to fade. By 10 o’clock, I was almost certain she wasn’t coming in. Later, I found out she had to take the day off.\nThis is a nice real-life demonstration of Bayesian view: At the very start, I had a subjective degree of belief about Isil showing up. As I collected more data (e.g., spotting her laptop, others arriving etc.), I updated that belief and it gradually shifted towards the opposite direction.\nOn the example above, I had an informative prior: It wasn’t 50-50 (equally likely) in terms of her showing up or not. On the other hand, this example started with an uninformative prior where each hypothesis is equally likely.\nNow, for this one, I’ll do a very similar thing: I’ll have a prior distribution for each parameter that represents my prior belief about its possible values (i.e., “possibilities above”). Then, I’ll repeat the steps that I did in the previous post and update my belief accordingly.\n\nimport numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport statsmodels as sm\nimport empiricaldist as emd\nimport scipy.stats as st\n\nimport utils as ut\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n\npossum = pd.read_csv('possum.csv', usecols=['pop', 'tail_l'])\ndf = possum.copy()\n\ndf.head(3)\n\n\n\n\n\n\n\n\npop\ntail_l\n\n\n\n\n0\nVic\n36.0\n\n\n1\nVic\n36.5\n\n\n2\nVic\n39.0\n\n\n\n\n\n\n\nI have the same data as before where each row corresponds to features of a possum from Australia and New Guinea. Same as before, let’s attempt to predict which region a possum is from via its tail length.\n\nqs_cept = np.linspace(-1.2, 0.8, 281) # possible values for intercept (beta_0)\nqs_slope = np.linspace(-1.4, 0, 221) # possible values for slope (beta_1)\n\nI’ll create a uniform distribution for both set of possible values (i.e., sample spaces), where each value is equally likely.\n\nuniform_cept = pd.Series(data=[1/len(qs_cept) for qs in qs_cept], index=qs_cept)\nuniform_cept.head(3)\n\n-1.200000    0.003559\n-1.192857    0.003559\n-1.185714    0.003559\ndtype: float64\n\n\n\nuniform_slope = pd.Series(data=[1/len(qs_slope) for qs in qs_slope], index=qs_slope)\nuniform_slope.head(3)\n\n-1.400000    0.004525\n-1.393636    0.004525\n-1.387273    0.004525\ndtype: float64\n\n\nTaking their outer product gives us the joint distribution.\nHeads up: If your priors are normalized (i.e., sum up to 1), so does your joint as well.\n\n## Joint dist\n\nA, B = np.meshgrid(uniform_slope, uniform_cept)\n\ndf_prior = pd.DataFrame(A*B, columns=uniform_slope.index, index=uniform_cept.index)\ndf_prior.iloc[:4, :4]\n\n\n\n\n\n\n\n\n-1.400000\n-1.393636\n-1.387273\n-1.380909\n\n\n\n\n-1.200000\n0.000016\n0.000016\n0.000016\n0.000016\n\n\n-1.192857\n0.000016\n0.000016\n0.000016\n0.000016\n\n\n-1.185714\n0.000016\n0.000016\n0.000016\n0.000016\n\n\n-1.178571\n0.000016\n0.000016\n0.000016\n0.000016\n\n\n\n\n\n\n\n\ndf_prior = df_prior.stack()\ndf_prior.head(3)\n\n-1.2  -1.400000    0.000016\n      -1.393636    0.000016\n      -1.387273    0.000016\ndtype: float64\n\n\nNow the dataframe for prior distribution is ready. Each row, column pair represents possible pair of values for both parameters.\nAlthough it’s not necessary, I’m going to create a different dataframe that represents likelihoods.\n\ndf_likeli = pd.DataFrame(index=qs_cept, columns=qs_slope)\ndf_likeli.fillna(1, inplace=True)\ndf_likeli = df_likeli.stack()\n\ndf_likeli.head(3)\n\n-1.2  -1.400000    1\n      -1.393636    1\n      -1.387273    1\ndtype: int64\n\n\nNow, I ask this question: For each pair of intercept and slope, b0 and b1, how likely I am to see the observed data?\n\ndf['pop'] = df['pop'].apply(lambda x: 1 if x == 'Vic' else 0) # dummy coding\n\n\n# Centering data\n\noffset = df['tail_l'].mean().round()\ndf['x'] = df['tail_l'] - offset\ndf['y'] = df['pop']\n\n# I refer our predictor as x from now on (for convenience), y becomes our target variable which takes 1 if possum is from Victoria region and 0 otherwise.\n\n\nagg_data = df.groupby('x')['y'].agg(['sum', 'count'])\nagg_data.head(10).T\n\n\n\n\n\n\n\nx\n-5.0\n-3.5\n-3.0\n-2.5\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n0.5\n\n\n\n\nsum\n2\n1\n4\n2\n6\n6\n9\n4\n4\n1\n\n\ncount\n2\n1\n5\n2\n9\n7\n13\n12\n6\n4\n\n\n\n\n\n\n\n\nns = agg_data['count'] # represents number of observation with corresponding x values\nks = agg_data['sum'] # represents successes, which means Victoria region\n\n\nfrom scipy.special import expit # inverse of logit\n\nx_values = agg_data.index # these are centered\n\nfor cept, slope in df_likeli.index:\n    probs = expit(cept + slope * x_values) # transformation to probabilities\n    likelis = st.binom.pmf(ks, ns, probs) # likelihood of each observation\n    df_likeli[cept, slope] = likelis.prod() # likelihood of the whole data under the selected pair of parameter values\n\n\ndf_likeli.head(6)\n\n-1.2  -1.400000    4.812632e-15\n      -1.393636    5.218338e-15\n      -1.387273    5.654017e-15\n      -1.380909    6.121460e-15\n      -1.374545    6.622520e-15\n      -1.368182    7.159117e-15\ndtype: float64\n\n\n\ndf_posterior = df_prior * df_likeli\ndf_posterior.head(6) # unnormalized\n\n-1.2  -1.400000    7.749685e-20\n      -1.393636    8.402985e-20\n      -1.387273    9.104551e-20\n      -1.380909    9.857265e-20\n      -1.374545    1.066411e-19\n      -1.368182    1.152818e-19\ndtype: float64\n\n\n\ndf_posterior = df_posterior / df_posterior.sum()\ndf_posterior.head() # normalized\n\n-1.2  -1.400000    8.913907e-09\n      -1.393636    9.665351e-09\n      -1.387273    1.047231e-08\n      -1.380909    1.133810e-08\n      -1.374545    1.226616e-08\ndtype: float64\n\n\n\nut.plot_contour(df_likeli.unstack())\nut.decorate(title = 'Likelihood')\n\n\n\n\n\n\n\n\n\nut.plot_contour(df_posterior.unstack())\nut.decorate(title = 'Posterior')\n\n\n\n\n\n\n\n\nIn Bayesian framework, the posterior distribution is a negotiation between the prior distribution and the likelihood: If the prior does not have much leverage (i.e., if it’s uninformative) then likelihood gets what it wants. In this case, since the prior was uniform, the likelihood dominated. Hence, both the likelihood and the posterior distribution are exactly the same.\n\nMARGINAL DISTRIBUTIONS\nNow, I sum the values for slope to get the marginal distribution. For example, to get the probability of slope being -1.2, I need to fix the slope to -1.2 and let the intercept vary and calculate the probabilities for every pair, and sum them up to get the total probability. Well, we do have the probabilities so only thing left to do is to sum them up.\n\nmarginal_slope = df_posterior.unstack().sum(axis=0)\nmarginal_slope.plot(label='Beta_1', color='C4')\n\nprint(round(marginal_slope.idxmax(), 3))\n\nut.decorate(\n    xlabel='Possibilities',\n    ylabel='PDF',\n    title = 'Posterior Distribution for Slope'\n)\n\n-0.706\n\n\n\n\n\n\n\n\n\nThe mean of the distribution is around -0.7, which means that 1 unit increase in tail length decreases the log(odds) in favor of a possum being from Victorian, by 0.7.\n\nmarginal_cept = df_posterior.unstack().sum(axis=1)\nmarginal_cept.plot(label='Beta_0', color='C3')\n\nprint(round(marginal_cept.idxmax(), 3))\n\nut.decorate(\n    xlabel='Possibilities',\n    ylabel='PDF',\n    title = 'Posterior Distribution for the Intercept'\n)\n\n-0.321\n\n\n\n\n\n\n\n\n\nOn the previous post, I had point estimates: Single pair of values for both parameters that maximizes the likelihood of the data. Now, instead of single pair, the result is distribution of possible values for each parameter.\nOne more trick before moving on: It is possible transform distributions to show probabilities. Let’s do it for the intercept, \\(\\beta_0\\).\n\nmarginal_cept.index = np.exp(marginal_cept.index) # getting rid of the log\nmarginal_cept.index = marginal_cept.index / (marginal_cept.index + 1) # turning odds into probabilities\n\nprint(round(marginal_cept.idxmax(), 3))\n\n0.42\n\n\nSo, the mean of the distribution is around 42%, standing for probability of Victorian possum when the tail length is 37 cm.\n\n\nPREDICTIONS\nSo, how to make predictions out of this?\nFirst, let’s put the posterior distribution to a pmf object.\n\nposterior_pmf = emd.Pmf(df_posterior)\nposterior_pmf.head()\n\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n-1.2\n-1.400000\n8.913907e-09\n\n\n-1.393636\n9.665351e-09\n\n\n-1.387273\n1.047231e-08\n\n\n\n\n\n\n\nPmf object offers “choice” method that allows to sample from the distribution. I’ll sample pairs from the joint distribution.\n\nsample = posterior_pmf.choice(201)\n\n\nmin_tail_length = df['tail_l'].min() - 1\nmax_tail_length = df['tail_l'].max() + 1\n\nCreating values to make predictions for.\n\nxs = np.array([num-offset for num in np.arange(min_tail_length, max_tail_length, 0.5)])\n\n\npred = np.empty((len(sample), len(xs)))\n\nfor i, (cept, slope) in enumerate(sample):\n    odds = np.exp(cept + slope * xs)\n    pred[i] = odds / (odds + 1)\n\nOkay, ended up with a list of predictions for each x under sampled model parameters. Let’s put them into a graph to observe most likely values at different tail lengths.\n\nfor ps in pred:\n    plt.plot(xs+offset, ps, color='C4', lw=0.5, alpha=0.3)\n\nplt.scatter(df['x']+offset, df['y'], s=30)\nut.decorate()\n\n\n\n\n\n\n\n\n\nlow, median, high = np.percentile(pred, [5, 50, 95], axis=0)\n\n\nplt.fill_between(xs+offset, low, high, color='C4', alpha=0.2, label='90% Credible Interval')\nplt.plot(xs+offset, median, color='C4', label='model')\nplt.scatter(df['x']+offset, df['y'], s=30)\n\nplt.yticks([val/10 for val in np.arange(0, 11)])\n\nut.decorate()\n\n\n\n\n\n\n\n\nDon’t worry about credible intervals / highest density intervals (HDIs), I think about talking about them in a different post.\nLet’s make a prediction: For example, what’s the probability of Victorian if the tail length is 33 cm?\n\nlow = pd.Series(low, xs+offset)\nmedian = pd.Series(median, xs+offset)\nhigh = pd.Series(high, xs+offset)\n\n\ntail_l = 33\nprint(\n    round(median[tail_l], 3),\n    (round(low[tail_l], 3), round(high[tail_l], 3))\n)\n\n0.929 (0.835, 0.976)\n\n\nThe probability of Victorian given the tail length of 33 cm is at least 83.5%.\nThat’s pretty much it. I posted this blog as a follow-up to the previous one, hoping to make the relation between likelihood and posterior apparent. If you enjoyed it, you may consider subscribing to the email list.\nHave a nice weekend!"
  },
  {
    "objectID": "posts/Interpretablog - Partial Regression Plots/index.html",
    "href": "posts/Interpretablog - Partial Regression Plots/index.html",
    "title": "Partial Regression/Added Variable/Predictor Residual Plots",
    "section": "",
    "text": "I wanted to introduce partial regression plots (or added variable plots, or predictor residual plots etc.), before moving any further down this series.\nIn the first post of this series, I showed a relationship by plotting \\(X_j\\) vs \\(y\\). In a multivariate setting, that doesn’t tell us much since if there seems to be a relationship, it might be due to shared effects with other variables that are not \\(X_j\\). What will we do is to plot what exactly the regression coefficient tells us: Marginal effect of \\(X_j\\), adjusted for other variables.\nThe idea is quite nice:\n\nWe have a variable \\(X_j\\). We fit a model to predict it with other variables.\nResiduals (\\(r_{Xj}\\)) from that model represent the portion of \\(X_j\\) that cannot be explained by the other variables.\nPlot it against \\(y\\) (or residuals of \\(y\\)).\n\nIf you’re interested in the contribution of \\(X_j\\) to prediction, after the contribution of all the other variables, you should plot against residuals of \\(y\\).\nLet’s exemplify this: We are here with our good old possum data, which I used in different examples as well.\n\ndf = pd.read_csv('possum.csv', usecols=['head_l', 'tail_l', 'total_l'])\ndf.head()\n\n\n\n\n\n\n\n\nhead_l\ntotal_l\ntail_l\n\n\n\n\n0\n94.1\n89.0\n36.0\n\n\n1\n92.5\n91.5\n36.5\n\n\n2\n94.0\n95.5\n39.0\n\n\n3\n93.2\n92.0\n38.0\n\n\n4\n91.5\n85.5\n36.0\n\n\n\n\n\n\n\n\nX_full = sm.add_constant(df[['head_l', 'tail_l']])\nmodel_full = sm.OLS(df['total_l'], X_full).fit()\n\nprint('Coefficient for `head_l`: ', np.round(model_full.params['head_l'], 3))\n\nCoefficient for `head_l`:  0.695\n\n\nHere we have the full model with intercept, and note the coefficient of the head_l.\nNow, I’m going to show both versions with raw values of \\(y\\) and residuals of \\(y\\) (\\(r_y\\)).\n\n# Regressing head_l ~ tail_l\n\nmodel_head_resid = LinearRegression().fit(df[['tail_l']], df['head_l'])\nhead_resids = df['head_l'] - model_head_resid.predict(df[['tail_l']]) # residualized head_l\n\n\nX_v1 = sm.add_constant(head_resids) # intercept\nmodel_v1 = sm.OLS(df['total_l'], X_v1).fit()\npred_v1 = model_v1.predict(X_v1)\n\n\nplt.figure(figsize=(7, 5))\n\nplt.scatter(head_resids, df['total_l'], label='Data')\nplt.plot(head_resids, pred_v1, color='red', label='Fitted Line')\n\nplt.title('V1: total_l ~ $r_{head_l}$')\n\nplt.xlabel('residualized head_l')\nplt.ylabel('total_l')\n\nplt.legend()\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nmodel_y_resid = LinearRegression().fit(df[['tail_l']], df['total_l'])\ny_resids = df['total_l'] - model_y_resid.predict(df[['tail_l']])\n\n\nX_v2 = sm.add_constant(head_resids) # don't need it here actually, but wanted to keep things consistent.\n\nmodel_v2 = sm.OLS(y_resids, X_v2).fit()\npred_v2 = model_v2.predict(X_v2)\n\n\nplt.figure(figsize=(7,5))\n\nplt.scatter(head_resids, y_resids, label='Data')\nplt.plot(head_resids, pred_v2, color='orange', label='Fitted Line')\n\nplt.title('V2: residualized y ~ residualized head_l')\n\nplt.xlabel('residualized head_l')\nplt.ylabel('residualized total_l')\n\nplt.legend()\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nprint('Coefficient for `head_l`: ', np.round(model_full.params['head_l'], 3))\nprint('Version 1 (y ~ residualized X):', np.round(model_v1.params[1], 3))\nprint('Version 2 (resid y ~ resid X):', np.round(model_v2.params[1], 3))\n\nCoefficient for `head_l`:  0.695\nVersion 1 (y ~ residualized X): 0.695\nVersion 2 (resid y ~ resid X): 0.695\n\n\nWhich one is better?\nWell, the first version is easier to interpret since y is in its raw. However, spread of it still contains the influence of other predictors as well. Hence, although the slope reflects the contribution after knowing the others, the vertical scatter is not corresponding to the full model.\nIn the second one, both axes have been purged of the influence of other variables. So, visually, we’re looking through the lens of “holding-others-constant” (literally).\nThis approach is beneficial for diagnostics as well, it seems to me, since:\n\nIt’s easier to see if a particular point is pulling the slope strongly.\nIf there’s high multicollinearity, \\(r_{X_j}\\) would show a very low variance since most of it gets explained by other variables.\nThe pattern is apparent, and if it shows different kind you may think about transformations.\nIt’s much easier to communicate compared to giving out the coefficient only.\n\nTo sum up, in a multivariate setting partial regression plots/predictor residual plots are useful for both interpretability, communication, and diagnostics."
  },
  {
    "objectID": "posts/Cat Adoption/index.html",
    "href": "posts/Cat Adoption/index.html",
    "title": "Do Black Cats Experience Better Adoption Since The Movie Flow",
    "section": "",
    "text": "I came across a post on Instagram last week. It had the following message (or something similar):\n\nOscar-winning film Flow raises adoptions for black cats.\n\nIt really caught my attention (yea, I love cats). At first glance, I immediately thought of taking an RDiT (Regression Discontinuity in Time) approach where the running variable is time and threshold is whether or not the observation belongs to the post-movie period. However, adoption is a censored variable, for some cats we observe the true event and for some others, we don’t (e.g., maybe a cat runs away or gets transferred to some other shelter). Hence, it hints at time-to-event modeling. So, I changed the question and decided to attempt to answer the following: &gt; Do black cats experience better adoption process than non-black cats, after the movie Flow?\nOne implication of “better adoption process” would be to wait less. So, the question maps into do black cats wait less to be adopted compared to non-black cats, after the movie Flow? I asked the ChatGPT about the data which directed me to data from Austin TX\nI planned on taking a difference-in-differences approach: Within both black and non-black cats, for pre and post movie period, estimate the days until adoption. Then take the difference within, which leads to difference in days between pre and post period for each black and non-black cats. And then, take the difference in differences. I believe the expression below makes it easier to understand: \\(\\displaystyle (\\text{black}_{post} - \\text{black}_{pre}) - (\\text{non-black}_{post} - \\text{non-black}_{pre})\\)\nFor time-to-event modeling, a natural choice is exponential distribution with \\(\\lambda\\), where \\(\\lambda\\) reflects the rate of adoption in our case. In general, one can think of it as average number of events per unit of time. However, since there are censored observations, we end up with two distributions that are assigned to observations. It would be easier if I just give the model specification.\n\\(\\displaystyle \\text{Exponential}(\\lambda_i)\\) if true event observed.\n\\(\\displaystyle \\text{Exponential-CCDF}(\\lambda_i)\\) if true event unobserved.\n\\(\\displaystyle \\lambda_i = 1 / \\mu_i\\)\n\\[\n\\log(\\mu_i)\n  = \\alpha_0\n  + \\alpha_{\\text{black}} \\cdot \\text{is\\_black}_i\n  + \\beta_{\\text{post}} \\cdot \\text{post\\_movie}_i\n  + \\beta_{\\text{inter}} \\cdot (\\text{post\\_movie}_i \\cdot \\text{is\\_black}_i)\n  + f_{\\text{time}}(\\text{month}_i)\n  + \\gamma \\cdot \\text{time_trend_z}\n\\]\nPrior distribution for each are normal, while month effects are hierarchical:\n\\(f_{\\text{time}}(\\text{month}_i) \\sim \\text{Normal}(0, \\sigma_{\\text{month}})\\), \\(\\sigma_{\\text{month}} \\sim \\text{Exponential}(1)\\)\nI aimed to capture seasonality and trend via time and month terms. On the other hand the interaction term, b_inter, gives the extra shift for the black cats, after the movie. However, instead of attempting to interpret the coefficients, we’ll go with posterior contrasts.\nWhile calculating the posterior contrasts, I averaged over the months that both present before and after the movie (since post-movie period doesn’t include summer time). Here’s how the before-after contrasts look both for black (black line) and non-black (red line) cats.\n\n\n\nPosterior contrasts within each color of cats\n\n\nSo, it seems like shift for the black cats after the movie is larger compared to non-black cats. 89% HDI for blacks are -21.25 to -18.23 while it’s -9.02 to -7.71 for non-blacks. But, let’s get the difference in differences.\n\n\n\nPosterior contrast between cats (difference in differences)\n\n\nThe shift for the black cats after the movie seems to be around 10.5 days. In other words, black cats wait 10.5 days less on average, after the movie, compared to non-black cats. That’s sweet :)\nAlright, to be honest, there’s a caveat here. By modeling it as exponential, I assume that the \\(\\lambda\\) is constant within each strata. There’s no dependence on how long the cat has been waiting. Within a particular stratum, waiting 1 day or 71 days does not make any difference. Well, that’s problematic because that’s unlikely to be true. So, this model can be thought as a baseline and improvements can be make. For example, I thought about swapping to piecewise exponential, where the lambda (the rate) is constant within the day intervals (e.g., 0-7 days, 7-14 days etc.). This might be a next step.\nAnyway, I hope you enjoyed this read. See you next time."
  },
  {
    "objectID": "posts/Interpretablog - Fire and Forget/index.html",
    "href": "posts/Interpretablog - Fire and Forget/index.html",
    "title": "Interpretablog - Fire & Forget",
    "section": "",
    "text": "Welcome to a new series on interpretability where I’ll go through methods to interpret models, starting from the linear regression to so called black-box ensembles. I wanted to start this series for a while, due to two reasons:\n\nI feel like anything I write is helpful for me, like I’m explaining to my future self. So, anytime I can’t recall something my own blog becomes my go-to source.\nThere seems to be a lot of confusions within the ML field in terms of interpretability (at least among the new comers), where most of the time wording suggest causality when it shouldn’t. I think I learned a few approaches to be helpful for that as well.\n\nIf you want a book to find everything in one place your go-to source should be Interpretable ML by Christoph Molnar. Most of the approaches that I learned is from there. I should also give credit to Regression and Other Stories.\nBy the way, throughout the series I won’t dive much into how to fit models. So, I assume some familiarity with GLMs, decision tree, and bagging and boosting methods in general. For example, I won’t walk through the summary table below, assuming you already know how to read it.\nLet’s start with a simple linear regression where we only have predictor show a simple trick that aids the interpretability.\n\nmodel = smf.ols('kid_score ~ mom_iq', data=cognitive)\nresults = model.fit()\n\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              kid_score   R-squared:                       0.201\nModel:                            OLS   Adj. R-squared:                  0.199\nMethod:                 Least Squares   F-statistic:                     108.6\nDate:                Wed, 16 Jul 2025   Prob (F-statistic):           7.66e-23\nTime:                        15:08:03   Log-Likelihood:                -1875.6\nNo. Observations:                 434   AIC:                             3755.\nDf Residuals:                     432   BIC:                             3763.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     25.7998      5.917      4.360      0.000      14.169      37.430\nmom_iq         0.6100      0.059     10.423      0.000       0.495       0.725\n==============================================================================\nOmnibus:                        7.545   Durbin-Watson:                   1.645\nProb(Omnibus):                  0.023   Jarque-Bera (JB):                7.735\nSkew:                          -0.324   Prob(JB):                       0.0209\nKurtosis:                       2.919   Cond. No.                         682.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWhat the table above tells us is that: \\(\\displaystyle \\text{E[kid's score | mom's iq]} = 25.7998 + 0.61 \\cdot \\text{mom's iq}\\)\nWhich implies the intercept, 25.7998, is the expected value of kid’s score when the mom’s iq is 0. That doesn’t make sense, right? Nobody has an IQ score of 0.\nSo, a way to make it make sense is via centering the predictor so that the intercept corresponds to the mean. Let me show.\n\noffset = np.mean(cognitive['mom_iq'])\ncognitive['mom_iq_c'] = cognitive['mom_iq'] - offset\n\nmodel = smf.ols('kid_score ~ mom_iq_c', data=cognitive)\nresults = model.fit()\n\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              kid_score   R-squared:                       0.201\nModel:                            OLS   Adj. R-squared:                  0.199\nMethod:                 Least Squares   F-statistic:                     108.6\nDate:                Wed, 16 Jul 2025   Prob (F-statistic):           7.66e-23\nTime:                        15:08:03   Log-Likelihood:                -1875.6\nNo. Observations:                 434   AIC:                             3755.\nDf Residuals:                     432   BIC:                             3763.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     86.7972      0.877     98.993      0.000      85.074      88.521\nmom_iq_c       0.6100      0.059     10.423      0.000       0.495       0.725\n==============================================================================\nOmnibus:                        7.545   Durbin-Watson:                   1.645\nProb(Omnibus):                  0.023   Jarque-Bera (JB):                7.735\nSkew:                          -0.324   Prob(JB):                       0.0209\nKurtosis:                       2.919   Cond. No.                         15.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAs you can see, intercept has changed since now it corresponds to the expected value of kid’s score when the mom_iq is the average.\n\nprint(np.round(cognitive['kid_score'].mean(), 4))\n\n86.7972\n\n\nWhat about the slope? Isn’t it interpreted as one unit increase in mom’s IQ is associated with 0.61 increase of kid’s score?\nI suggest you to stay away from such interpretation. The model does not provide information about changes within units but about comparisons between units. So, the most secure way of interpretation is as follows: Under the model, a kid with a mom’s score of x+1 will be, on average, 0.61 IQ higher compared to a kid with a mom’s score of x.\nLet’s extend this to a more general setting where there’s more than 1 variable.\n\nmodel = smf.ols('kid_score ~ mom_iq + mom_hs', data=cognitive)\nresults = model.fit()\n\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              kid_score   R-squared:                       0.214\nModel:                            OLS   Adj. R-squared:                  0.210\nMethod:                 Least Squares   F-statistic:                     58.72\nDate:                Wed, 16 Jul 2025   Prob (F-statistic):           2.79e-23\nTime:                        15:08:03   Log-Likelihood:                -1872.0\nNo. Observations:                 434   AIC:                             3750.\nDf Residuals:                     431   BIC:                             3762.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept        25.7315      5.875      4.380      0.000      14.184      37.279\nmom_hs[T.yes]     5.9501      2.212      2.690      0.007       1.603      10.297\nmom_iq            0.5639      0.061      9.309      0.000       0.445       0.683\n==============================================================================\nOmnibus:                        7.327   Durbin-Watson:                   1.625\nProb(Omnibus):                  0.026   Jarque-Bera (JB):                7.530\nSkew:                          -0.313   Prob(JB):                       0.0232\nKurtosis:                       2.845   Cond. No.                         683.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nIn this setting, the coefficient corresponds to the expected difference between two observations, or group of observations, that only differ on the interested predictor by 1. Very simple way of showing it is like below:\n\\(\\displaystyle \\beta_0 + \\beta_1X_1 + \\beta_2X_2\\)\n\\(\\displaystyle \\beta_0 + \\beta_1(X_1+1) + \\beta_2X_2 = \\beta_0 + \\beta_1X_1 + \\beta_1 + \\beta_2X_2\\)\nif I take the difference: \\(\\displaystyle (\\beta_0 + \\beta_1X_1 + \\beta_1 + \\beta_2X_2) - (\\beta_0 + \\beta_1X_1 + \\beta_2X_2) = \\beta_1\\)\nThis also points out the problem with collinearity. If \\(X_1\\) and \\(X_2\\) changes together, how am I gonna be left with \\(\\beta_1\\) alone since others won’t cancel each other. Hence, we won’t be able to attribute the change to a single variable.\nWith these interpretations from the table in mind, we can also make use of plots which are more intuitive.\n\nmean_kid_by_mom = cognitive.groupby('mom_iq')['kid_score'].mean()\n\n\nlow_end, high_end = cognitive['mom_iq'].min(), cognitive['mom_iq'].max()\nqs = np.linspace(low_end, high_end, 101)\n\n\nnewdata_yes = pd.DataFrame()\n\nnewdata_yes['mom_iq'] = qs\nnewdata_yes['mom_hs'] = 'yes'\nnewdata_yes['preds'] = results.predict(newdata_yes)\n\n\nnewdata_no = pd.DataFrame()\n\nnewdata_no['mom_iq'] = qs\nnewdata_no['mom_hs'] = 'no'\nnewdata_no['preds'] = results.predict(newdata_no)\n\n\nplt.figure(figsize=(6,4))\n\nplt.plot(mean_kid_by_mom, 'o', alpha=.43)\nplt.plot(newdata_yes['mom_iq'], newdata_yes['preds'], label='HS_Yes', color='C1', lw=2)\nplt.plot(newdata_no['mom_iq'], newdata_no['preds'], label='HS_No', color='C3', lw=2)\n\nplt.xlabel('mom\\'s iq')\nplt.ylabel('kid score')\nplt.title('Kid_score ~ mom_iq + mom_hs')\n\nplt.legend()\n\n\n\n\n\n\n\n\nWith plots, it’s very easy to understand the model and its assumptions. By looking at the plot, for example, it’s apparent that coefficient of mom’s iq does not depend on high school variable — the slope is the same for both levels of high school.\nWe can include an interaction term in the model specification, so that it can depend on the level of high school.\n\nmodel = smf.ols('kid_score ~ mom_iq + mom_hs + mom_iq:mom_hs', data=cognitive)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              kid_score   R-squared:                       0.230\nModel:                            OLS   Adj. R-squared:                  0.225\nMethod:                 Least Squares   F-statistic:                     42.84\nDate:                Wed, 16 Jul 2025   Prob (F-statistic):           3.07e-24\nTime:                        15:08:04   Log-Likelihood:                -1867.5\nNo. Observations:                 434   AIC:                             3743.\nDf Residuals:                     430   BIC:                             3759.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n========================================================================================\n                           coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nIntercept              -11.4820     13.758     -0.835      0.404     -38.523      15.559\nmom_hs[T.yes]           51.2682     15.338      3.343      0.001      21.122      81.414\nmom_iq                   0.9689      0.148      6.531      0.000       0.677       1.260\nmom_iq:mom_hs[T.yes]    -0.4843      0.162     -2.985      0.003      -0.803      -0.165\n==============================================================================\nOmnibus:                        8.014   Durbin-Watson:                   1.660\nProb(Omnibus):                  0.018   Jarque-Bera (JB):                8.258\nSkew:                          -0.333   Prob(JB):                       0.0161\nKurtosis:                       2.887   Cond. No.                     3.10e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.1e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nJust like that, we allowed the coefficient of mom_iq to change depending on the level of mom_hs. But how do we interpret them now?\nThe most intuitive way is via the plot:\n\nnewdata_hs = pd.DataFrame()\n\nnewdata_hs['mom_iq'] = np.linspace(np.min(cognitive['mom_iq']), np.max(cognitive['mom_iq']), 101)\nnewdata_hs['mom_hs'] = 'yes'\n\npred_hs = results.predict(newdata_hs)\n\n\nnewdata_nohs = pd.DataFrame()\n\nnewdata_nohs['mom_iq'] = np.linspace(np.min(cognitive['mom_iq']), np.max(cognitive['mom_iq']), 101)\nnewdata_nohs['mom_hs'] = 'no'\n\npred_nohs = results.predict(newdata_nohs)\n\n\nplt.figure(figsize=(6, 4))\n\nplt.plot(cognitive['mom_iq'], cognitive['kid_score'], 'o', alpha=.43)\nplt.plot(newdata_hs['mom_iq'], pred_hs, label='HS_yes', color='C1', ls='-', lw=2)\nplt.plot(newdata_nohs['mom_iq'], pred_nohs, label='HS_no', color='C3', ls='-', lw=2)\n\nplt.xlabel('Mom\\'s iq')\nplt.ylabel('Kid\\'s score')\nplt.legend()\n\n\n\n\n\n\n\n\nSeaborn makes quick plotting available.\n\nsns.lmplot(\n    data=cognitive, x='mom_iq', y='kid_score', hue='mom_hs', \n    ci=None, scatter_kws={'alpha':.4, 's':20},\n    height=4, aspect=1.23\n)\n\n\n\n\n\n\n\n\nPlots show that the lines are not parallel, so coefficients are not the same for both levels of high school variable.\nI want to emphasize this, interaction effect is ADDITIVE to the main effect. Since interpretation of “while holding all other variables constant” is meaningless - we can’t hold mom_iq constant while increasing the interaction variable.\nWhat it means to be additive? Well, think of it this way. The coefficient of interaction is negative. When the high school level is yes, we add -0.4843 to the slope of the mom_iq. Since the coefficient of mom_iq positive, it means that mom_iq matters less when the level of high school is “yes”.\nI hope this post clarified some aspects of interpretation of linear models. Of course, there are extensions of the classic linear regression, called Generalized Linear Models (GLMs). Actually, linear regression itself is a special case in GLM framework where the link function is identity function.\nAnyway, I believe I’ll follow this up with model agnostic methods since most of you go for ML models. However, make no mistake, you may actually need these parametric approaches since you don’t always end up with a lot of data.\nUntil then, take care!"
  },
  {
    "objectID": "posts/Non-Identifiability Issue/index.html",
    "href": "posts/Non-Identifiability Issue/index.html",
    "title": "(Non)Identifiability",
    "section": "",
    "text": "I remember first time learning regression and remember the bullet points of assumptions. I believe people still learn that way, knowing that multicollinearity is an issue. But, what exactly is the issue? If you are following my posts on the basketball analytics newsletter, adjusted plus-minus (APM) models are actually up against same issue.\nAnyway, let’s get into it with an example. Btw, I’m going to using a Bayesian regression on this one since I feel like it makes the example more intuitive. However, feel free to give it a go with the frequentist way, which you’ll end up noticing similar things.\n\ndf = pd.read_csv('possum.csv', usecols=['skull_w', 'head_l'])\ndf.head()\n\n\n\n\n\n\n\n\nhead_l\nskull_w\n\n\n\n\n0\n94.1\n60.4\n\n\n1\n92.5\n57.6\n\n\n2\n94.0\n60.0\n\n\n3\n93.2\n57.1\n\n\n4\n91.5\n56.3\n\n\n\n\n\n\n\nYou probably remember the possum data from other posts. Let’s predict head_l from skull_w. But I’ll do something extreme, I’ll add exact copy of skull_w and put both the original and the copy into the model.\nBut before, here’s what it looks like with a simple linear regression (the frequentist way).\n\nmodel = smf.ols('head_l ~ skull_w', data=df)\nres = model.fit()\n\nres.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n46.1954\n4.554\n10.145\n0.000\n37.163\n55.227\n\n\nskull_w\n0.8158\n0.080\n10.207\n0.000\n0.657\n0.974\n\n\n\n\n\nAround 0.816 — keep that in mind.\n\naz.plot_posterior(trace, var_names=['skull_w', 'skull_w_copy', 'beta_sum'], hdi_prob=.89)\n\narray([&lt;Axes: title={'center': 'skull_w'}&gt;,\n       &lt;Axes: title={'center': 'skull_w_copy'}&gt;,\n       &lt;Axes: title={'center': 'beta_sum'}&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\n\nskull_w_dist = trace.posterior['skull_w'][0]\nskull_w_copy_dist = trace.posterior['skull_w_copy'][0]\n\nplt.figure(figsize=(7, 5))\n\nplt.scatter(skull_w_dist, skull_w_copy_dist, s=15, alpha=.43)\n\n\n\n\n\n\n\n\n\nfor i in range(10):\n    print((float(skull_w_dist[i]), float(skull_w_copy_dist[i])))\n\n(3.9048626538535167, -3.1255189869958886)\n(-2.9323664950018005, 3.808560558848153)\n(-0.253518722705609, 1.12998380832265)\n(-1.7757320956912432, 2.644872542399044)\n(-1.1693805471416598, 2.0054535527772908)\n(-5.312591811956688, 6.1362012177510135)\n(-3.6857422924962964, 4.417532979288446)\n(0.46372693930833064, 0.20153951513903898)\n(-1.3630216489906541, 2.000311282396281)\n(1.2653379661450885, -0.2655350255316545)\n\n\nOkay, the things that get attention on the first row of plots:\n\nThe posterior distributions are extremely wide for both betas, reflecting the uncertainty.\nCould you calculate the sum of their means? :)\nLooking at the distribution of beta_sum: It’s centered around 0.82 with not so wide range. It’s because only the sum is identifiable.\n\nUnidentifiability means that the model does not make it possible to estimate the value of the parameter. Multicollinearity falls into this bucket of problems.\nThe second plot is showing actually what’s going on. After knowing the value of one, what’s the value of the other? Well, both variables carry the same information. Hence, as you know one the other doesn’t help much — ridge of equally plausible joint values.\n\\(\\displaystyle y = \\beta_{0} + \\beta_{1}X_1 + \\beta_{2}X_2\\)\nThose variables are the same in our example, so let’s denote both as X.\n\\(\\displaystyle y = \\beta_{0} + \\beta_{1}X + \\beta_{2}X\\)\nWhich leads to:\n\\(\\displaystyle y = \\beta_{0} + (\\beta_{1}+\\beta_{2})X\\)\nHence, they are not separable. The effect on y is their sum.\nAlright, so, is the model wrong?\nNope. You just can’t separate the effect of either variable, so you can’t make up your mind about which one is more important. Predictions will be just fine, if that’s what you care about.\n\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\naz.plot_ppc(\n    trace,\n    group='posterior',\n    kind='kde',\n    num_pp_samples=200,\n    ax=ax\n)\n\nplt.title('With both included')\n\nText(0.5, 1.0, 'With both included')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\naz.plot_ppc(\n    trace,\n    group='posterior',\n    kind='kde',\n    num_pp_samples=200,\n    ax=ax\n)\n\nplt.title('With only one included')\n\nText(0.5, 1.0, 'With only one included')\n\n\n\n\n\n\n\n\n\nI hope this post helps with internalizing what the issue is with multicollinearity, and sheds light into identifiability. I’ll keep up with stats posts in the future. Until next time, take care."
  },
  {
    "objectID": "posts/Man on a Mission - March-April 2025/index.html",
    "href": "posts/Man on a Mission - March-April 2025/index.html",
    "title": "Man on a Mission: March-April 2025",
    "section": "",
    "text": "Another month has passed. In these monthly posts, I won’t dive deep into every little bit of stuff that I studied during the month (that would be impossible), what I’ll try to convey the taste to increase your appetite.\nI haven’t been around for the last month and a half since there are more important things in life other than a blog. Due to the situations in Turkey, which you can read more here, it was extremely hard to focus on anything other than politics or law (there is also earthquake threat) which I spent most of my time with during last month.\nOther than these, I personally try to focus on Statistics nowadays which I’ll be giving more information below.\n\n\nGelman’s Regression and Other Stories (ROS)\nGelman’s ROS is the book that I’ve read a lot lately and really liked it. The book goes through Statistics but mainly from Bayesian perspective and uses simulations to facilitate understanding. The book uses R’s stanarm package but there is port of it in Python called Bambi which I already started exploring. It felt a bit counterintuitive at first but played around with it and now it makes more sense.\nGenerally, I have been focusing on simulations and methods related to Bayesian framework which made me dive into MCMCs a bit (just the general ones, such as Metropolis or Gibbs). Alongside that, I have developed interest in hierarchical modeling which I think about using for my basketball analytics projects.\n\n\nExplainable AI\nI’ve finished Molnar’s Interpretable ML a couple of months ago but he updated the book, so I have that opened in one of my tabs and will go through it soon.\n\n\nSports Analytics\nI have two Shiny Apps now related to Euroleague analytics: - https://hooplytics.shinyapps.io/EL_Player_Stats - https://hooplytics.shinyapps.io/EL_Team_Stats\nI wanted to include the defensive related stats but Turkey’s agenda got in the way, so I’m thinking about getting it done during May. I’ll also write a blog post (or maybe even a YT video) about how to navigate through the apps.\n\nThat’s pretty much all for now, I read/watched/listened a lot of politics and law related content but hoping to dial it down a bit to create more time for data science related topics (if Turkey allows me to focus on those things)…"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nI always had the idea of blogging but a suggestion that I received recently pushed me towards taking an action. I am hoping to use this place to share the stuff that I learn, it makes me develop an intuition and I feel like I learn better.\nMain things that I think about blogging fall under statistics (both frequentist and Bayesian methods), machine learning, and psychology (less frequently).\nAim of this blog is to make this place a useful place for the mentioned topics above, while improving myself on them as well. I hope you find this place useful."
  },
  {
    "objectID": "posts/Interpretablog - Partial Dependence/index.html",
    "href": "posts/Interpretablog - Partial Dependence/index.html",
    "title": "Interpretablog - Partial Dependence",
    "section": "",
    "text": "Throughout this interpretability series, I’ll mainly focus on model-agnostic methods which are appropriate for any type of model from linear regression to multilayer perceptrons. The following, partial dependence plot, is a model-agnostic method.\nIdea is very simple. Actually, I remember making use of it before knowing any method although I wasn’t aware of its limitations… Below, I have a model trained on Euroleague play-by-play data, which uses cumulative three point attempts (cum_P3A), cumulative three point makes (cum_P3M), shot angle, shot zone area to predict whether the shot will go in or not in the upcoming three point attempt.\n\nrf_bayes_search = BayesSearchCV(\n    rf,\n    rf_param_grid,\n    n_iter=20,      \n    cv=skf.split(X_train, y_train),           \n    scoring='neg_log_loss',\n    n_jobs=-2,\n    random_state=0\n)\n\nrf_bayes_search.fit(X_train, y_train)\n\nBayesSearchCV(cv=&lt;generator object _BaseKFold.split at 0x28c08eac0&gt;,\n              estimator=RandomForestClassifier(random_state=0), n_iter=20,\n              n_jobs=-2, random_state=0, scoring='neg_log_loss',\n              search_spaces={'class_weight': ['balanced'], 'max_depth': (3, 6),\n                             'max_features': (0.3, 1.0, 'uniform'),\n                             'min_samples_leaf': (30, 150),\n                             'min_samples_split': (50, 250),\n                             'n_estimators': (50, 300)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCViFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nRandomForestC...andom_state=0)\n\n\n\nsearch_spaces \n{'class_weight': ['balanced'], 'max_depth': (3, ...), 'max_features': (0.3, ...), 'min_samples_leaf': (30, ...), ...}\n\n\n\noptimizer_kwargs \nNone\n\n\n\nn_iter \n20\n\n\n\nscoring \n'neg_log_loss'\n\n\n\nfit_params \nNone\n\n\n\nn_jobs \n-2\n\n\n\nn_points \n1\n\n\n\niid \n'deprecated'\n\n\n\nrefit \nTrue\n\n\n\ncv \n&lt;generator ob...t 0x28c08eac0&gt;\n\n\n\nverbose \n0\n\n\n\npre_dispatch \n'2*n_jobs'\n\n\n\nrandom_state \n0\n\n\n\nerror_score \n'raise'\n\n\n\nreturn_train_score \nFalse\n\n\n\n\n            \n        \n    best_estimator_: RandomForestClassifierRandomForestClassifier(class_weight='balanced', max_depth=6, max_features=0.3,\n                       min_samples_leaf=30, min_samples_split=94,\n                       n_estimators=50, random_state=0)RandomForestClassifier?Documentation for RandomForestClassifier\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n50\n\n\n\ncriterion \n'gini'\n\n\n\nmax_depth \n6\n\n\n\nmin_samples_split \n94\n\n\n\nmin_samples_leaf \n30\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n0.3\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n0\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nclass_weight \n'balanced'\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\nLet’s say we want to know the affect of cum_P3A on the prediction. We create a grid of values for cum_P3A and for each observation in our dataset, we set the value of cum_P3A to the first value of the grid. We keep all the other variables the same, and make predictions via our model for each row. Then, average those.\nSo, for example, for the value of 5 we go through each row within the dataframe and set the cum_P3A to 5 and make predictions and average those.\nWhen we repeat the process for each value in the grid, we get the partial dependence function for cum_P3A:\n\\(\\displaystyle \\text{PD}_{cum3PA} = \\frac{1}{N} \\sum^{N}_{i=1} \\hat{f}(x_{cum3PA}, x_{-cum3PA}^{i})\\)\n\ngrid = np.arange(0, 80)\n\n\nlist_ice = []\n\nfor i, row in sampled_df.iterrows():\n    tmp_df = pd.DataFrame([row.values] * len(grid), columns=sampled_df.columns)\n    tmp_df['cum_P3A'] = grid\n    tmp_probas = rf_bayes_search.best_estimator_.predict_proba(tmp_df)[:, 1]\n    tmp_df['probas'] = tmp_probas\n    tmp_df['idx'] = i\n\n    list_ice.append(tmp_df)\n\n\navg_preds = []\n\nfor val in sorted(full_df['cum_P3A'].unique().tolist()):\n\n    avg_preds.append(np.mean(full_df.loc[full_df['cum_P3A'] == val, 'probas']))\n\n\nplt.figure(figsize=(7, 5))\n\nplt.plot(grid, avg_preds, lw=2, color='orange')\n\nplt.xlabel('3PA')\nplt.ylabel('Prediction')\n\nText(0, 0.5, 'Prediction')\n\n\n\n\n\n\n\n\n\nSo, that’s the partial dependence plot (PDP) for a cum_3PA. There are two problems that should worry you:\n\nWe miss out on heterogenous effects. A relatively flat line might be due to not having any affect at all or due to heterogenous effects canceling each other out. Individual conditional expectation (ICE) plots can help with that, which I show below.\nI didn’t choose cum_3PA arbitrarily. We don’t take joint distribution into account with PDP which may result us ending up with counterfactual rows that are not likely or even possible in this case. For example, for a row that has 10 as cum_3PM, having anything less than 10 for cum_3PA doesn’t make any sense at all but we allow that. I’ll show the joint distribution but before that, let’s start with ICEs.\n\n\nplt.figure(figsize=(7, 5))\n\nfor idx in full_df['idx'].unique().tolist():\n    plt.plot(\n        full_df.loc[full_df['idx'] == idx, 'cum_P3A'],\n        full_df.loc[full_df['idx'] == idx, 'probas'],\n        alpha=.03,\n        color='black',\n        lw=0.5\n        )\n\nplt.plot(grid, avg_preds, lw=2, color='orange')\n\nplt.xlabel('3PA')\nplt.ylabel('Prediction')\n\nText(0, 0.5, 'Prediction')\n\n\n\n\n\n\n\n\n\nThose black lines are ICEs, where each line corresponds to a single row. The orange line is the partial dependence curve. They are related since PDP is the average of those ICEs.\nLuckily, we don’t have to write those codes ourselves (I only did that for demonstrative purposes), sklearn has a built in function for it.\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\n\nfig, ax = plt.subplots(figsize=(7, 5))\n\nPartialDependenceDisplay.from_estimator(\n    rf_bayes_search.best_estimator_, \n    sampled_df, \n    features=['cum_P3A'], \n    ax=ax,\n    kind='both',\n    centered=True\n)\n\nplt.tight_layout()\nplt.show()\n\n/opt/anaconda3/envs/pymc_env/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:717: FutureWarning: The column 2 contains integer data. Partial dependence plots are not supported for integer data: this can lead to implicit rounding with NumPy arrays or even errors with newer pandas versions. Please convert numerical featuresto floating point dtypes ahead of time to avoid problems. This will raise ValueError in scikit-learn 1.9.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nCentering the ICEs makes it easier to compare the effect of a variable between different rows, so centered=True. Also, the rugs on the bottom helps with the variable’s distribution so that you can understand where the distribution is more dense and interpret accordingly. However, that doesn’t take joint distribution into account:\n\nplt.figure(figsize=(7, 5))\n\nplt.scatter(\n    sampled_df['cum_P3A'], sampled_df['cum_P3M'],\n    alpha=.3,\n    s=13\n)\n\nplt.xlabel('P3A')\nplt.ylabel('P3M')\n\nText(0, 0.5, 'P3M')\n\n\n\n\n\n\n\n\n\nMarginal distribution for cum_P3A covers a wide range, as it is apparent from the x-axis of the scatter plot. However, when we condition on, let’s say cum_P3M = 30, things differ:\n\nplt.figure(figsize=(6,4))\n\nsns.kdeplot(sampled_df.loc[sampled_df['cum_P3M'] == 30, 'cum_P3A'])\n\n\n\n\n\n\n\n\nSome values for cum_P3A doesn’t make any sense at all, like having lower number of attempts than made shots. Some others are highly unlikely, like making 30 in 30 attempts. I’ll talk about Accumulated Local Effects (ALE) in a different post, where it addresses the correlation issue.\nDespite its limitations, I like PDPs since the idea is very intuitive: Tracking the average prediction as I vary a particular variable. Also, you can make use of it in aggregate or certain portions of the data. ICEs are nice as well, although the plot gets extremely crowded if you have a relatively big data (I used a small sample due to this issue, for demonstrative purposes).\nAs long as you keep the limitations in mind, you should be fine :)"
  },
  {
    "objectID": "posts/Man on a Mission - May-June 2025/index.html",
    "href": "posts/Man on a Mission - May-June 2025/index.html",
    "title": "Man on a Mission: May-June 2025",
    "section": "",
    "text": "Another month has passed. In these monthly posts, I won’t dive deep into every little bit of stuff that I studied during the month (that would be impossible), what I’ll try to convey the taste to increase your appetite.\nFor the last two months, majority of my time was spent on basketball analytics. I started a newsletter, called The Read Step, which I post about EuroLeague analytics and basketball analytics in general. Don’t forget to subscribe if you’re interested.\nI’ve been interested in Bayesian modeling for the last 3 months, and I was using Bambi, which is a port of R’s stanarm. However, it has its limitations so I got much more familiar with PyMC in the last month which I used to build a Bayesian hierarchical model to estimate home-court advantage in EuroLeague. Here’s the post to it, if you wonder about it.\nI’ll be starting the classic, Statistical Rethinking, thinking about following the book while going through the videos on YouTube as well.\nOther than these, I bought Asimov’s I, Robot and can’t wait to get to it! During the vacation I read Sinan Alper’s and Onurcan Yilmaz’s Komplo Teorilerine Neden İnanırız? (Why Do We Believe In Conspiracies). It felt nice to get back to reading some social science stuff."
  },
  {
    "objectID": "posts/Navigating Through Euroleague Shiny App/index.html",
    "href": "posts/Navigating Through Euroleague Shiny App/index.html",
    "title": "Navigating Through Euroleague Shiny App",
    "section": "",
    "text": "I recently shared my shiny app related to Euroleague Basketball on LinkedIn. Now, I want to elaborate a bit on its sections.\nBefore going on, I’d like to point out that the site includes GLOSSARY which basically has everything I have got to say here. Despite that, I realized many people do not check it and that’s why I’m writing this. If glossary is for you, you can go directly to the site and check it there but if it’s not, you may want to stick here.\n\nOffense-Defense\nUnder the offense and defense sections, there are 4 tabs: Traditional, Advanced, Scoring, By Zone.\nTraditional, as the name implies, includes traditional statistics such as total games played, total points, total assists etc. I included games played so it’s possible to turn them into per-game stats, if anyone wants to.\n\nAdvanced tab consists of advanced statistics and stuff that you may not be able to find anywhere else for Euroleague data. POSS_IMP, possession importance, is one of them which attempts to quantify possession importance: How much a possession can change the win probability for the home team? This allows you to filter out garbage time, time of the match where the winner is already decided, while looking at the stats. In aggregate, it’s impossible to distinguish if a player’s 10 points came when it mattered or not.\nThere is also USG_RATE, shows fraction of possessions that the player used (e.g., taking a shot, turning the ball over) while he is on the court. If I was on the court for 10 offensive possessions and I used 2 of them, it means my usg_rate is 0.2. Total possession counts are also there, under POSS, which allows you to solve for total possessions if you need to.\nPer possession stats are the ones that have PP at the end of their variable name. These are important to understand how effective players are. Because there is a difference between scoring 2 points in 10 possessions or scoring 2 points in 20 possessions.\n\nScoring tab is related to scoring stats. It also includes possession importance. In addition to field goal percentages, it also includes assisted percentages, which shows how many of those baskets were assisted. Rather than given it in aggregate, I divide that into 2s and 3s to provide more information. Also, it shows percentage of baskets that came from fastbreaks, second chance, and turnover.\n\nBy zone includes these scoring stats broken down into different shot zones and shot areas. This allows you to see where, on the court, a player drives his scoring from along with his efficiency from there and how much he depends on other creators (fraction of assisted field goals).\n\n\n\nTools\nUnder tools, I have two tabs: Shot-chart, xPTS (expected points).\nShot-chart is probably the most fun tool for anyone to use.\n\n\n\nShane Larkin’s Regular Season Shot Chart\n\n\nThis tool plots a shot graph but allows for a lot more information and filtering options: Plot itself includes hexagons, which quantifies the shot volume from where the hexagon is with its size. In addition to that, it’s colored, which shows the percentage from there relative to league average. Raw percentages are included on top of those hexagons inside the boxes.\n\n\n\nShot chart options\n\n\nOptions of plotting allows you to filter the shots based on possession importance, who is on/off the court with the shot taker, and relative to league/team/position shot average. This provides you with a starting point to investigate player interactions: How a player gets affected from other player(s).\n\nUnder the xPTS tab, I’d say most important columns are PTS_ABOVE_X and PTS_ADDED. Former means how many points a player provides above league average shot per his shot attempt. Latter is the total of that, which gets affected by shot volume. While looking at these stats, do not forget the fact that it’s hard to stay efficient if you’re a high usage guy: The defenders look out for you all the time. With that being said, coaches allow only certain players to use that many shots because they have the capacity of being good despite that much attraction from defense.\nI have more ideas for this tool, hoping to add them in the future.\n\nI hope you enjoy the app and feel free to reach out to me for the features that you may want to see in it. Here are the links for the apps:\n\nEuroleague Player Stats App\nEuroleague Team Stats App"
  },
  {
    "objectID": "posts/what happens to sample statistics/index.html",
    "href": "posts/what happens to sample statistics/index.html",
    "title": "What Happens to Sample Statistics?",
    "section": "",
    "text": "Let’s start from the basics: The idea is to gather data to make an inference about the population. We use what we know (sample data) to estimate what we don’t (population).\nSo, let’s see what happens as one collects more data.\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nfor n in range(10, 101, 10):\n    sampled = np.random.normal(loc=100, scale=15, size=n)\n    print('Sampling ' + str(n) + ' observations')\n    print('Mean: ' + str(np.mean(sampled)))\n    print('Standard Deviation ' + str(np.std(sampled)))\n    print('\\n')\n\nSampling 10 observations\nMean: 96.54930308046485\nStandard Deviation 17.69744525686926\n\n\nSampling 20 observations\nMean: 95.26529645787349\nStandard Deviation 13.557714188035797\n\n\nSampling 30 observations\nMean: 99.59631039122944\nStandard Deviation 16.67387851610786\n\n\nSampling 40 observations\nMean: 94.76579341950723\nStandard Deviation 18.06257861698366\n\n\nSampling 50 observations\nMean: 99.61537083458882\nStandard Deviation 14.897878602988285\n\n\nSampling 60 observations\nMean: 98.68203398707892\nStandard Deviation 14.644083079076118\n\n\nSampling 70 observations\nMean: 100.36682454658889\nStandard Deviation 15.427573719883997\n\n\nSampling 80 observations\nMean: 101.00471630426532\nStandard Deviation 12.678064261146886\n\n\nSampling 90 observations\nMean: 102.7631649616841\nStandard Deviation 13.03915710302493\n\n\nSampling 100 observations\nMean: 99.17823535292733\nStandard Deviation 14.951583438089918\n\n\n\n\nAs one increases the sample size taken from the population, sample statistics will approach towards the population parameter.\nDOES NOT NECESSARILY DECREASE! (do not confuse std_dev and std_err) As one can see from the example above!\nBut standard error WILL decrease as the sample size increases. It should make sense intuitively: I have more confidence in my estimates if I know more.\n\nfor n in range(10, 101, 10):\n    sampled = np.random.normal(loc=100, scale=15, size=n)\n    print('Sampling ' + str(n) + ' observations')\n    std_err = np.std(sampled) / np.sqrt(n)\n    print('Standard error approximation: ' + str(std_err))\n    print('\\n')\n\nSampling 10 observations\nStandard error approximation: 4.452823707231046\n\n\nSampling 20 observations\nStandard error approximation: 3.798915557039333\n\n\nSampling 30 observations\nStandard error approximation: 2.3932757697459746\n\n\nSampling 40 observations\nStandard error approximation: 2.624220779126406\n\n\nSampling 50 observations\nStandard error approximation: 2.2840746792281217\n\n\nSampling 60 observations\nStandard error approximation: 2.015124706208566\n\n\nSampling 70 observations\nStandard error approximation: 1.7230901229016737\n\n\nSampling 80 observations\nStandard error approximation: 1.5993946754011168\n\n\nSampling 90 observations\nStandard error approximation: 1.66349862578967\n\n\nSampling 100 observations\nStandard error approximation: 1.577820207327762\n\n\n\n\nHow close is the approximation? Let’s try it for one sample\n\nn = 51\n\npop = np.random.normal(loc=100, scale=15, size=300000) # Population with normal distribution(mean=100, sd=15)\n\nsampled = np.random.choice(pop, size=n) # randomly sampling\nestimated_mean = np.mean(sampled) # sample mean\nestimated_sd = np.std(sampled) # sample standard deviation\n\nestimated_std_err = np.std(sampled) / n**.5 # estimated standard error, expected variation for my sample statistic.\n\nprint(estimated_mean, estimated_sd, estimated_std_err)\n\n95.27311481976386 15.960503088053418 2.2349174605268747\n\n\n\n# Let's take many samples and estimate the mean\n\nmean_estimates = []\n\nfor i in range(1000): # Let's do it 1000 times, sampling 51 in each iteration.\n    sampled = np.random.choice(pop, size=n)\n    mean_estimates.append(np.mean(sampled))\n\nnp.std(mean_estimates)\n\n2.1480690130242537\n\n\nAs one can see, it’s not that far away.\n\nplt.figure(figsize=(10,6))\n\ng = sns.swarmplot(data=mean_estimates, orient=\"h\", size=6, alpha=.8, color=\"purple\", linewidth=0.5,\n                 edgecolor=\"black\")\n\n\n\n\n\n\n\n\nWhat happens when one lowers the sample size? More variation, less confidence. As the sample size increases the estimates approach towards the parameter, so with large sample size each sample ends up having similar estimates. However, that’s not the case with low sample size.\n\nn = 16\nmean_estimates = []\n\nfor i in range(1000): # Let's do it 1000 times\n    sampled = np.random.choice(pop, size=n)\n    mean_estimates.append(np.mean(sampled))\n\nnp.std(mean_estimates)\n\n3.713500386895356\n\n\n\nplt.figure(figsize=(10,6))\n\ng = sns.swarmplot(data=mean_estimates, orient=\"h\", size=6, alpha=.8, color=\"purple\", linewidth=0.5,\n                 edgecolor=\"black\")\n\n\n\n\n\n\n\n\nWatch out the x-axis, it’s much wider now.\n\nmean_estimates = {\n        16:[],\n        23:[],\n        30:[],\n        51:[],\n        84:[],\n        101:[]\n    }\n\nfor n in [16, 23, 30, 51, 84, 101]:\n    for i in range(500):\n        sampled = np.random.choice(pop, size=n)\n        mean_estimates[n].append(np.mean(sampled))\n\n\nfor key in mean_estimates.keys():\n    print('Sample size: ' + str(key))\n    print('Standard deviation (std_err) around the estimates: ' + str(np.std(mean_estimates[key])))\n    print('\\n')\n\nSample size: 16\nStandard deviation (std_err) around the estimates: 3.855337925425897\n\n\nSample size: 23\nStandard deviation (std_err) around the estimates: 3.1917223557725363\n\n\nSample size: 30\nStandard deviation (std_err) around the estimates: 2.7639204989190023\n\n\nSample size: 51\nStandard deviation (std_err) around the estimates: 2.0366022769176806\n\n\nSample size: 84\nStandard deviation (std_err) around the estimates: 1.736807903528174\n\n\nSample size: 101\nStandard deviation (std_err) around the estimates: 1.504399688755003"
  },
  {
    "objectID": "posts/Diff-in-diffs/index.html",
    "href": "posts/Diff-in-diffs/index.html",
    "title": "Quasi Stuff - Difference in Differences",
    "section": "",
    "text": "Introduction to Difference in Differences (DiD)\nIn a classic causal inference setting, at the end of the day what we are interested in is \\(\\text{E}[Y_1 - Y_0 | \\text{D}=1]\\) where D is the treatment.\nWhen we have an intervention (e.g., policy changes, marketing campaign etc), what we end up with is a time period before and after the intervention. Hence, average treatment effect on treated (ATET): \\(\\hat{\\text{ATET}} = \\text{E}[Y_1(1) - Y_0(1) | \\text{D}=1]\\) where \\(Y_D(T)\\) is the potential outcome for treatment D on time T. To make things a bit clear with the word “potential”: We either observe the treated or untreated, can’t observe both at once for any individual since we can’t turn back time and do it the other way. Okay, since we got that out of the way, let’s continue.\nYou may have thought about different approaches to this ATET, like doing a before vs after: \\(\\hat{\\text{ATET}} = \\text{E}[Y(1) | \\text{D}=1] - \\text{E}[Y(0) | \\text{D}=1]\\)\nWell, this one assumes \\(\\text{E}[Y_0(1) | \\text{D}=1] = \\text{E}[Y_0(0) | \\text{D}=1]\\). In plain English, it says that the Y before and after the intervention would be equal, in the case of no intervention. This might sound reasonable at first but imagine a line plot with any kind of trend (positive or negative) – in either of them, it’s problematic since there’s some temporal effect on Y. So, cutting the line plot vertically in any point and looking at both sides wouldn’t yield the same y values.\nOne other approach is: \\(\\hat{\\text{ATET}} = \\text{E}[Y(1) | \\text{D}=1] - \\text{E}[Y(1) | \\text{D}=0]\\). So, comparing the treated group to untreated group. We assume that the counterfactual can be replaced: \\(\\text{E}[Y_0(1) | \\text{D}=0] = \\text{E}[Y_0(1) | \\text{D}=1]\\). This requires a similar baseline level, which is sometimes accomplished by stratifying or propensity matching (they have their own issues).\nDiD solves this by:\n\\(\\displaystyle \\text{E}[Y_0(1) | \\text{D}=1] = \\text{E}[Y_0(0) | \\text{D}=1] + (\\text{E}[Y_0(1) | \\text{D=0}] - \\text{E}[Y_0(0) | \\text{D}=0])\\)\nIt states that the counterfactual, the version of the treated had it not been treated, after the intervention, is equal to the treated before the treatment plus the trend of the control. Hence, an another assumption: Trends are the same between the treatment and the control. \\(\\text{E}[Y_0(1) - Y_0(0) | \\text{D}=1] = \\text{E}[Y_0(1) - Y_0(0) | \\text{D}=0]\\)\nAlright, let’s put everything together then (alongside some algebraic arrangements):\n\\(\\displaystyle \\hat{\\text{ATET}} = (\\text{E}[Y(1) | \\text{D}=1] - \\text{E}[Y(1) | \\text{D}=0]) - (\\text{E}[Y(0) | \\text{D}=1] - \\text{E}[Y(0) | \\text{D}=0])\\)\nThus the name, difference in differences, since what we have is the difference between the treatment and control before and after the intervention.\nLet’s see it in action.\n\nimport pandas as pd\nimport numpy as np\n\nimport duckdb as db\n\nfrom matplotlib import pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom linearmodels.panel import PanelOLS\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\n# amazon = pd.read_excel('amazon.xlsx')\namazon = db.query(\"SELECT * FROM read_csv_auto('am_did.csv', IGNORE_ERRORS=TRUE)\").to_df()\ngoodreads = db.query(\"SELECT * FROM read_csv_auto('gr_did.csv', IGNORE_ERRORS=TRUE)\").to_df()\n\n\namazon.drop(['column0', 'reviewerID', 'reviewText', 'summary'], axis=1, inplace=True)\ngoodreads.drop(['column0', 'book_id', 'review_id', 'review_text', 'date_updated', 'user_id'], axis=1, inplace=True)\n\nWe have Amazon and Goodreads data. Goodreads added a new feature on May 21, 2014 which allows users to post questions to others (including authors). Let’s find out this feature’s effect on book ratings.\n\namazon['date'] = pd.to_datetime(amazon['unixReviewTime'])\ngoodreads['date'] = pd.to_datetime(goodreads['date'])\n\n\nfirst_date = pd.to_datetime('1996-11-14')\n\n\namazon['t'] = (amazon['unixReviewTime'].dt.to_period('M') - first_date.to_period('M')).apply(lambda x: x.n)\ngoodreads['t'] = (goodreads['date'].dt.to_period('M') - first_date.to_period('M')).apply(lambda x: x.n)\n\n\ngoodreads.loc[goodreads['year_month'] == 'May 2014', :].head()\n\n\n\n\n\n\n\n\nrating\ndate\nyear_month\nasin\nt\n\n\n\n\n105\n5\n2014-05-26\nMay 2014\n1855496593\n210\n\n\n531\n5\n2014-05-27\nMay 2014\n1855496593\n210\n\n\n733\n5\n2014-05-06\nMay 2014\n1855496593\n210\n\n\n972\n5\n2014-05-12\nMay 2014\n1855496593\n210\n\n\n977\n5\n2014-05-10\nMay 2014\n1855496593\n210\n\n\n\n\n\n\n\n\namazon['goodr'] = 0\ngoodreads['goodr'] = 1\n\n\ndf = pd.concat(\n    [amazon[['goodr', 'year_month', 'date', 'rating', 't', 'asin']], goodreads[['goodr', 'year_month', 'date', 'rating', 't', 'asin']]],\n    ignore_index=True\n)\ndf.head()\n\n\n\n\n\n\n\n\ngoodr\nyear_month\ndate\nrating\nt\nasin\n\n\n\n\n0\n0\nJan 2007\n2007-01-03\n4\n122\n000100039X\n\n\n1\n0\nApr 2015\n2015-04-16\n5\n221\n000100039X\n\n\n2\n0\nApr 2015\n2015-04-08\n5\n221\n000100039X\n\n\n3\n0\nJul 2013\n2013-07-03\n5\n200\n000100039X\n\n\n4\n0\nNov 2016\n2016-11-13\n5\n240\n000100039X\n\n\n\n\n\n\n\n\ndf['qa'] = (df['date'] &gt;= pd.to_datetime('2014-05-21')).astype(int)\ndf.head()\n\n\n\n\n\n\n\n\ngoodr\nyear_month\ndate\nrating\nt\nasin\nqa\n\n\n\n\n0\n0\nJan 2007\n2007-01-03\n4\n122\n000100039X\n0\n\n\n1\n0\nApr 2015\n2015-04-16\n5\n221\n000100039X\n1\n\n\n2\n0\nApr 2015\n2015-04-08\n5\n221\n000100039X\n1\n\n\n3\n0\nJul 2013\n2013-07-03\n5\n200\n000100039X\n0\n\n\n4\n0\nNov 2016\n2016-11-13\n5\n240\n000100039X\n1\n\n\n\n\n\n\n\n\ndf.groupby(['goodr', 'qa']).agg(\n    rating_mean = ('rating', np.mean)\n).reset_index()\n\n\n\n\n\n\n\n\ngoodr\nqa\nrating_mean\n\n\n\n\n0\n0\n0\n4.255\n\n\n1\n0\n1\n4.361\n\n\n2\n1\n0\n3.791\n\n\n3\n1\n1\n3.801\n\n\n\n\n\n\n\nAnd, here’s the diff-in-diffs.\n\n(3.801 - 3.791) - (4.361 - 4.255)\n\n-0.09599999999999964\n\n\nLet’s check the parallel trends assumption.\n\ngoodr_agg = goodr.groupby('t').agg(\n    avg_rating = ('rating', np.mean)\n).reset_index()\n\namaz_agg = amaz.groupby('t').agg(\n    avg_rating = ('rating', np.mean)\n).reset_index()\n\ngoodr_agg.head()\n\n\n\n\n\n\n\n\nt\navg_rating\n\n\n\n\n0\n125\n3.832\n\n\n1\n126\n3.788\n\n\n2\n127\n3.706\n\n\n3\n128\n3.755\n\n\n4\n129\n3.776\n\n\n\n\n\n\n\n\nplt.figure(figsize=(7, 5))\n\nplt.plot(goodr_agg.t, goodr_agg.avg_rating, label='goodr')\nplt.plot(amaz_agg.t, amaz_agg.avg_rating, label='amazon')\n\nplt.plot(np.repeat(210, 201), np.linspace(3, 5, 201), ls='--', color='black', label='QA release', alpha=.57)\n\nplt.ylim(3.5, 4.5)\n\nplt.legend()\n\n\n\n\n\n\n\n\nThey look pretty similar prior to the treatment, hence it’s fine (DiD would be biased otherwise).\nWell, do we just conclude negative effect? Not really. We would like to control for other variables which is possible via regression. On top of that, we get standard error which helps with quantifying the uncertainty surrounding the estimate.\n\nlm = smf.ols('rating ~ C(qa)*C(goodr)', data=df)\nres = lm.fit()\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.045\n\n\nModel:\nOLS\nAdj. R-squared:\n0.045\n\n\nMethod:\nLeast Squares\nF-statistic:\n2.295e+04\n\n\nDate:\nSun, 07 Sep 2025\nProb (F-statistic):\n0.00\n\n\nTime:\n21:47:22\nLog-Likelihood:\n-2.2720e+06\n\n\nNo. Observations:\n1469344\nAIC:\n4.544e+06\n\n\nDf Residuals:\n1469340\nBIC:\n4.544e+06\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.2552\n0.002\n2275.194\n0.000\n4.252\n4.259\n\n\nC(qa)[T.1]\n0.1059\n0.002\n45.324\n0.000\n0.101\n0.110\n\n\nC(goodr)[T.1]\n-0.4646\n0.003\n-155.565\n0.000\n-0.470\n-0.459\n\n\nC(qa)[T.1]:C(goodr)[T.1]\n-0.0954\n0.004\n-23.027\n0.000\n-0.104\n-0.087\n\n\n\n\n\n\n\n\nOmnibus:\n364326.390\nDurbin-Watson:\n1.825\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n740463.435\n\n\nSkew:\n-1.501\nProb(JB):\n0.00\n\n\nKurtosis:\n4.757\nCond. No.\n6.81\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe can also add fixed effects as well. Different model specifications can result in different estimates, hence it’s important to choose right type of model. Here, for convenience, I go for single fixed effect.\n\ndf['year_month'] = df['year_month'].astype('category')\ndf['goodr_qa'] =  df['goodr'] * df['qa']\n\ndf.set_index(['year_month', df.index], inplace=True)\n\nexog = sm.add_constant(df[['goodr', 'qa', 'goodr_qa']])\n\n\nfe_model = PanelOLS(df['rating'], exog, entity_effects=True).fit()\nfe_model.summary\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\nrating\nR-squared:\n0.0375\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n0.1461\n\n\nNo. Observations:\n1469344\nR-squared (Within):\n0.0375\n\n\nDate:\nSun, Sep 07 2025\nR-squared (Overall):\n0.0433\n\n\nTime:\n21:47:23\nLog-likelihood\n-2.27e+06\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n1.908e+04\n\n\nEntities:\n258\nP-value\n0.0000\n\n\nAvg Obs:\n5695.1\nDistribution:\nF(3,1469083)\n\n\nMin Obs:\n1.0000\n\n\n\n\nMax Obs:\n2.768e+04\nF-statistic (robust):\n1.908e+04\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n1469344\nDistribution:\nF(3,1469083)\n\n\nAvg Obs:\n1.0000\n\n\n\n\nMin Obs:\n1.0000\n\n\n\n\nMax Obs:\n1.0000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nconst\n4.3027\n0.0106\n406.99\n0.0000\n4.2820\n4.3235\n\n\ngoodr\n-0.4520\n0.0032\n-142.57\n0.0000\n-0.4582\n-0.4458\n\n\nqa\n0.0212\n0.0179\n1.1876\n0.2350\n-0.0138\n0.0563\n\n\ngoodr_qa\n-0.1070\n0.0043\n-24.868\n0.0000\n-0.1155\n-0.0986\n\n\n\nF-test for Poolability: 19.050P-value: 0.0000Distribution: F(257,1469083)Included effects: Entity\n\n\nYou can see the difference in each results. What else can be done? It’s possible to add two-way fixed effects, adding books to the model, also to cluster standard errors since observation within each book have more similarity compared to others which would result in more appropriate standard error estimates.\nI’m not going to dive further for now, since I haven’t discussed these very explicitly in the blog (yet). However, if you’re interested you can take a look at the following:\nTilburg Science Hub - DiD"
  },
  {
    "objectID": "posts/Bayes Theorem/index.html",
    "href": "posts/Bayes Theorem/index.html",
    "title": "An Intro Example to Bayes’ Theorem",
    "section": "",
    "text": "Imagine that you and I are playing a guessing game. I have two dice, 12-sided and 6-sided, and I am holding them in both of my hands. You are trying to guess which hand has the 12-sided die. At this very moment, there is no information for you. So, it’s not uncommon for a rational agent to think 50-50.\nHowever, you tell me to roll the die I hold on my left hand, and close your eyes. I inform you that I rolled above 3 (i.e., &gt;= 4). Is it still 50-50? It feels like it’s not, since rolling above 3 is more likely with the 12-sided die, right? So, how should you update your initial belief?\nI emphasized the “update” above, this is what one should think of when one encounters the word Bayesian: Updating the prior beliefs in the light of new data. Let’s go through the example.\n\n\nAt the beginning, before any information, it’s 50-50: \\(P(12\\: sided \\ LH) = 0.5\\) and \\(P(12\\: sided \\ RH) = 0.5\\) where LH and RH stands for left-hand and right-hand, respectively. Once you have the information that I rolled bigger than 3, there are two possible scenarios: Either I rolled bigger than 3 with 12-sided or with 6-sided.\n\n\\(P(&gt;3\\: |\\ 12\\ sided) = 0.75\\)\n\\(P(&gt;3\\: |\\ 6\\ sided) = 0.5\\)\n\nLet’s put those in a tree diagram.\n\n\n\nTree Diagram\n\n\n\nThe first column (left to the first vertical white bar) represents initial beliefs.\nThe second column represents the probabilities given the first column.\nThird column is the multiplication of the three, and represents P(A and B).\n\nWell, you are not interested in the whole diagram since you have observed some data: I rolled a number bigger than 3. You wonder:\n\\(P(12\\ sided\\ LH\\: | &gt;3)\\), which is \\(\\dfrac{P(12\\ sided\\ LH\\: ,\\ &gt;3)}{P(&gt;3)}\\).\nFor the numerator, you can track the first row of the tree diagram which leads to 0.375. The denominator consists of two parts, rolling above 3 under two different hypotheses: Following the path of rolling above 3 with 12-sided on the left leads to 0.375, which is the first part. In addition, tracking the other scenario of rolling above 3 with 6-sided on hand leads to 0.25. Hence: \\(\\dfrac{0.375}{0.375+0.25} = 0.6\\)\nThis represents your new belief of having the 12-sided die on my left hand. If you tell me to roll the die again and construct a new tree diagram, the first column will consist of 0.6 and 0.4 and the following columns would be adjusted accordingly.\n\n\n\nDo you have to construct tree diagram each time? After all, even for this simple question the whole process takes a bit of time. With a little bit of algebra, you don’t have to: \\(P(A\\: |\\ B) = \\dfrac{P(A,B)}{P(B)}\\). Multiplying both sides by the P(B): \\(P(A\\: |\\ B) P(B) = P(A,B)\\) and since P(A,B) = P(B,A)\n\\(P(A\\: |\\ B) P(B) = P(B,A)\\) which leads to \\(P(A\\: |\\ B) P(B) = P(B\\: |\\ A) P (A)\\)\nand voila: \\(P(A\\: |\\ B) = \\dfrac{P(B\\: |\\ A) P(A)}{P(B)}\\)\n\nThe left hand side is called posterior probability, and you may come across it in the form of P(hypothesis | data).\nThe denominator on the right is total probability of the data, sometimes referred to as marginal probability.\nP(A) is your initial belief here, the prior.\nWhile P(B | A) is called likelihood which is the probability of the data under given the hypothesis.\n\nFor our example: \\(P(12\\ sided\\: |\\ &gt; 3) = \\dfrac{P(&gt;3\\: |\\ 12\\ sided) P(12\\ sided)}{P(&gt;3)}\\)\n\n\n\nYou may see versions of this where instead of hypothesis there can be theory or parameters (and instead of data, evidence) but they all are the same initially. This type of approach has its advantages such as incorporating the prior knowledge: If I would roll the die again, you would make those calculations with new priors (learned from the first roll), making use of what you already know. It allows for priors that are subjective: Maybe I am known to favor my left hand so it is possible for you to have an initial belief that is not reflected as 50-50.\nI will talk more about this view in the future posts but if you’re interested, you can check probability and Bayesian chapter in each of the books below:\n\nLearning Statistics with R by Daniel Navarro\nPhilosophy of Quantitative Methods by Brian D. Haig\nDoing Bayesian Data Analysis by John K. Kruschke\nImproving Your Statistical Inferences by Daniel Lakens\nThink Bayes by Allen B. Downey\n\nand I believe I remember the example above from Mine Çetinkaya-Rundel."
  },
  {
    "objectID": "posts/Bayes Theorem/index.html#usual-dice-example",
    "href": "posts/Bayes Theorem/index.html#usual-dice-example",
    "title": "An Intro Example to Bayes’ Theorem",
    "section": "",
    "text": "Imagine that you and I are playing a guessing game. I have two dice, 12-sided and 6-sided, and I am holding them in both of my hands. You are trying to guess which hand has the 12-sided die. At this very moment, there is no information for you. So, it’s not uncommon for a rational agent to think 50-50.\nHowever, you tell me to roll the die I hold on my left hand, and close your eyes. I inform you that I rolled above 3 (i.e., &gt;= 4). Is it still 50-50? It feels like it’s not, since rolling above 3 is more likely with the 12-sided die, right? So, how should you update your initial belief?\nI emphasized the “update” above, this is what one should think of when one encounters the word Bayesian: Updating the prior beliefs in the light of new data. Let’s go through the example.\n\n\nAt the beginning, before any information, it’s 50-50: \\(P(12\\: sided \\ LH) = 0.5\\) and \\(P(12\\: sided \\ RH) = 0.5\\) where LH and RH stands for left-hand and right-hand, respectively. Once you have the information that I rolled bigger than 3, there are two possible scenarios: Either I rolled bigger than 3 with 12-sided or with 6-sided.\n\n\\(P(&gt;3\\: |\\ 12\\ sided) = 0.75\\)\n\\(P(&gt;3\\: |\\ 6\\ sided) = 0.5\\)\n\nLet’s put those in a tree diagram.\n\n\n\nTree Diagram\n\n\n\nThe first column (left to the first vertical white bar) represents initial beliefs.\nThe second column represents the probabilities given the first column.\nThird column is the multiplication of the three, and represents P(A and B).\n\nWell, you are not interested in the whole diagram since you have observed some data: I rolled a number bigger than 3. You wonder:\n\\(P(12\\ sided\\ LH\\: | &gt;3)\\), which is \\(\\dfrac{P(12\\ sided\\ LH\\: ,\\ &gt;3)}{P(&gt;3)}\\).\nFor the numerator, you can track the first row of the tree diagram which leads to 0.375. The denominator consists of two parts, rolling above 3 under two different hypotheses: Following the path of rolling above 3 with 12-sided on the left leads to 0.375, which is the first part. In addition, tracking the other scenario of rolling above 3 with 6-sided on hand leads to 0.25. Hence: \\(\\dfrac{0.375}{0.375+0.25} = 0.6\\)\nThis represents your new belief of having the 12-sided die on my left hand. If you tell me to roll the die again and construct a new tree diagram, the first column will consist of 0.6 and 0.4 and the following columns would be adjusted accordingly.\n\n\n\nDo you have to construct tree diagram each time? After all, even for this simple question the whole process takes a bit of time. With a little bit of algebra, you don’t have to: \\(P(A\\: |\\ B) = \\dfrac{P(A,B)}{P(B)}\\). Multiplying both sides by the P(B): \\(P(A\\: |\\ B) P(B) = P(A,B)\\) and since P(A,B) = P(B,A)\n\\(P(A\\: |\\ B) P(B) = P(B,A)\\) which leads to \\(P(A\\: |\\ B) P(B) = P(B\\: |\\ A) P (A)\\)\nand voila: \\(P(A\\: |\\ B) = \\dfrac{P(B\\: |\\ A) P(A)}{P(B)}\\)\n\nThe left hand side is called posterior probability, and you may come across it in the form of P(hypothesis | data).\nThe denominator on the right is total probability of the data, sometimes referred to as marginal probability.\nP(A) is your initial belief here, the prior.\nWhile P(B | A) is called likelihood which is the probability of the data under given the hypothesis.\n\nFor our example: \\(P(12\\ sided\\: |\\ &gt; 3) = \\dfrac{P(&gt;3\\: |\\ 12\\ sided) P(12\\ sided)}{P(&gt;3)}\\)\n\n\n\nYou may see versions of this where instead of hypothesis there can be theory or parameters (and instead of data, evidence) but they all are the same initially. This type of approach has its advantages such as incorporating the prior knowledge: If I would roll the die again, you would make those calculations with new priors (learned from the first roll), making use of what you already know. It allows for priors that are subjective: Maybe I am known to favor my left hand so it is possible for you to have an initial belief that is not reflected as 50-50.\nI will talk more about this view in the future posts but if you’re interested, you can check probability and Bayesian chapter in each of the books below:\n\nLearning Statistics with R by Daniel Navarro\nPhilosophy of Quantitative Methods by Brian D. Haig\nDoing Bayesian Data Analysis by John K. Kruschke\nImproving Your Statistical Inferences by Daniel Lakens\nThink Bayes by Allen B. Downey\n\nand I believe I remember the example above from Mine Çetinkaya-Rundel."
  },
  {
    "objectID": "posts/p-value distribution/index.html",
    "href": "posts/p-value distribution/index.html",
    "title": "Which p-values to expect under different realities?",
    "section": "",
    "text": "Statistics classes in social sciences, during an undergraduate degree, revolve around p-values mainly. Despite that, I have never seen the mention of p-value distributions. So, here we go:\n\nimport numpy as np\nimport scipy.stats as st\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nHow does the p-value distribution look like when there is no effect?\n\np_val_list = [] # a bag to hold p-values\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51) # sampling from a normal distribution with mean 100, std 15\n    trt = np.random.normal(loc=100, scale=15, size=51) # sampling from a normal distribution with mean 100, std 15\n    p_val = st.ttest_ind(trt, ctrl)[1]  # doing a t-test and grabbing the p-value\n\n    p_val_list.append(p_val) # storing the p-value in the bag\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('When the true effect size = 0')\n\nText(0.5, 1.0, 'When the true effect size = 0')\n\n\n\n\n\n\n\n\n\nEvery p-value is equally likely. That’s why chosen alpha level corresponds to how often you will fool yourself in the long run, when there is no effect.\nIf the chosen alpha level is .10, then under the null hypothesis 10 percent of the p-values fall below 0.10.\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.05\n\n\nThis is our long-term type I error rate, since theoretically the power is undefined in this case (i.e., the null is the truth). We don’t expect to fool ourselves more than 5% in the long run, when there is no effect.\nLet’s see what happens when there is an effect:\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=104.5, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.3')\n\nText(0.5, 1.0, \"For Cohen's d of 0.3\")\n\n\n\n\n\n\n\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.32\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.3')\n\nText(0.5, 1.0, \"For Cohen's d of 0.3\")\n\n\n\n\n\n\n\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=107.5, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--', alpha=.5)\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.71\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=109.5, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--', alpha=.5)\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.7')\n\nText(0.5, 1.0, \"For Cohen's d of 0.7\")\n\n\n\n\n\n\n\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.89\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.7')\n\nText(0.5, 1.0, \"For Cohen's d of 0.7\")\n\n\n\n\n\n\n\n\n\n\nprint(len([val for val in p_val_list if val &gt;= .00 and val &lt;= .01]) / len(p_val_list))\n\n0.715716\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=51)\n    trt = np.random.normal(loc=112, scale=15, size=51)\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xticks(np.arange(0, 1, 0.05))\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.8')\n\nText(0.5, 1.0, \"For Cohen's d of 0.8\")\n\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist([val for val in p_val_list if val &lt;= .05], bins=5, edgecolor='black', linewidth=.9)\nax.axhline(y=10000, color='r', linestyle='--')\n\n# ax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xlim(0, 0.05)\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.8')\n\nText(0.5, 1.0, \"For Cohen's d of 0.8\")\n\n\n\n\n\n\n\n\n\n\nprint(len([val for val in p_val_list if val &gt;= .04 and val &lt;= .05]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .03 and val &lt;= .04]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .02 and val &lt;= .03]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .01 and val &lt;= .02]) / len(p_val_list), '\\n',\n      len([val for val in p_val_list if val &gt;= .00 and val &lt;= .01]) / len(p_val_list))\n\n0.005389 \n 0.008187 \n 0.014401 \n 0.032377 \n 0.91928\n\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.98\n\n\nAs the statistical power increases, distribution of p-values pile up at the very left: Some p-values below 0.05 become more likely (ones more close to 0.00). And when you have very high power, certain p-values below 0.05 (relatively high ones) become more likely under the null:\nHence, wouldn’t be wise to reject the null despite p-value &lt; .05\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=65) # different sample size\n    trt = np.random.normal(loc=107.5, scale=15, size=65) # different sample size\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.81\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xticks(np.arange(0, 1, 0.05))\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\n\np_val_list = []\n\nfor i in range(0, 1000000):\n    ctrl = np.random.normal(loc=100, scale=15, size=100) # different sample size\n    trt = np.random.normal(loc=107.5, scale=15, size=100) # different sample size\n    p_val = st.ttest_ind(trt, ctrl)[1]\n\n    p_val_list.append(p_val)\n\n\npower = [val for val in p_val_list if val &lt;= 0.05] # .05 is the chosen for alpha\nprint('Power: ', round(len(power) / len(p_val_list), 2))\n\nPower:  0.94\n\n\n\nfig = plt.figure(figsize=(9, 5))\nax = fig.add_subplot(1, 1, 1)\n\nplt.ticklabel_format(style='plain', axis='y')\n\nax.hist(p_val_list, bins=100, edgecolor='black', linewidth=.9)\nax.xaxis.set_minor_locator(AutoMinorLocator(5))\nax.yaxis.set_minor_locator(AutoMinorLocator(5))\nax.tick_params(which='both', width=2)\nax.tick_params(which='major', length=7)\nax.tick_params(which='minor', length=4)\n\nax.set_xticks(np.arange(0, 1, 0.05))\nax.set_ylim(0, 1000000)\nplt.title('For Cohen\\'s d of 0.5')\n\nText(0.5, 1.0, \"For Cohen's d of 0.5\")\n\n\n\n\n\n\n\n\n\nAs one can see, statistical power also depends on the sample size and it should make sense: p-value is a function of test statistic (t, in this case) and following from there, function of sample size. If this doesn’t feel comfortable to you, I suggest you to take a look at this post, and check what happens to standard error as the sample size increases.\nIn the examples below, we always knew the underlying distributions of the populations we sample from. In reality, we usually end up with a result and we wonder the true effect: What is the chance that there is an effect, given that I observed one. This conditional probability is called positive predictive value, something that I heard from a political scientist during pre-COVID days, and if you’re interested in these stuff I suggest you to take a look at it. You can start from Ioannidis’ paper or Lakens’ book for more structured follow."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "YIGIT ASIK, Data Scientist",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDo Black Cats Experience Better Adoption Since The Movie Flow\n\n\n\n\n\n\nStats\n\n\n\n\n\n\n\n\n\nDec 28, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\n\n\nStats\n\n\n\n\n\n\n\n\n\nOct 12, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretablog - Handling the Enemy with ALEs\n\n\n\n\n\n\nStats\n\n\nInterpretability\n\n\n\n\n\n\n\n\n\nSep 30, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\n(Non)Identifiability\n\n\n\n\n\n\nStats\n\n\nInference\n\n\n\n\n\n\n\n\n\nSep 21, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals, Multicollinearity, and the Problem with Treatment-Only Predictors\n\n\n\n\n\n\nStats\n\n\nInference\n\n\nInterpretability\n\n\n\n\n\n\n\n\n\nSep 13, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nQuasi Stuff - Difference in Differences\n\n\n\n\n\n\nStats\n\n\nInference\n\n\n\n\n\n\n\n\n\nSep 7, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nPartial Regression/Added Variable/Predictor Residual Plots\n\n\n\n\n\n\nStats\n\n\nInterpretability\n\n\n\n\n\n\n\n\n\nAug 6, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretablog - Partial Dependence\n\n\n\n\n\n\nInterpretability\n\n\n\n\n\n\n\n\n\nAug 3, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretablog - Fire & Forget\n\n\n\n\n\n\nStats\n\n\nInterpretability\n\n\n\n\n\n\n\n\n\nJul 16, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nMan on a Mission: May-June 2025\n\n\n\n\n\n\nmonthly\n\n\n\n\n\n\n\n\n\nJun 20, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating Through Euroleague Shiny App\n\n\n\n\n\n\nSports Analytics\n\n\n\n\n\n\n\n\n\nMay 3, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nMan on a Mission: March-April 2025\n\n\n\n\n\n\nmonthly\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nHow To Structure A Shiny App: Brief Overview\n\n\n\n\n\n\nCode\n\n\n2-cents\n\n\n\n\n\n\n\n\n\nMar 16, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nHow To Automatize Your Tasks\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n\n\n\nMar 8, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nMan on a Mission: February 2025\n\n\n\n\n\n\nmonthly\n\n\n\n\n\n\n\n\n\nMar 1, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nIntuitive Conceptual Framing For Summary Statistics\n\n\n\n\n\n\nStats\n\n\n\n\n\n\n\n\n\nFeb 22, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nNicer Contour Plots\n\n\n\n\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nFeb 16, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s an Interaction Effect? An example\n\n\n\n\n\n\nStats\n\n\nInference\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nHow Basketball Analytics May Have An Effect on Your Analytics Job\n\n\n\n\n\n\nSports Analytics\n\n\n2-cents\n\n\n\n\n\n\n\n\n\nFeb 5, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Way - Predictions Out of Posterior Distribution\n\n\n\n\n\n\nStats\n\n\nBayesian\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nMan on a Mission: January 2025\n\n\n\n\n\n\nmonthly\n\n\n\n\n\n\n\n\n\nJan 31, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Model Fitting with Likelihood\n\n\n\n\n\n\nStats\n\n\nBayesian\n\n\n\n\n\n\n\n\n\nJan 25, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Distribution of log(odds ratio) Through Simulation\n\n\n\n\n\n\nStats\n\n\nInference\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWhich p-values to expect under different realities?\n\n\n\n\n\n\nStats\n\n\nInference\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Updating: Poisson & Gamma\n\n\n\n\n\n\nStats\n\n\nBayesian\n\n\nSports Analytics\n\n\n\n\n\n\n\n\n\nJan 10, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nAn Intro Example to Bayes’ Theorem\n\n\n\n\n\n\nStats\n\n\nprobability\n\n\nBayesian\n\n\n\n\n\n\n\n\n\nJan 5, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nMan on a Mission: Starting 2025\n\n\n\n\n\n\nmonthly\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Happens to Sample Statistics?\n\n\n\n\n\n\nStats\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 28, 2024\n\n\nYiğit Aşık\n\n\n\n\n\n\nNo matching items"
  }
]
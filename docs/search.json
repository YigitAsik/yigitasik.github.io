[
  {
    "objectID": "posts/what happens to sample statistics/index.html",
    "href": "posts/what happens to sample statistics/index.html",
    "title": "What Happens to Sample Statistics?",
    "section": "",
    "text": "Let’s start from the basics: The idea is to gather data to make an inference about the population. We use what we know (sample data) to estimate what we don’t (population).\nSo, let’s see what happens as one collects more data.\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nfor n in range(10, 101, 10):\n    sampled = np.random.normal(loc=100, scale=15, size=n)\n    print('Sampling ' + str(n) + ' observations')\n    print('Mean: ' + str(np.mean(sampled)))\n    print('Standard Deviation ' + str(np.std(sampled)))\n    print('\\n')\n\nSampling 10 observations\nMean: 96.54930308046485\nStandard Deviation 17.69744525686926\n\n\nSampling 20 observations\nMean: 95.26529645787349\nStandard Deviation 13.557714188035797\n\n\nSampling 30 observations\nMean: 99.59631039122944\nStandard Deviation 16.67387851610786\n\n\nSampling 40 observations\nMean: 94.76579341950723\nStandard Deviation 18.06257861698366\n\n\nSampling 50 observations\nMean: 99.61537083458882\nStandard Deviation 14.897878602988285\n\n\nSampling 60 observations\nMean: 98.68203398707892\nStandard Deviation 14.644083079076118\n\n\nSampling 70 observations\nMean: 100.36682454658889\nStandard Deviation 15.427573719883997\n\n\nSampling 80 observations\nMean: 101.00471630426532\nStandard Deviation 12.678064261146886\n\n\nSampling 90 observations\nMean: 102.7631649616841\nStandard Deviation 13.03915710302493\n\n\nSampling 100 observations\nMean: 99.17823535292733\nStandard Deviation 14.951583438089918\n\n\n\n\nAs one increases the sample size taken from the population, sample statistics will approach towards the population parameter.\nDOES NOT NECESSARILY DECREASE! (do not confuse std_dev and std_err) As one can see from the example above!\nBut standard error WILL decrease as the sample size increases. It should make sense intuitively: I have more confidence in my estimates if I know more.\n\nfor n in range(10, 101, 10):\n    sampled = np.random.normal(loc=100, scale=15, size=n)\n    print('Sampling ' + str(n) + ' observations')\n    std_err = np.std(sampled) / np.sqrt(n)\n    print('Standard error approximation: ' + str(std_err))\n    print('\\n')\n\nSampling 10 observations\nStandard error approximation: 4.452823707231046\n\n\nSampling 20 observations\nStandard error approximation: 3.798915557039333\n\n\nSampling 30 observations\nStandard error approximation: 2.3932757697459746\n\n\nSampling 40 observations\nStandard error approximation: 2.624220779126406\n\n\nSampling 50 observations\nStandard error approximation: 2.2840746792281217\n\n\nSampling 60 observations\nStandard error approximation: 2.015124706208566\n\n\nSampling 70 observations\nStandard error approximation: 1.7230901229016737\n\n\nSampling 80 observations\nStandard error approximation: 1.5993946754011168\n\n\nSampling 90 observations\nStandard error approximation: 1.66349862578967\n\n\nSampling 100 observations\nStandard error approximation: 1.577820207327762\n\n\n\n\nHow close is the approximation? Let’s try it for one sample\n\nn = 51\n\npop = np.random.normal(loc=100, scale=15, size=300000) # Population with normal distribution(mean=100, sd=15)\n\nsampled = np.random.choice(pop, size=n) # randomly sampling\nestimated_mean = np.mean(sampled) # sample mean\nestimated_sd = np.std(sampled) # sample standard deviation\n\nestimated_std_err = np.std(sampled) / n**.5 # estimated standard error, expected variation for my sample statistic.\n\nprint(estimated_mean, estimated_sd, estimated_std_err)\n\n95.27311481976386 15.960503088053418 2.2349174605268747\n\n\n\n# Let's take many samples and estimate the mean\n\nmean_estimates = []\n\nfor i in range(1000): # Let's do it 1000 times, sampling 51 in each iteration.\n    sampled = np.random.choice(pop, size=n)\n    mean_estimates.append(np.mean(sampled))\n\nnp.std(mean_estimates)\n\n2.1480690130242537\n\n\nAs one can see, it’s not that far away.\n\nplt.figure(figsize=(10,6))\n\ng = sns.swarmplot(data=mean_estimates, orient=\"h\", size=6, alpha=.8, color=\"purple\", linewidth=0.5,\n                 edgecolor=\"black\")\n\n\n\n\n\n\n\n\nWhat happens when one lowers the sample size? More variation, less confidence. As the sample size increases the estimates approach towards the parameter, so with large sample size each sample ends up having similar estimates. However, that’s not the case with low sample size.\n\nn = 16\nmean_estimates = []\n\nfor i in range(1000): # Let's do it 1000 times\n    sampled = np.random.choice(pop, size=n)\n    mean_estimates.append(np.mean(sampled))\n\nnp.std(mean_estimates)\n\n3.713500386895356\n\n\n\nplt.figure(figsize=(10,6))\n\ng = sns.swarmplot(data=mean_estimates, orient=\"h\", size=6, alpha=.8, color=\"purple\", linewidth=0.5,\n                 edgecolor=\"black\")\n\n\n\n\n\n\n\n\nWatch out the x-axis, it’s much wider now.\n\nmean_estimates = {\n        16:[],\n        23:[],\n        30:[],\n        51:[],\n        84:[],\n        101:[]\n    }\n\nfor n in [16, 23, 30, 51, 84, 101]:\n    for i in range(500):\n        sampled = np.random.choice(pop, size=n)\n        mean_estimates[n].append(np.mean(sampled))\n\n\nfor key in mean_estimates.keys():\n    print('Sample size: ' + str(key))\n    print('Standard deviation (std_err) around the estimates: ' + str(np.std(mean_estimates[key])))\n    print('\\n')\n\nSample size: 16\nStandard deviation (std_err) around the estimates: 3.855337925425897\n\n\nSample size: 23\nStandard deviation (std_err) around the estimates: 3.1917223557725363\n\n\nSample size: 30\nStandard deviation (std_err) around the estimates: 2.7639204989190023\n\n\nSample size: 51\nStandard deviation (std_err) around the estimates: 2.0366022769176806\n\n\nSample size: 84\nStandard deviation (std_err) around the estimates: 1.736807903528174\n\n\nSample size: 101\nStandard deviation (std_err) around the estimates: 1.504399688755003"
  },
  {
    "objectID": "posts/Man on a Mission - Starting 2025/index.html",
    "href": "posts/Man on a Mission - Starting 2025/index.html",
    "title": "Man on a Mission: Starting 2025",
    "section": "",
    "text": "Some time ago, I stumbled upon the website Less Certainty, More Inquiry. If you click on the link, you’ll see that Darryl Blackport shares what he listens to or reads every week of the year. I found the concept intriguing and thought it would be a great fit for me, as I’m often immersed in studying or reading something.\nThis year, I’m taking inspiration from his approach. I’ll share what I study each month, providing a brief overview of the topics I’ve explored, focusing on what I found particularly interesting or useful. To kick things off, here are my learning goals for 2025. Let’s dive in.\n\n\n\n\n\n\nFor most of my data science journey, I’ve taken a theory-first approach, focusing heavily on statistics. However, with recent advancements in tools like LLMs, I think it’s time for a shift. In 2025, I plan to delve into deep learning (a domain I’ve avoided so far) with a hands-on approach.\nHere’s my roadmap for deep learning:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n\nAdvanced Learning Algorithms\n\nDeep Learning Specialization\n\n\n\n\n\n\n\nI’ve been passionate about sports analytics for nearly a year now and have made significant strides in basketball analytics. (You can check out some of my work here.)\nThis year, I aim to expand my focus by creating videos on football (soccer) analytics, in addition to basketball, on my YouTube channel. To prepare for this, I’ve started the following specialization:\n\nSports Performance Analytics Specialization\n\nAlso, Dean Oliver’s new book, Basketball Beyond Paper, is on my reading list—it’s a must-read for anyone in this space.\n\n\n\n\n\n\nI consider statistics to be one of my stronger skills compared to other data scientists in the industry (at least in Turkey), but there’s always room for growth.\nCurrently, I’m halfway through Think Bayes, and I’m looking forward to Tom Faulkenberry’s upcoming book when it’s released.\nThere are specific topics, like meta-analysis, that I want to strengthen, but I don’t plan to tackle them systematically. Instead, I’ll explore them as curiosity strikes.\n\n\n\n\n\n\nThis one’s a “maybe.” Deep learning and LLMs are higher priorities, and the list is already packed. However, gaining production-level data science expertise could be beneficial in the long run.\nFor now, it’s something I’ll keep on the back burner, but we’ll see how the year unfolds.\n\n\n\n\n\n\n\nThis blog post was polished with help of ChatGPT! I write the whole piece and feed it to ChatGPT to improve the flow.\nAnd that’s pretty much it for my 2025 learning goals. Stay tuned for updates!"
  },
  {
    "objectID": "posts/Man on a Mission - Starting 2025/index.html#man-on-a-mission-series",
    "href": "posts/Man on a Mission - Starting 2025/index.html#man-on-a-mission-series",
    "title": "Man on a Mission: Starting 2025",
    "section": "",
    "text": "Some time ago, I stumbled upon the website Less Certainty, More Inquiry. If you click on the link, you’ll see that Darryl Blackport shares what he listens to or reads every week of the year. I found the concept intriguing and thought it would be a great fit for me, as I’m often immersed in studying or reading something.\nThis year, I’m taking inspiration from his approach. I’ll share what I study each month, providing a brief overview of the topics I’ve explored, focusing on what I found particularly interesting or useful. To kick things off, here are my learning goals for 2025. Let’s dive in.\n\n\n\n\n\n\nFor most of my data science journey, I’ve taken a theory-first approach, focusing heavily on statistics. However, with recent advancements in tools like LLMs, I think it’s time for a shift. In 2025, I plan to delve into deep learning (a domain I’ve avoided so far) with a hands-on approach.\nHere’s my roadmap for deep learning:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n\nAdvanced Learning Algorithms\n\nDeep Learning Specialization\n\n\n\n\n\n\n\nI’ve been passionate about sports analytics for nearly a year now and have made significant strides in basketball analytics. (You can check out some of my work here.)\nThis year, I aim to expand my focus by creating videos on football (soccer) analytics, in addition to basketball, on my YouTube channel. To prepare for this, I’ve started the following specialization:\n\nSports Performance Analytics Specialization\n\nAlso, Dean Oliver’s new book, Basketball Beyond Paper, is on my reading list—it’s a must-read for anyone in this space.\n\n\n\n\n\n\nI consider statistics to be one of my stronger skills compared to other data scientists in the industry (at least in Turkey), but there’s always room for growth.\nCurrently, I’m halfway through Think Bayes, and I’m looking forward to Tom Faulkenberry’s upcoming book when it’s released.\nThere are specific topics, like meta-analysis, that I want to strengthen, but I don’t plan to tackle them systematically. Instead, I’ll explore them as curiosity strikes.\n\n\n\n\n\n\nThis one’s a “maybe.” Deep learning and LLMs are higher priorities, and the list is already packed. However, gaining production-level data science expertise could be beneficial in the long run.\nFor now, it’s something I’ll keep on the back burner, but we’ll see how the year unfolds.\n\n\n\n\n\n\n\nThis blog post was polished with help of ChatGPT! I write the whole piece and feed it to ChatGPT to improve the flow.\nAnd that’s pretty much it for my 2025 learning goals. Stay tuned for updates!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "YIGIT ASIK, Data Scientist",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAn Intro Example to Bayes’ Theorem\n\n\n\n\n\n\nstats\n\n\nprobability\n\n\nBayesian\n\n\n\n\n\n\n\n\n\nJan 5, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nMan on a Mission: Starting 2025\n\n\n\n\n\n\nupdate\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Happens to Sample Statistics?\n\n\n\n\n\n\nstats\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\nYiğit Aşık\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 28, 2024\n\n\nYiğit Aşık\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Yiğit, though many people call me Yigas—a nickname blending my first and last names. I hold a degree in English Literature, but I currently work as a data scientist at DenizBank in Istanbul. I am mainly interested in causal inference, predictive modeling, explainable AI. In my free time, I dive into basketball data, exploring patterns and creating statistical models that lead to new metrics."
  },
  {
    "objectID": "about.html#the-start",
    "href": "about.html#the-start",
    "title": "About",
    "section": "The Start",
    "text": "The Start\nDuring my first year at university, I spent a semester break in Beirut, Lebanon, where I earned a human biomechanics trainer certificate. I’d been training basketball players at various levels, including those in the Turkish Basketball League (TBL). While in Beirut, I often heard about the impact of prolonged stress on human health. This led me to watch Dr. Robert Sapolsky’s TED Talk, The Biology of Our Best and Worst Selves, and I thought, “I want to understand human behavior the way he does.”"
  },
  {
    "objectID": "about.html#getting-into-psychology-lab",
    "href": "about.html#getting-into-psychology-lab",
    "title": "About",
    "section": "Getting Into Psychology Lab",
    "text": "Getting Into Psychology Lab\nI began watching Stanford’s Human Behavioral Biology courses online. While intriguing, I found the material challenging without a background in the field. To get answers, I reached out to my university’s Psychology Department, where I met Dr. Hasan Bahçekapılı and Dr. Onurcan Yılmaz. Soon, I was visiting Dr. Yılmaz’s office regularly to discuss topics related to psychology, and I grew fascinated with his research interests—especially how morality, politics, religion, and decision-making intersect. Gradually, I shifted my focus from behavioral biology to social and evolutionary psychology. When he offered me a spot in the lab (MINT Lab) he was forming, I jumped at the opportunity."
  },
  {
    "objectID": "about.html#learning-statistics",
    "href": "about.html#learning-statistics",
    "title": "About",
    "section": "Learning Statistics",
    "text": "Learning Statistics\nAhead of the lab’s start, I took an edX’s Science of Religion course to ensure I had foundational knowledge. Although enjoyable, I quickly realized, once we started reading research papers, that I needed a solid ground in statistics to evaluate them. As an English Literature major, I needed a resource that started from scratch, and OpenIntro Statistics and Learning Stats with JASP became essential guides for me. Studying statistics was unexpectedly enjoyable, and I decided to develop my skills further."
  },
  {
    "objectID": "about.html#learning-mathematics",
    "href": "about.html#learning-mathematics",
    "title": "About",
    "section": "Learning Mathematics",
    "text": "Learning Mathematics\nNot knowing mathematics bothered me, and I began to wonder how it might enhance my understanding of statistics. Fortunately, I connected with Dr. Basar Coşkunoğlu, whom I knew through playing Hearthstone. With his patient guidance, I started with basics like functions and inequalities, eventually advancing to calculus and linear algebra. We reached a point where I could continue independently, and I still study linear algebra occasionally, using my notes, Mathematics for Machine Learning, and Gilbert Strang’s works."
  },
  {
    "objectID": "about.html#python-for-data-science-ml",
    "href": "about.html#python-for-data-science-ml",
    "title": "About",
    "section": "Python for Data Science & ML",
    "text": "Python for Data Science & ML\nIn the lab and academia, statistical tools like JASP, Jamovi, and SPSS are prevalent, with some usage of R. Around this time, however, my academic interests began shifting, so I decided to learn Python. I enrolled in a Data Science & Machine Learning Bootcamp, which focused on programming and industry cases, building on my previous knowledge from Introduction to Statistical Learning."
  },
  {
    "objectID": "about.html#data-science-internship",
    "href": "about.html#data-science-internship",
    "title": "About",
    "section": "Data Science Internship",
    "text": "Data Science Internship\nWhile still an undergraduate, I received a scholarship from TUBITAK (The Scientific and Technological Research Council of Turkey) for research participation. Wanting broader experience, I sought a part-time role or long-term internship to balance with school. I started building a project portfolio and applied to various positions. Many interviews revealed that prospective employers lacked data science teams, which felt limiting for my first role. Finally, I applied for and was accepted to DenizBank’s data science internship program."
  },
  {
    "objectID": "about.html#transition-to-full-time",
    "href": "about.html#transition-to-full-time",
    "title": "About",
    "section": "Transition to Full-Time",
    "text": "Transition to Full-Time\nDuring my internship, things went well, and DenizBank invited me to join full-time after graduation. Despite six months of on-the-job coding, I needed to pass a data scientist test in SQL, Python/R, and statistics to secure the position, which I did. I now work full-time as a Data Scientist at DenizBank."
  },
  {
    "objectID": "about.html#basketball-analytics",
    "href": "about.html#basketball-analytics",
    "title": "About",
    "section": "Basketball Analytics",
    "text": "Basketball Analytics\nAfter ten years of playing and training in basketball, I always hoped to integrate it into my work. Inspired by a Formula 1 analytics account, I launched a basketball analytics account of my own. You can find links in the footer."
  },
  {
    "objectID": "posts/Bayes Theorem/index.html",
    "href": "posts/Bayes Theorem/index.html",
    "title": "An Intro Example to Bayes’ Theorem",
    "section": "",
    "text": "Imagine that you and I are playing a guessing game. I have two dice, 12-sided and 6-sided, and I am holding them in both of my hands. You are trying to guess which hand has the 12-sided die. At this very moment, there is no information for you. So, it’s not uncommon for a rational agent to think 50-50.\nHowever, you tell me to roll the die I hold on my left hand, and close your eyes. I inform you that I rolled above 3 (i.e., &gt;= 4). Is it still 50-50? It feels like it’s not, since rolling above 3 is more likely with the 12-sided die, right? So, how should you update your initial belief?\nI emphasized the “update” above, this is what one should think of when one encounters the word Bayesian: Updating the prior beliefs in the light of new data. Let’s go through the example.\n\n\nAt the beginning, before any information, it’s 50-50: \\(P(12\\: sided \\ LH) = 0.5\\) and \\(P(12\\: sided \\ RH) = 0.5\\) where LH and RH stands for left-hand and right-hand, respectively. Once you have the information that I rolled bigger than 3, there are two possible scenarios: Either I rolled bigger than 3 with 12-sided or with 6-sided.\n\n\\(P(&gt;3\\: |\\ 12\\ sided) = 0.75\\)\n\\(P(&gt;3\\: |\\ 6\\ sided) = 0.5\\)\n\nLet’s put those in a tree diagram.\n\n\n\nTree Diagram\n\n\n\nThe first column (left to the first vertical white bar) represents initial beliefs.\nThe second column represents the probabilities given the first column.\nThird column is the multiplication of the three, and represents P(A and B).\n\nWell, you are not interested in the whole diagram since you have observed some data: I rolled a number bigger than 3. You wonder:\n\\(P(12\\ sided\\ LH\\: | &gt;3)\\), which is \\(\\dfrac{P(12\\ sided\\ LH\\: ,\\ &gt;3)}{P(&gt;3)}\\).\nFor the numerator, you can track the first row of the tree diagram which leads to 0.375. The denominator consists of two parts, rolling above 3 under two different hypotheses: Following the path of rolling above 3 with 12-sided on the left leads to 0.375, which is the first part. In addition, tracking the other scenario of rolling above 3 with 6-sided on hand leads to 0.25. Hence: \\(\\dfrac{0.375}{0.375+0.25} = 0.6\\)\nThis represents your new belief of having the 12-sided die on my left hand. If you tell me to roll the die again and construct a new tree diagram, the first column will consist of 0.6 and 0.4 and the following columns would be adjusted accordingly.\n\n\n\nDo you have to construct tree diagram each time? After all, even for this simple question the whole process takes a bit of time. With a little bit of algebra, you don’t have to: \\(P(A\\: |\\ B) = \\dfrac{P(A,B)}{P(B)}\\). Multiplying both sides by the P(B): \\(P(A\\: |\\ B) P(B) = P(A,B)\\) and since P(A,B) = P(B,A)\n\\(P(A\\: |\\ B) P(B) = P(B,A)\\) which leads to \\(P(A\\: |\\ B) P(B) = P(B\\: |\\ A) P (A)\\)\nand voila: \\(P(A\\: |\\ B) = \\dfrac{P(B\\: |\\ A) P(A)}{P(B)}\\)\n\nThe left hand side is called posterior probability, and you may come across it in the form of P(hypothesis | data).\nThe denominator on the right is total probability of the data, sometimes referred to as marginal probability.\nP(A) is your initial belief here, the prior.\nWhile P(B | A) is called likelihood which is the probability of the data under given the hypothesis.\n\nFor our example: \\(P(12\\ sided\\: |\\ &gt; 3) = \\dfrac{P(&gt;3\\: |\\ 12\\ sided) P(12\\ sided)}{P(&gt;3)}\\)\n\n\n\nYou may see versions of this where instead of hypothesis there can be theory or parameters (and instead of data, evidence) but they all are the same initially. This type of approach has its advantages such as incorporating the prior knowledge: If I would roll the die again, you would make those calculations with new priors (learned from the first roll), making use of what you already know. It allows for priors that are subjective: Maybe I am known to favor my left hand so it is possible for you to have an initial belief that is not reflected as 50-50.\nI will talk more about this view in the future posts but if you’re interested, you can check probability and Bayesian chapter in each of the books below:\n\nLearning Statistics with R by Daniel Navarro\nPhilosophy of Quantitative Methods by Brian D. Haig\nDoing Bayesian Data Analysis by John K. Kruschke\nImproving Your Statistical Inferences by Daniel Lakens\nThink Bayes by Allen B. Downey\n\nand I believe I remember the example above from Mine Çetinkaya-Rundel."
  },
  {
    "objectID": "posts/Bayes Theorem/index.html#usual-dice-example",
    "href": "posts/Bayes Theorem/index.html#usual-dice-example",
    "title": "An Intro Example to Bayes’ Theorem",
    "section": "",
    "text": "Imagine that you and I are playing a guessing game. I have two dice, 12-sided and 6-sided, and I am holding them in both of my hands. You are trying to guess which hand has the 12-sided die. At this very moment, there is no information for you. So, it’s not uncommon for a rational agent to think 50-50.\nHowever, you tell me to roll the die I hold on my left hand, and close your eyes. I inform you that I rolled above 3 (i.e., &gt;= 4). Is it still 50-50? It feels like it’s not, since rolling above 3 is more likely with the 12-sided die, right? So, how should you update your initial belief?\nI emphasized the “update” above, this is what one should think of when one encounters the word Bayesian: Updating the prior beliefs in the light of new data. Let’s go through the example.\n\n\nAt the beginning, before any information, it’s 50-50: \\(P(12\\: sided \\ LH) = 0.5\\) and \\(P(12\\: sided \\ RH) = 0.5\\) where LH and RH stands for left-hand and right-hand, respectively. Once you have the information that I rolled bigger than 3, there are two possible scenarios: Either I rolled bigger than 3 with 12-sided or with 6-sided.\n\n\\(P(&gt;3\\: |\\ 12\\ sided) = 0.75\\)\n\\(P(&gt;3\\: |\\ 6\\ sided) = 0.5\\)\n\nLet’s put those in a tree diagram.\n\n\n\nTree Diagram\n\n\n\nThe first column (left to the first vertical white bar) represents initial beliefs.\nThe second column represents the probabilities given the first column.\nThird column is the multiplication of the three, and represents P(A and B).\n\nWell, you are not interested in the whole diagram since you have observed some data: I rolled a number bigger than 3. You wonder:\n\\(P(12\\ sided\\ LH\\: | &gt;3)\\), which is \\(\\dfrac{P(12\\ sided\\ LH\\: ,\\ &gt;3)}{P(&gt;3)}\\).\nFor the numerator, you can track the first row of the tree diagram which leads to 0.375. The denominator consists of two parts, rolling above 3 under two different hypotheses: Following the path of rolling above 3 with 12-sided on the left leads to 0.375, which is the first part. In addition, tracking the other scenario of rolling above 3 with 6-sided on hand leads to 0.25. Hence: \\(\\dfrac{0.375}{0.375+0.25} = 0.6\\)\nThis represents your new belief of having the 12-sided die on my left hand. If you tell me to roll the die again and construct a new tree diagram, the first column will consist of 0.6 and 0.4 and the following columns would be adjusted accordingly.\n\n\n\nDo you have to construct tree diagram each time? After all, even for this simple question the whole process takes a bit of time. With a little bit of algebra, you don’t have to: \\(P(A\\: |\\ B) = \\dfrac{P(A,B)}{P(B)}\\). Multiplying both sides by the P(B): \\(P(A\\: |\\ B) P(B) = P(A,B)\\) and since P(A,B) = P(B,A)\n\\(P(A\\: |\\ B) P(B) = P(B,A)\\) which leads to \\(P(A\\: |\\ B) P(B) = P(B\\: |\\ A) P (A)\\)\nand voila: \\(P(A\\: |\\ B) = \\dfrac{P(B\\: |\\ A) P(A)}{P(B)}\\)\n\nThe left hand side is called posterior probability, and you may come across it in the form of P(hypothesis | data).\nThe denominator on the right is total probability of the data, sometimes referred to as marginal probability.\nP(A) is your initial belief here, the prior.\nWhile P(B | A) is called likelihood which is the probability of the data under given the hypothesis.\n\nFor our example: \\(P(12\\ sided\\: |\\ &gt; 3) = \\dfrac{P(&gt;3\\: |\\ 12\\ sided) P(12\\ sided)}{P(&gt;3)}\\)\n\n\n\nYou may see versions of this where instead of hypothesis there can be theory or parameters (and instead of data, evidence) but they all are the same initially. This type of approach has its advantages such as incorporating the prior knowledge: If I would roll the die again, you would make those calculations with new priors (learned from the first roll), making use of what you already know. It allows for priors that are subjective: Maybe I am known to favor my left hand so it is possible for you to have an initial belief that is not reflected as 50-50.\nI will talk more about this view in the future posts but if you’re interested, you can check probability and Bayesian chapter in each of the books below:\n\nLearning Statistics with R by Daniel Navarro\nPhilosophy of Quantitative Methods by Brian D. Haig\nDoing Bayesian Data Analysis by John K. Kruschke\nImproving Your Statistical Inferences by Daniel Lakens\nThink Bayes by Allen B. Downey\n\nand I believe I remember the example above from Mine Çetinkaya-Rundel."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nI always had the idea of blogging but a suggestion that I received recently pushed me towards taking an action. I am hoping to use this place to share the stuff that I learn, it makes me develop an intuition and I feel like I learn better.\nMain things that I think about blogging fall under statistics (both frequentist and Bayesian methods), machine learning, and psychology (less frequently).\nAim of this blog is to make this place a useful place for the mentioned topics above, while improving myself on them as well. I hope you find this place useful."
  }
]
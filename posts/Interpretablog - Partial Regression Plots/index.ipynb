{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ab127f91",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Partial Regression/Added Variable/Predictor Residual Plots\"\n",
    "author: \"Yiğit Aşık\"\n",
    "date: \"2025-08-06\"\n",
    "categories: [Stats, Interpretability]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e315e7",
   "metadata": {},
   "source": [
    "I wanted to introduce *partial regression plots* (or added variable plots, or predictor residual plots etc.), before moving any further down this series.\n",
    "\n",
    "In the first post of this series, I showed a relationship by plotting $X_j$ vs $y$. In a multivariate setting, that doesn't tell us much since if there seems to be a relationship, it might be due to shared effects with other variables that are not $X_j$. What will we do is to plot what exactly the regression coefficient tells us: Marginal effect of $X_j$, adjusted for other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5e7d6",
   "metadata": {},
   "source": [
    "Idea is pretty nice:\n",
    "\n",
    "* We have a variable $X_j$. We fit a model to predict it with other variables.\n",
    "* Residuals ($r_{Xj}$) from that model represent the part of $X_j$ that cannot be explained by other variables.\n",
    "* Plot it against $y$ (or residuals of $y$).\n",
    "\n",
    "If you're interested in the contribution of $X_j$ to prediction, after the contribution of all the other variables, you should plot against residuals of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16d4a1",
   "metadata": {},
   "source": [
    "Let's exemplify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271bd1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import empiricaldist as emd\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import thinkstats as ts\n",
    "import utils as ut\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae78bab",
   "metadata": {},
   "source": [
    "We are here with our good old possum data, which I used in different examples as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc948fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('possum.csv', usecols=['head_l', 'tail_l', 'total_l'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = sm.add_constant(df[['head_l', 'tail_l']])\n",
    "model_full = sm.OLS(df['total_l'], X_full).fit()\n",
    "\n",
    "print('Coefficient for `head_l`: ', np.round(model_full.params['head_l'], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ece6d",
   "metadata": {},
   "source": [
    "Here we have the full model with intercept, and note the coefficient of the head_l.\n",
    "\n",
    "Now, I'm going to show both versions with raw values of $y$ and residuals of $y$ ($r_y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressing head_l ~ tail_l\n",
    "\n",
    "model_head_resid = LinearRegression().fit(df[['tail_l']], df['head_l'])\n",
    "head_resids = df['head_l'] - model_head_resid.predict(df[['tail_l']]) # residualized head_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v1 = sm.add_constant(head_resids) # intercept\n",
    "model_v1 = sm.OLS(df['total_l'], X_v1).fit()\n",
    "pred_v1 = model_v1.predict(X_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "plt.scatter(head_resids, df['total_l'], label='Data')\n",
    "plt.plot(head_resids, pred_v1, color='red', label='Fitted Line')\n",
    "\n",
    "plt.title('V1: total_l ~ $r_{head_l}$')\n",
    "\n",
    "plt.xlabel('residualized head_l')\n",
    "plt.ylabel('total_l')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa22c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_y_resid = LinearRegression().fit(df[['tail_l']], df['total_l'])\n",
    "y_resids = df['total_l'] - model_y_resid.predict(df[['tail_l']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5495f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v2 = sm.add_constant(head_resids) # don't need it here actually, but wanted to keep things similar.\n",
    "\n",
    "model_v2 = sm.OLS(y_resids, X_v2).fit()\n",
    "pred_v2 = model_v2.predict(X_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(head_resids, y_resids, label='Data')\n",
    "plt.plot(head_resids, pred_v2, color='orange', label='Fitted Line')\n",
    "\n",
    "plt.title('V2: residualized y ~ residualized head_l')\n",
    "\n",
    "plt.xlabel('residualized head_l')\n",
    "plt.ylabel('residualized total_l')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a323a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient for `head_l`: ', np.round(model_full.params['head_l'], 3))\n",
    "print('Version 1 (y ~ residualized X):', np.round(model_v1.params[1], 3))\n",
    "print('Version 2 (resid y ~ resid X):', np.round(model_v2.params[1], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a5347",
   "metadata": {},
   "source": [
    "Which one is better?\n",
    "\n",
    "Well, the first version is easier to interpret since `y` is in its raw. However, spread of it contains the influence of other predictors as well. Hence, although the slope reflects the contribution after knowing the others, the vertical scatter is not corresponding to the full model.\n",
    "\n",
    "In the second one, axes are purged from the influence of other variables. So, visually, we're looking through the lens of \"holding-others-constant\" (literally)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d865370",
   "metadata": {},
   "source": [
    "This approach is beneficial for diagnostics as well, it seems to me, since:\n",
    "\n",
    "1) It's easier to see if a particular point is pulling the slope strongly.\n",
    "2) If there's high multicollinearity, $r_{X_j}$ would show a very low variance since most of it gets explained by other variables.\n",
    "3) The pattern is apparent, and if it shows different kind you may think about transformations.\n",
    "4) It's much easier to communicate compared to giving out the coefficient only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
